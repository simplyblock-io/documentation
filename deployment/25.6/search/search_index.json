{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-simplyblock-documentation","title":"Welcome to the Simplyblock Documentation","text":"<p>Welcome to the Simplyblock Documentation, your comprehensive resource for understanding, deploying, and managing simplyblock's cloud-native, high-performance storage platform. This documentation provides detailed information on architecture, installation, configuration, and best practices, ensuring you have the necessary guidance to maximize the efficiency and reliability of your simplyblock deployment.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Learn the basics</p> <p>General information about simplyblock, the documentation, and important terms. Read here first.</p> <p> Important Notes</p> </li> <li> <p> Plan the deployment</p> <p>Before starting to deploy simplyblock, take a moment to make yourself familiar with the required node sizing and other considerations for a performant and stable cluster operation.</p> <p> Deployment Planning</p> </li> <li> <p> Deploy Simplyblock</p> <p>Deploy simplyblock on Kubernetes, bare metal, or virtualized Linux machines. Choose between hyper-converged, disaggregated, or hybrid deployment models.</p> <p> Simplyblock Deployment</p> </li> <li> <p> Operate Simplyblock</p> <p>After the installation of a simplyblock cluster, learn how to operate and maintain it.</p> <p> Simplyblock Usage  Simplyblock Operations</p> </li> </ul>"},{"location":"#keep-updated","title":"Keep Updated","text":"<p>Sign up for our newsletter and keep updated on what's happening at simplyblock.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Simplyblock is a cloud-native, software-defined storage platform designed for high performance, scalability, and resilience. It provides NVMe over TCP (NVMe/TCP) block storage, enabling efficient data access across distributed environments. Understanding the architecture, key concepts, and common terminology is essential for effectively deploying and managing simplyblock in various infrastructure setups, including Kubernetes clusters, virtualized environments, and bare-metal deployments. This documentation provides a comprehensive overview of simplyblock\u2019s internal architecture, the components that power it, and the best practices for integrating it into your storage infrastructure.</p> <p>This section covers several critical topics, including the architecture of simplyblock, core concepts such as Logical Volumes (LVs), Storage Nodes, and Management Nodes, as well as Quality of Service (QoS) mechanisms and redundancy strategies. Additionally, we define common terminology used throughout the documentation to ensure clarity and consistency. Readers will also find guidelines on document conventions, such as formatting, naming standards, and command syntax, which help maintain uniformity across all technical content.</p> <p>Simplyblock is an evolving platform, and community contributions play a vital role in improving its documentation. Whether you are a developer, storage administrator, or end user, your insights and feedback are valuable. This section provides details on how to contribute to the documentation, report issues, suggest improvements, and submit pull requests. By working together, we can ensure that simplyblock\u2019s documentation remains accurate, up-to-date, and beneficial for all users.</p>"},{"location":"architecture/high-availability-fault-tolerance/","title":"High Availability and Fault Tolerance","text":"<p>Simplyblock is designed to provide enterprise-grade high availability (HA) and fault tolerance for enterprise and cloud-native storage environments. Through a combination of distributed architecture and advanced data protection mechanisms, simplyblock ensures continuous data access, resilience against failures, and minimal service disruption. Fault tolerance is embedded at multiple levels of the system, from data redundancy to control plane and storage path resilience.</p>"},{"location":"architecture/high-availability-fault-tolerance/#fault-tolerance-and-high-availability-mechanisms","title":"Fault Tolerance and High Availability Mechanisms","text":"<p>Simplyblock\u2019s architecture provides robust fault tolerance and high availability by combining distributed erasure coding, multipath access with failover, and redundant management and storage planes. These capabilities ensure that Simplyblock storage clusters deliver the reliability and resiliency required for critical, high-demand workloads in modern distributed environments.</p>"},{"location":"architecture/high-availability-fault-tolerance/#1-distributed-erasure-coding","title":"1. Distributed Erasure Coding","text":"<p>Simplyblock protects data using distributed erasure coding, which ensures that data is striped across multiple storage nodes along with parity fragments. This provides:</p> <ul> <li>Redundancy: Data can be reconstructed even if one or more nodes fail, depending on the configured erasure coding   scheme (such as 1+1, 1+2, 2+1, or 2+2).</li> <li>Efficiency: Storage overhead is minimized compared to full replication while maintaining strong fault tolerance.</li> <li>Automatic Rebuilds: In the event of node or disk failures, missing data is rebuilt automatically using parity   information stored across the cluster.</li> </ul>"},{"location":"architecture/high-availability-fault-tolerance/#2-multipathing-with-primary-and-secondary-nodes","title":"2. Multipathing with Primary and Secondary Nodes","text":"<p>Simplyblock supports NVMe-over-Fabrics (NVMe-oF) multipathing to provide path redundancy between clients and storage:</p> <ul> <li>Primary and Secondary Paths: Each Logical Volume (LV) is accessible through both a primary node and one or   more secondary nodes.</li> <li>Automatic Failover: If the primary node becomes unavailable, traffic is automatically redirected to a secondary   node with minimal disruption.</li> <li>Load Balancing: Multipathing also distributes I/O across available paths to optimize performance and reliability.</li> </ul>"},{"location":"architecture/high-availability-fault-tolerance/#3-redundant-control-plane-and-storage-plane","title":"3. Redundant Control Plane and Storage Plane","text":"<p>To ensure cluster-wide availability, Simplyblock operates with full redundancy in both its control plane and storage plane:</p> <ul> <li> <p>Control Plane (Management Nodes):</p> <ul> <li>Deployed as a highly available set of management nodes, typically in a quorum-based configuration.</li> <li>Responsible for cluster health, topology management, and coordination.</li> <li>Remains operational even if one or more management nodes fail.</li> </ul> </li> <li> <p>Storage Plane (Storage Nodes):</p> <ul> <li>Storage services are distributed across multiple storage nodes.</li> <li>Data and workloads are automatically rebalanced and protected in case of node or device failures.</li> <li>Failures are handled transparently with automatic recovery processes.</li> </ul> </li> </ul>"},{"location":"architecture/high-availability-fault-tolerance/#benefits-of-simplyblocks-high-availability-design","title":"Benefits of Simplyblock\u2019s High Availability Design","text":"<ul> <li>No single point of failure across the control plane, storage plane, and data paths.</li> <li>Seamless failover and recovery from node, network, or disk failures.</li> <li>Efficient use of storage capacity while ensuring redundancy through erasure coding.</li> <li>Continuous operation during maintenance and upgrade procedures.</li> </ul>"},{"location":"architecture/simplyblock-architecture/","title":"Simplyblock Architecture","text":"<p>Simplyblock is a cloud-native, distributed block storage platform designed to deliver scalable, high-performance, and resilient storage through a software-defined architecture. Centered around NVMe-over-Fabrics (NVMe-oF), simplyblock separates compute and storage to enable scale-out elasticity, high availability, and low-latency operations in modern, containerized environments. The architecture is purpose-built to support Kubernetes-native deployments with seamless integration, but supports virtual and even physical machines as clients as well.</p>"},{"location":"architecture/simplyblock-architecture/#control-plane","title":"Control Plane","text":"<p>The control plane hosts the Simplyblock Management API and CLI endpoints with identical features. The CLI is equally available on all management nodes. The API and CLI are secured using HTTPS / TLS.</p> <p>The control plane operates through redundant management nodes that handle cluster health, metadata, and orchestration. A quorum-based model ensures no single point of failure.</p>"},{"location":"architecture/simplyblock-architecture/#control-plane-responsibilities","title":"Control Plane Responsibilities","text":"<p>The control plane provides the following functionality:</p> <ul> <li>Lifecycle management of clusters:<ul> <li>Deploy storage clusters</li> <li>Manages nodes and devices</li> <li>Resize and reconfigure clusters</li> </ul> </li> <li>Lifecycle management of logical volumes and pools<ul> <li>For Kubernetes, the Simplyblock CSI driver integrates with the persistent volume lifecycle management</li> </ul> </li> <li>Cluster operations<ul> <li>I/O Statistics</li> <li>Capacity Statistics</li> <li>Alerts</li> <li>Logging</li> <li>others</li> </ul> </li> </ul> <p>The control plane also provides real-time collection and aggregation of I/O stats (performance, capacity, utilization), proactive cluster monitoring and health checks, monitoring dashboards, alerting, a log file repository with a management interface, data migration, and automated node and device restart services.</p> <p>For monitoring dashboards and alerting, the simplyblock control plane provides Grafana and Prometheus. Both systems are configured to provide a set of standard alerts that can be delivered via Slack or email. Additionally, customers are free to define their own custom alerts.</p> <p>For log management, simplyblock uses Graylog. For a comprehensive insight, Graylog is configured to collect container logs from the control plane and storage plane services, the RPC communication between the control plane and storage cluster and the data services logs (SPDK\u00a0\u29c9 or Storage Performance Development Kit).</p>"},{"location":"architecture/simplyblock-architecture/#control-plane-state-storage","title":"Control Plane State Storage","text":"<p>The control plane is implemented as a stack of containers running on one or more management nodes. For production environments, simplyblock requires at least three management nodes for high availability. The management nodes run as a set of replicated, stateful services.</p> <p>For internal state storage, the control plane uses (FoundationDB\u00a0\u29c9) as its key-value store. FoundationDB, by itself, operates in a replicated high-available cluster across all management nodes.</p>"},{"location":"architecture/simplyblock-architecture/#storage-plane","title":"Storage Plane","text":"<p>The storage plane consists of distributed storage nodes that run on Linux-based systems and provide logical volumes ( LVs) as virtual NVMe devices. Using SPDK and DPDK (Data Plane Development Kit), simplyblock achieves high-speed, user-space storage operations with minimal latency.</p> <p>To achieve that, simplyblock detaches NVMe devices from the Linux kernel, bypassing the typical kernel-based handling. It then takes full control of the device directly, handling all communication with the hardware in user-space. That removes transitions from user-space to kernel and back, improving latency and reducing processing time and context switches.</p>"},{"location":"architecture/simplyblock-architecture/#scaling-and-performance","title":"Scaling and Performance","text":"<p>Simplyblock supports linear scale-out by adding storage nodes without service disruption. Performance increases with additional cores, network interfaces, and NVMe devices, with SPDK minimizing CPU overhead for maximum throughput.</p> <p>Data written to a simplyblock logical volume is split into chunks and distributed across the storage plane cluster nodes. This improves throughput by parallelizing the access to data through multiple storage nodes.</p>"},{"location":"architecture/simplyblock-architecture/#data-protection-fault-tolerance","title":"Data Protection &amp; Fault Tolerance","text":"<p>Simplyblock's storage engine implements erasure coding, a RAID-like system, which uses parity information to protect data and restore it in case of a failure. Due to the fully distributed nature of simplyblock's erasure coding implementation, parity information is not only stored on disks other than the original data chunk, but also on other nodes. This improves data protection and enables higher fault tolerance than typical implementations. While most erasure coding implementations provide a Maximum Tolerable Failure (MFT) in terms of how many disks can fail, simplyblock defines it as the number of nodes that can fail.</p> <p>As a second layer, simplyblock leverages NVMe-oF multipathing to ensure continuous access to logical volumes by automatically handling failover between primary and secondary nodes. Each volume is presented with multiple active paths, allowing I/O operations to seamlessly reroute through secondary nodes if the primary node becomes unavailable due to failure, maintenance, or network disruption. This multipath configuration is managed transparently by the NVMe-oF subsystem, providing path redundancy, eliminating single points of failure, and maintaining high availability without requiring manual intervention. The system continuously monitors path health, and when the primary path is restored, it can be automatically reintegrated, ensuring optimal performance and reliability.</p> <p>Last, simplyblock provides robust encryption for data-at-rest, ensuring that all data stored on logical volumes is protected using industry-standard AES_XTS encryption with minimal performance overhead. This encryption is applied at the volume level and is managed transparently within the simplyblock cluster, allowing compliance with strict regulatory requirements such as GDPR, HIPAA, and PCI-DSS. Furthermore, simplyblock\u2019s architecture is designed for strong multitenant isolation, ensuring that encryption keys, metadata, and data are securely segregated between tenants. This guarantees that unauthorized access between workloads and users is prevented, making simplyblock an ideal solution for shared environments where security, compliance, and tenant separation are critical.</p>"},{"location":"architecture/simplyblock-architecture/#technologies-in-simplyblock","title":"Technologies in Simplyblock","text":"<p>Building strong and reliable distributed storage technology has to build on a strong foundation. That's why simplyblock uses a variety of open-source key technologies as its basis.</p> Component Technologies Networking NVMe-oF\u00a0\u29c9, NVMe/TCP, NVMe/RoCE, DPDK\u00a0\u29c9 Storage SPDK\u00a0\u29c9, FoundationDB\u00a0\u29c9, MongoDB\u00a0\u29c9 Observability Prometheus\u00a0\u29c9, Thanos\u00a0\u29c9, Grafana\u00a0\u29c9 Logging Graylog\u00a0\u29c9, OpenSearch\u00a0\u29c9 Kubernetes SPDK CSI\u00a0\u29c9, Kubernetes CSI\u00a0\u29c9 Operating System Linux\u00a0\u29c9"},{"location":"architecture/what-is-simplyblock/","title":"What is Simplyblock?","text":"<p>Simplyblock is a high-performance, distributed storage orchestration layer designed for cloud-native environments. It provides NVMe over TCP (NVMe/TCP) block storage to hosts and offers block storage to containers through its Container Storage Interface (CSI) driver.</p>"},{"location":"architecture/what-is-simplyblock/#what-makes-simplyblock-special","title":"What makes Simplyblock Special?","text":"<ul> <li>Environment Agnostic: Simplyblock operates seamlessly across major cloud providers, regional, and specialized   providers, bare-metal and virtual provisioners, and private clouds, including both virtualized and bare-metal</li> <li> <p>Kubernetes environments.</p> </li> <li> <p>Containerized Architecture: The solution comprises:</p> <ul> <li>Storage Nodes: Container stacks delivering distributed data services via NVMe over Fabrics (NVMe over TCP),   forming storage clusters.</li> <li>Management Nodes: Container stacks offering control and management services, collectively known as the control   plane.</li> </ul> </li> <li> <p>Platform Support: Simplyblock supports deployment on virtual machines, bare-metal instances, and Kubernetes   containers, compatible with x86 and ARM architectures.</p> </li> <li> <p>Deployment Flexibility: Simplyblock offers the greatest deployment flexibility in the industry. It can be deployed   hyper-converged, disaggregated, and in a hybrid fashion, combining the best of both worlds.</p> </li> </ul>"},{"location":"architecture/what-is-simplyblock/#customer-benefits-across-industries","title":"Customer Benefits Across Industries","text":"<p>Simplyblock offers tailored advantages to various sectors:</p> <ul> <li> <p>Financial Services: Enhances data management by boosting performance, strengthening security, and optimizing cloud   storage costs.</p> </li> <li> <p>Media and Gaming: Improves storage performance, reduces costs, and streamlines data management, facilitating   efficient handling of large media files and gaming data.</p> </li> <li> <p>Technology and SaaS Companies: Provides cost savings and performance enhancements, simplifying storage management   and improving application performance without significant infrastructure changes.</p> </li> <li> <p>Telecommunications: Offers ultra-low-latency access to data, enhances security, and simplifies complex storage   infrastructures, aiding in the efficient management of customer records and network telemetry.</p> </li> <li> <p>Blockchain and Cryptocurrency: Delivers cost efficiency, enhanced performance, scalability, and robust data   security, addressing the unique storage demands of blockchain networks.</p> </li> </ul>"},{"location":"architecture/concepts/","title":"Concepts","text":"<p>Understanding the fundamental concepts behind simplyblock is essential for effectively utilizing its distributed storage architecture. Simplyblock provides a cloud-native, software-defined storage solution that enables highly scalable, high-performance storage for containerized and virtualized environments. By leveraging NVMe over TCP (NVMe/TCP) and advanced data management features, simplyblock ensures low-latency access, high availability, and seamless scalability. This documentation section provides detailed explanations of key storage concepts within simplyblock, helping users understand how its storage components function and interact within a distributed system.</p> <p>The concepts covered in this section include Logical Volumes (LVs), Snapshots, Clones, Hyper-Convergence, Disaggregation, and more. Each concept plays is crucial in optimizing storage performance, ensuring data durability, and enabling efficient resource allocation. Whether you are deploying simplyblock in a Kubernetes environment, a virtualized infrastructure, or a bare-metal setup, understanding these core principles will help you design, configure, and manage your storage clusters effectively.</p> <p>By familiarizing yourself with these concepts, you will gain insight into how simplyblock abstracts storage resources, provides scalable and resilient data services, and integrates with modern cloud-native environments. This knowledge is essential for leveraging simplyblock to meet your organization's storage performance, reliability, and scalability requirements.</p>"},{"location":"architecture/concepts/automatic-rebalancing/","title":"Automatic Rebalancing","text":"<p>Automatic rebalancing is a fundamental feature of distributed data storage systems designed to maintain an even distribution of data across storage nodes. This process ensures optimal performance, prevents resource overutilization, and enhances system resilience by dynamically redistributing data in response to changes in cluster topology or workload patterns.</p> <p>In a distributed storage system, data is typically spread across multiple storage nodes for redundancy, scalability, and performance. Over time, various factors can lead to an imbalance in data distribution, such as:</p> <ul> <li>The addition of new storage nodes, which initially lack any data.</li> <li>The removal or failure of existing nodes, requiring data redistribution to maintain availability.</li> <li>The equal distribution of data across storage nodes.</li> </ul> <p>Automatic rebalancing addresses these issues by dynamically redistributing data across the cluster. This process is driven by an algorithm that continuously monitors data distribution and redistributes data when imbalances are detected. The goal is to achieve uniform data placement while minimizing performance overhead during the rebalancing process.</p>"},{"location":"architecture/concepts/disaggregated/","title":"Disaggregated","text":"<p>Disaggregated storage represents a modern approach to distributed storage architectures, where compute and storage resources are decoupled. This separation allows for greater flexibility, scalability, and efficiency in managing data across large-scale distributed environments.</p> <p>Traditional storage architectures typically integrate compute and storage within the same nodes, leading to resource contention and inefficiencies. Disaggregated storage solutions address these limitations by separating storage resources from compute resources, enabling independent scaling of each component based on workload demands.</p> <p>Key characteristics of disaggregated storage solutions include:</p> <ul> <li>Independent Scalability: Compute and storage can be scaled separately, optimizing resource utilization and   reducing unnecessary hardware expansion.</li> <li>Resource Efficiency: Storage is pooled and accessible across multiple compute nodes, reducing data duplication and   improving overall efficiency.</li> <li>Improved Performance: By reducing bottlenecks associated with tightly coupled storage, applications can achieve   better latency and throughput.</li> <li>Flexibility and Adaptability: Different storage technologies (e.g., NVMe-over-Fabrics, object storage) can be   integrated seamlessly, allowing organizations to adopt the best-fit storage solutions for specific workloads.</li> <li>Simplified Management: Centralized storage management reduces complexity, enabling easier provisioning,   monitoring, and maintenance of storage resources.</li> </ul>"},{"location":"architecture/concepts/erasure-coding/","title":"Erasure Coding","text":"<p>Erasure coding is a data protection mechanism used in distributed storage systems to enhance fault tolerance and optimize storage efficiency. It provides redundancy by dividing data into multiple fragments and encoding it with additional parity fragments, enabling data recovery in the event of node failures.</p> <p>Traditional data redundancy methods, such as replication, require multiple full copies of data, leading to significant storage overhead. Erasure coding improves upon this by using mathematical algorithms to generate parity fragments, allowing data reconstruction with fewer overheads.</p> <p>The core principle of erasure coding involves breaking data into k data fragments and computing m parity fragments. These k+m fragments are distributed across multiple storage nodes. The system can recover lost data using any k available fragments, even if up to m fragments are missing or corrupted.</p> <p>Erasure coding has a number of key characteristics:</p> <ul> <li>High Fault Tolerance: Erasure coding can tolerate multiple node failures while allowing full data recovery.</li> <li>Storage Efficiency: Compared to replication, erasure coding requires less additional storage to achieve similar levels of redundancy.</li> <li>Computational Overhead: Encoding and decoding operations involve computational complexity, which may impact performance in latency-sensitive applications.</li> <li>Flexibility: The parameters k and m can be adjusted to balance redundancy, performance, and storage overhead.</li> </ul>"},{"location":"architecture/concepts/hyper-converged/","title":"Hyper-Converged","text":"<p>Hyper-converged storage is a key component of hyper-converged infrastructure (HCI), where compute, storage, and networking resources are tightly integrated into a unified system. This approach simplifies management, enhances scalability, and optimizes resource utilization in distributed data storage environments.</p> <p>Traditional storage architectures often separate compute and storage into distinct hardware layers, requiring complex management and specialized hardware. Hyper-converged storage consolidates these resources within the same nodes, forming a software-defined storage (SDS) layer that dynamically distributes and manages data across the cluster.</p> <p>Key characteristics of hyper-converged storage include:</p> <ul> <li>Integrated Storage and Compute: Storage resources are virtualized and distributed across the compute nodes,   eliminating the need for dedicated storage arrays.</li> <li>Scalability: New nodes can be added seamlessly, increasing both compute and storage capacity without complex   reconfiguration.</li> <li>Software-Defined Storage (SDS): A software layer abstracts and manages storage resources, enabling automation,   fault tolerance, and efficiency.</li> <li>High Availability and Resilience: Data is replicated across nodes to ensure redundancy and fault tolerance,   minimizing downtime.</li> <li>Simplified Management: A unified management interface enables streamlined provisioning, monitoring, and   maintenance of storage and compute resources.</li> </ul>"},{"location":"architecture/concepts/logical-volumes/","title":"Logical Volumes","text":"<p>Logical Volumes (LVs) in Simplyblock are virtual NVMe devices that provide scalable, high-performance storage within a distributed storage cluster. They enable flexible storage allocation, efficient resource utilization, and seamless data management for cloud-native applications.</p> <p>A Logical Volume (LV) in simplyblock is an abstracted storage entity dynamically allocated from a storage pool managed by the simplyblock system. Unlike traditional block storage, simplyblock\u2019s LVs offer advanced features such as thin provisioning, snapshotting, and replication to enhance resilience and scalability.</p> <p>Key characteristics of Logical Volumes include:</p> <ul> <li>Dynamic Allocation: LVs can be created, resized, and deleted on demand without manual intervention in the   underlying hardware.</li> <li>Thin Provisioning: Storage space is allocated only when needed, optimizing resource utilization.</li> <li>High Performance: Simplyblock\u2019s architecture ensures low-latency access to LVs, making them suitable for demanding   workloads.</li> <li>Fault Tolerance: Data is distributed across multiple nodes to prevent data loss and improve reliability.</li> <li>Integration with Kubernetes: LVs can be used as persistent storage for Kubernetes workloads, enabling seamless   stateful application management.</li> </ul>"},{"location":"architecture/concepts/persistent-volumes/","title":"Persistent Volumes","text":"<p>Persistent Volumes (PVs) in Kubernetes provide a mechanism for managing storage resources independently of individual Pods. Unlike ephemeral storage, which is tied to the lifecycle of a Pod, PVs ensure data persistence across Pod restarts and rescheduling, enabling stateful applications to function reliably in a Kubernetes cluster.</p> <p>In Kubernetes, storage resources are abstracted through the Persistent Volume framework, which decouples storage provisioning from application deployment. A Persistent Volume (PV) represents a piece of storage that has been provisioned in the cluster, while a Persistent Volume Claim (PVC) is a request for storage made by an application.</p> <p>Key characteristics of Persistent Volumes include:</p> <ul> <li>Decoupled Storage Management: PVs exist independently of Pods, allowing storage to persist even when Pods are deleted or rescheduled.</li> <li>Dynamic and Static Provisioning: Storage can be provisioned manually by administrators (static provisioning) or automatically by storage classes (dynamic provisioning).</li> <li>Access Modes: PVs support multiple access modes, such as ReadWriteOnce (RWO), ReadOnlyMany (ROX), and ReadWriteMany (RWX), defining how storage can be accessed by Pods.</li> <li>Reclaim Policies: When a PV is no longer needed, it can be retained, recycled, or deleted based on its configured reclaim policy.</li> <li>Storage Classes: Kubernetes allows administrators to define different types of storage using StorageClasses, enabling automated provisioning of PVs based on workload requirements.</li> </ul>"},{"location":"architecture/concepts/simplyblock-cluster/","title":"Simplyblock Cluster","text":"<p>The simplyblock storage platform consists of three different types of cluster nodes and belongs to the control plane or storage plane.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#control-plane","title":"Control Plane","text":"<p>The control plane orchestrates, monitors, and controls the overall storage infrastructure. It provides centralized administration, policy enforcement, and automation for managing storage nodes, logical volumes (LVs), and cluster-wide configurations. The control plane operates independently of the storage plane, ensuring that control and metadata operations do not interfere with data processing. It facilitates provisioning, fault management, and system scaling while offering APIs and CLI tools for seamless integration with external management systems. A single control plane can manage multiple clusters.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#storage-plane","title":"Storage Plane","text":"<p>The storage plane is the layer responsible for managing and distributing data across storage nodes within a cluster. It handles data placement, replication, fault tolerance, and access control, ensuring that logical volumes (LVs) provide high-performance, low-latency storage to applications. The storage plane operates independently of the control plane, allowing seamless scalability and dynamic resource allocation without disrupting system operations. By leveraging NVMe-over-TCP and software-defined storage principles, simplyblock\u2019s storage plane ensures efficient data distribution, high availability, and resilience, making it ideal for cloud-native and high-performance computing environments.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#management-node","title":"Management Node","text":"<p>A management node is a node of the control plane cluster. The management node runs the necessary management services including the Cluster API, services such as Grafana, Prometheus, and Graylog, as well as the FoundationDB database cluster.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#storage-node","title":"Storage Node","text":"<p>A storage node is a node of the storage plane cluster. The storage node provides storage capacity to the distributed storage pool of a specific storage cluster. The storage node runs the necessary data management services including the Storage Node Management API, the SPDK service, and handles logical volume primary connections of NVMe-oF multipathing.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#secondary-node","title":"Secondary Node","text":"<p>A secondary node is a node of the storage plane cluster. The secondary node provides automatic fail over and high availability for logical volumes using NVMe-oF multipathing. In a highly available cluster, simplyblock automatically provisions secondary nodes alongside primary nodes and assigns one secondary node per primary.</p>"},{"location":"architecture/concepts/snapshots-clones/","title":"Snapshots and Clones","text":"<p>Volume snapshots and volume clones are essential data management features in distributed storage systems that enable data protection, recovery, and replication. While both techniques involve capturing the state of a volume at a specific point in time, they serve distinct purposes and operate using different mechanisms.</p>"},{"location":"architecture/concepts/snapshots-clones/#volume-snapshots","title":"Volume Snapshots","text":"<p>A volume snapshot is a read-only, point-in-time copy of a storage volume. It preserves the state of the volume at the moment the snapshot is taken, allowing users to restore data or create new volumes based on the captured state. Snapshots are typically implemented using copy-on-write (COW) or redirect-on-write (ROW) techniques, minimizing storage overhead and improving efficiency.</p> <p>Key characteristics of volume snapshots include:</p> <ul> <li>Space Efficiency: Snapshots share common data blocks with the original volume, requiring minimal additional   storage.</li> <li>Fast Creation: As snapshots do not duplicate data immediately, they can be created almost instantaneously.</li> <li>Versioning and Recovery: Users can revert a volume to a previous state using snapshots, aiding in disaster   recovery and data protection.</li> <li>Performance Considerations: While snapshots are efficient, excessive snapshot accumulation can impact performance   due to metadata overhead and fragmentation.</li> </ul>"},{"location":"architecture/concepts/snapshots-clones/#volume-clones","title":"Volume Clones","text":"<p>A volume clone is a writable, independent copy of a storage volume, created from either an existing volume or a snapshot. Unlike snapshots, clones are fully functional duplicates that can operate as separate storage entities.</p> <p>Key characteristics of volume clones include:</p> <ul> <li>Writable and Independent: Clones can be modified without affecting the original volume.</li> <li>Use Case for Testing and Development: Clones are commonly used for staging environments, testing, and application   sandboxing.</li> <li>Storage Overhead: Unlike snapshots, clones may require additional storage capacity to accommodate changes made   after cloning.</li> <li>Immediate Availability: A clone provides an instant copy of the original volume, avoiding long data copying   processes.</li> </ul>"},{"location":"architecture/concepts/storage-pooling/","title":"Storage Pooling","text":"<p>Storage pooling is a technique used in distributed data storage systems to aggregate multiple storage devices into a single, unified storage resource. This approach enhances resource utilization, improves scalability, and simplifies management by abstracting physical storage infrastructure into a logical storage pool.</p> <p>Traditional storage architectures often rely on dedicated storage devices assigned to specific applications or workloads, leading to inefficiencies in resource allocation and potential underutilization. Storage pooling addresses these challenges by combining storage resources from multiple nodes into a shared pool, allowing dynamic allocation based on demand.</p> <p>Key characteristics of storage pooling include:</p> <ul> <li>Resource Aggregation: Multiple physical storage devices, such as HDDs, SSDs, or NVMe drives, are combined into a single logical storage entity.</li> <li>Dynamic Allocation: Storage capacity can be allocated dynamically to workloads based on usage patterns and demand.</li> <li>Improved Efficiency: By eliminating the constraints of static storage assignments, storage pooling optimizes resource utilization and reduces wasted capacity.</li> <li>Scalability: Additional storage devices or nodes can seamlessly integrate into the storage pool without disrupting operations.</li> <li>Simplified Management: Centralized control and monitoring enable streamlined administration of storage resources.</li> </ul>"},{"location":"deployments/","title":"Deployments","text":"<p>Simplyblock is a highly flexible storage solution. It can be installed in a variety of different deployment models inside and outside of Kubernetes.</p> <p>Info</p> <p>Bare Metal and VM-based installations refer to installations onto a dedicated or virtual machine running Linux deployed on premises, or in a private or public cloud. For AWS EC2-based installation, there is a specific installation guide that is recommended to follow if an AWS EC2 installation is planned.</p> <ul> <li> <p> Kubernetes</p> <p>Kubernetes deployments include AWS' EKS and GCP's GKE.</p> <p> Systems Recommendations  Node Sizing  Erasure Coding Configuration  Prerequisites  Hyper-Converged Setup  Disaggregated Setup  Hybrid Setup</p> </li> <li> <p> Bare-Metal &amp; VM-based Linux</p> <p>Bare-Metal deployments are either virtualized or physical host.</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Install Storage Cluster  Install Kubernetes CSI Driver</p> </li> <li> <p> AWS EC2</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Install Storage Cluster  Install Kubernetes CSI Driver</p> </li> <li> <p> Google Compute Engine</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Install Storage Cluster  Install Kubernetes CSI Driver</p> </li> <li> <p> Air Gapped</p> <p> General Information</p> </li> </ul>"},{"location":"deployments/air-gap/","title":"Air Gap Installation","text":"<p>Simplyblock can be installed in an air-gapped environment. However, the necessary images must be downloaded to install and run the control plane, the storage nodes, and the Kubernetes CSI driver. In addition, for Kubernetes deployments, you want to download or clone the  simplyblock helm repository\u00a0\u29c9 which contains the helm charts for Kubernetes-based storage and caching nodes, as well as the Kubernetes CSI driver.</p> <p>For an air-gapped installation, we recommend an air-gapped container repository installation. Tools such as JFrog Artifactory\u00a0\u29c9 or Sonatype Nexus\u00a0\u29c9 help with the setup and management of container images in air-gapped environments.</p> <p>The general installation instructions are similar to non-air-gapped installations, with the need to update the container download locations to point to your local container repository.</p> <p>Learn more about the deployment options:</p> <ul> <li>Deploy into Kubernetes</li> <li>Deploy on Bare Metal (or virtualized) Linux</li> <li>Deploy on AWS EC2</li> </ul>"},{"location":"deployments/aws-ec2/","title":"AWS EC2 (Amazon Linux)","text":"<p>An installation on Amazon EC2 is mostly comparable to a bare-metal or virtualized installation. There are, however, a few minor differences. Hence, we decided to give it a separate documentation section.</p> <p>An installation on Amazon EKS is comparable to a standard Kubernetes installation. The differences between an Amazon EKS installation and a basic Kubernetes</p> <p>Warning</p> <p>Amazon Linux 2 and Amazon Linux 2023 do not support NVMe over Fabrics Multipathing!</p> <ul> <li> <p> Prerequisites</p> <p> Node Sizing  Prerequisites</p> </li> <li> <p> Simplyblock Installation</p> <p> Install Simplyblock</p> </li> <li> <p> Kubernetes CSI Driver Installation</p> <p> Install Kubernetes CSI Driver</p> </li> <li> <p> Caching Node Installation</p> <p> Install Caching Nodes</p> </li> </ul>"},{"location":"deployments/aws-ec2/data-migration/","title":"Data Migration","text":"<p>When migrating existing data to simplyblock, the process can be performed at the block level or the file system level, depending on the source system and migration requirements. Because Simplyblock provides logical Volumes (LVs) as virtual block devices, data can be migrated using standard block device cloning tools such as <code>dd</code>, as well as file-based tools like <code>rsync</code> after the block device has been formatted.</p> <p>Therefore, sata migration to simplyblock is a straightforward process using common block-level and file-level tools. For full disk cloning, <code>dd</code> and similar utilities are effective. For selective file migrations, <code>rsync</code> provides flexibility and reliability. Proper planning and validation of available storage capacity are essential to ensure successful and complete data transfers.</p>"},{"location":"deployments/aws-ec2/data-migration/#block-level-migration-using-dd","title":"Block-Level Migration Using <code>dd</code>","text":"<p>A block-level copy duplicates the entire content of a source block device, including partition tables, file systems, and data. This method is ideal when migrating entire disks or volumes.</p> Creating a block-level clone of a block device<pre><code>dd if=/dev/source-device of=/dev/simplyblock-device bs=4M status=progress\n</code></pre> <ul> <li><code>if=</code> specifies the input (source) device.</li> <li><code>of=</code> specifies the output (Simplyblock Logical Volume) device.</li> <li><code>bs=4M</code> sets the block size for efficiency.</li> <li><code>status=progress</code> provides real-time progress updates.</li> </ul> <p>Info</p> <p>Ensure that the simplyblock logical volume is at least as large as the source device to prevent data loss.</p>"},{"location":"deployments/aws-ec2/data-migration/#alternative-block-level-cloning-tools","title":"Alternative Block-Level Cloning Tools","text":"<p>Other block-level tools such as <code>Clonezilla</code>, <code>partclone</code>, or <code>dcfldd</code> may also be used for disk duplication, depending on the specific environment and desired features like compression or network transfer.</p>"},{"location":"deployments/aws-ec2/data-migration/#file-level-migration-using-rsync","title":"File-Level Migration Using <code>rsync</code>","text":"<p>For scenarios where only file contents need to be migrated (for example, after creating a new file system on a simplyblock logical volume), <code>rsync</code> is a reliable tool.</p> <ol> <li> <p>First, format the Simplyblock Logical Volume:    </p>Format the simplyblock block device with ext4<pre><code>mkfs.ext4 /dev/simplyblock-device\n</code></pre> </li> <li> <p>Mount the Logical Volume:    </p>Mount the block device<pre><code>mount /dev/simplyblock-device /mnt/simplyblock\n</code></pre> </li> <li> <p>Use <code>rsync</code> to copy files from the source directory:    </p>Synchronize the source disks content using rsync<pre><code>rsync -avh --progress /source/data/ /mnt/simplyblock/\n</code></pre> <ul> <li><code>-a</code> preserves permissions, timestamps, and symbolic links.</li> <li><code>-v</code> provides verbose output.</li> <li><code>-h</code> makes output human-readable.</li> <li><code>--progress</code> shows transfer progress.</li> </ul> </li> </ol>"},{"location":"deployments/aws-ec2/data-migration/#minimal-downtime-migration-strategy","title":"Minimal-Downtime Migration Strategy","text":"<p>An alternative, but more complex solution enables minimal downtime. This option utilizes the Linux <code>dm</code> (Device Mapper) subsystem.</p> <p>Using the Device Mapper, the current and new block devices will be moved into a RAID-1 and synchronized (re-silvered) in the background.  This solution requires two minimal downtimes to create and remount the devices.</p> <p>Warning</p> <p>This method is quite involved, requires a lot of steps, and can lead to data loss in case of wrong commands or parameters. It should only be used by advanced users that understand the danger of the commands below. Furthermore, this migration method MUST NOT be used for boot devices!</p> <p>In this walkthrough, we assume the new simplyblock logical volume is already connected to the system.</p>"},{"location":"deployments/aws-ec2/data-migration/#preparation","title":"Preparation","text":"<p>To successfully execute this data migration, a few values are required. First of all, the two device names of the currently used and new device need to be collected.</p> <p>This can be done by executing the command <code>lsblk</code> to list all attached block devices.</p> lsblk provides information about all attached block devices<pre><code>lsblk\n</code></pre> <p>In this example, sda is the boot device which hosts the operating system, while sdb is the currently used block device and nvme0n1 is the newly attached simplyblock logical volume. The latter two should be noted down.</p> <p>Danger</p> <p>It is important to understand the difference between the currently used and the new device. Using them in the wrong order in the following steps will cause any or all data to be lost!</p> Find the source and target block devices using lsblk<pre><code>[root@demo ~]# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0   25G  0 disk\n\u251c\u2500sda1                      8:1    0    1G  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0    2G  0 part  /boot\n\u2514\u2500sda3                      8:3    0 21.9G  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0   11G  0 lvm   /\nsdb                         8:16   0   25G  0 disk\n\u2514\u2500sdb1                      8:17   0   25G  0 part  /data/pg\nsr0                        11:0    1 57.4M  0 rom\nnvme0n1                   259:0    0   25G  0 disk\n</code></pre> <p>Next up the cluster size of the current device is required. The value must be set on the RAID to-be-created. It needs to be noted down.</p> Find the block size of the source filesystem<pre><code>tune2fs -l /dev/sdb1 | grep -i 'block size'\n</code></pre> <p>In this example, the block size is 4 KiB (4096 bytes).</p> Example output of the block size<pre><code>[root@demo ~]# tune2fs -l /dev/sdb1 | grep -i 'block size'\nBlock size:               4096\n</code></pre> <p>Last, it is important to ensure that the new target device is at least as large or larger than the current device. <code>lsblk</code> can be used again to get the required numbers.</p> lsblk with byte sizes of the block devices<pre><code>lsblk -b\n</code></pre> <p>In this example, both devices are the same size, 26843545600 bytes in total disk capacity.</p> Example output of lsblk -b<pre><code>[root@demo ~]# lsblk -b\nNAME                      MAJ:MIN RM        SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0 26843545600  0 disk\n\u251c\u2500sda1                      8:1    0  1127219200  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0  2147483648  0 part  /boot\n\u2514\u2500sda3                      8:3    0 23566745600  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0 11781799936  0 lvm   /\nsdb                         8:16   0 26843545600  0 disk\n\u2514\u2500sdb1                      8:17   0 26843513344  0 part  /data/pg\nsr0                        11:0    1    60225536  0 rom\nnvme0n1                   259:0    0 26843545600  0 disk\n</code></pre>"},{"location":"deployments/aws-ec2/data-migration/#device-mapper-raid-setup","title":"Device Mapper RAID Setup","text":"<p>Danger</p> <p>From here on out, mistakes can cause any or all data to be lost! It is strongly recommended to only go further, if ensured that the values above are correct and after a full data backup is created. It is also recommended to test the backup before continuing. A failure to do so can cause issues in case it cannot be replayed.</p> <p>Now, it's time to create the temporary RAID for disk synchronization. Anything beyond this point is dangerous.</p> <p>Warning</p> <p>Any service accessing the current block device or any of its partitions need to be shutdown and the block device and its partitions need to be unmounted. It is required for the device to not be busy. </p>Example of PostgreSQL shutdown and partition unmount<pre><code>service postgresql stop\numount /data/pg\n</code></pre> Building a RAID-1 with mdadm<pre><code>mdadm --build --chunk=&lt;CHUNK_SIZE&gt; --level=1 \\\n    --raid-devices=2 --bitmap=none \\\n    &lt;RAID_NAME&gt; &lt;CURRENT_DEVICE_FILE&gt; missing\n</code></pre> <p>In this example, the RAID is created using the /dev/sdb device file and 4096 as the chunk size. The newly created RAID is called migration. The RAID-level is 1 (meaning, RAID-1) and it includes 2 devices. The missing at the end of the command is required to tell the device mapper that the second device of the RAID is missing for now. It will be added later.</p> Example output of a RAID-1 with mdadm<pre><code>[root@demo ~]# mdadm --build --chunk=4096 --level=1 --raid-devices=2 --bitmap=none migration /dev/sdb missing\nmdadm: array /dev/md/migration built and started.\n</code></pre> <p>To ensure that the RAID was created successfully, all device files with /dev/md* can be listed. In this case, /dev/md127 is the actual RAID device, while /dev/md/migration is the device mapper file.</p> Finding the new device mapper device files<pre><code>[root@demo ~]# ls /dev/md*\n/dev/md127  /dev/md127p1\n\n/dev/md:\nmigration  migration1\n</code></pre> <p>After the RAID device name is confirmed, the new RAID device can be mounted. In this example, the original block device was partitioned. Hence, the RAID device also has one partition /dev/md127p1. This is what needs to be mounted to the same mount point as the original disk before, /data/pg in this example.</p> Mount the new device mapper device file<pre><code>[root@demo ~]# mount /dev/md127p1 /data/pg/\n</code></pre> <p>Info</p> <p>All services that require access to the data can be started again. The RAID itself is still in a degraded state, but it provides the same data security as the original device.</p> <p>Now the second, new device must be added to the RAID setup to start the re-silvering (data synchronization) process. This is again done using <code>mdadm</code> tool.</p> Add the new simplyblock block device to RAID-1<pre><code>mdadm &lt;RAID_DEVICE_MAPPER_FILE&gt; --add &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>In the example, we add /dev/nvme0n1 (the simplyblock logical volume) to the RAID named \"migration.\"</p> Example output of mdadm --add<pre><code>[root@demo ~]# mdadm /dev/md/migration --add /dev/nvme0n1\nmdadm: added /dev/nvme0n1\n</code></pre> <p>After the device was added to the RAID setup, a background process is automatically started to synchronize the newly added device to the first device in the setup. This process is called re-silvering.</p> <p>Info</p> <p>While the devices are synchronized, the read and write performance may be impacted due to the additional I/O operations of the synchronization process. However, the process runs on a very low priority and shouldn't impact the live operation too extensively. For AWS users: if the migration uses an Amazon EBS volume as the source, ensure enough IOPS to cover live operation and migration.</p> <p>The synchronization process status can be monitored using one of two commands:</p> Check status of re-silvering<pre><code>mdadm -D &lt;RAID_DEVICE_FILE&gt;\ncat /proc/mdstat\n</code></pre> Example output of a status check via mdadm<pre><code>[root@demo ~]#mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean, degraded, recovering\n    Active Devices : 1\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 1\n\nConsistency Policy : resync\n\n    Rebuild Status : 98% complete\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      spare rebuilding   /dev/nvme0n1\n</code></pre> Example output of a status check via /proc/mdstat<pre><code>[root@demo ~]# cat /proc/mdstat \nPersonalities : [raid1] \nmd0 : active raid1 sdb[1] nvme0n1[0]\n      10484664 blocks super 1.2 [2/2] [UU]\n      [========&gt;............]  resync = 42.3% (4440832/10484664) finish=0.4min speed=201856K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>"},{"location":"deployments/aws-ec2/data-migration/#after-the-synchronization-is-done","title":"After the Synchronization is done","text":"<p>Eventually, the synchronization finishes. At this point, the two devices (original and new) are kept in sync by the device mapper system.</p> Example out of a finished synchronzation<pre><code>[root@demo ~]# mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean\n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      active sync   /dev/nvme0n1\n</code></pre> <p>To fully switch to the new simplyblock logical volume, a second, minimal, downtime is required.</p> <p>The RAID device needs to be unmounted and the device mapper stopped.</p> Stopping the device mapper RAID-1<pre><code>umount &lt;MOUNT_POINT&gt;\nmdadm --stop &lt;DEVICE_MAPPER_FILE&gt;\n</code></pre> <p>In this example /data/pg and /dev/md/migration are used.</p> Example output of a stopped RAID-1<pre><code>[root@demo ~]# umount /data/pg/\n[root@demo ~]# mdadm --stop /dev/md/migration\nmdadm: stopped /dev/md/migration\n</code></pre> <p>Now, the system should be restarted. If a system reboot takes too long and is out of the scope of the available maintenance window, a re-read of the partition tables can be forced.</p> Re-read partition table<pre><code>blockdev --rereadpt &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>After re-reading the partition table of a device, the partition should be recognized and visible.</p> Example output of re-reading the partition table<pre><code>[root@demo ~]# blockdev --rereadpt /dev/nvme0n1\n[root@demo ~]# ls /dev/nvme0n1p1\n/dev/nvme0n1p1\n</code></pre> <p>As a last step, the partition must be mounted to the same mount point as the RAID device before. If the mount is successful, the services can be started again.</p> Mounting the plain block device and restarting services<pre><code>[root@demo ~]# mount /dev/nvme0n1p1 /data/pg/\n[root@demo ~]# service postgresql start\n</code></pre>"},{"location":"deployments/aws-ec2/install-caching-nodes/","title":"Install Caching Nodes (Kubernetes)","text":"<p>Caching nodes are simplyblock storage containers co-located with the workloads on Kubernetes workers. They utilize directly attached NVMe disks on the worker nodes to provide an ultra-low-latency write-through cache for disaggregated or hybrid simplyblock clusters.</p>"},{"location":"deployments/aws-ec2/install-caching-nodes/#prerequisites","title":"Prerequisites","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>Caching nodes, like storage nodes, require huge page memory to hold the internal state. Huge pages should be 2MiB in size, and a minimum of 4096 huge pages should be allocated at boot time of the operating system.</p> <pre><code>demo@worker-1 ~&gt; sudo sysctl -w vm.nr_hugepages=4096\n</code></pre> <p>Info</p> <p>To see how huge pages can be pre-reserved at boot time, see the node sizing documentation section on Huge Pages.</p> <pre><code>demo@worker-1 ~&gt; sudo systemctl restart kubelet\n</code></pre> <pre><code>demo@worker-1 ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n</code></pre> <pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi      0 (0%)    0 (0%)\n</code></pre> <pre><code>demo@worker-1 ~&gt; sudo yum install -y nvme-cli\ndemo@worker-1 ~&gt; sudo modprobe nvme-tcp\ndemo@worker-1 ~&gt; sudo modprobe nbd\n</code></pre> <pre><code>demo@demo ~&gt; kubectl label nodes worker-1.kubernetes-cluster.local type=simplyblock-cache\n</code></pre> <p>Afterward, the worker node can be described to check that the node-selector type is successfully set.</p> Example output of a successfully configured worker node<pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep simplyblock-cache\n                    type=simplyblock-cache\n</code></pre> Example Caching Node-enabled StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  type: cache\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock-csi/","title":"Install Simplyblock CSI","text":"<p>Simplyblock provides a seamless integration with Kubernetes through its Kubernetes CSI driver.</p>"},{"location":"deployments/aws-ec2/install-simplyblock-csi/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p> <p>To install the Simplyblock CSI Driver, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended. If a manual installation is preferred, see the CSI Driver Repository\u00a0\u29c9.</p> <p>Either way, the installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying the CSI driver is the control plane address. It is recommended to front the simplyblock API with an AWS load balancer service. Hence, your control plane address is the \"public\" endpoint of this load balancer.</p> <p>Anyhow, deploying the Simplyblock CSI Driver using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver.</p> Install Simplyblock's CSI Driver<pre><code>CLUSTER_UUID=\"&lt;UUID&gt;\"\nCLUSTER_SECRET=\"&lt;SECRET&gt;\"\nCNTR_ADDR=\"&lt;CONTROL-PLANE-ADDR&gt;\"\nPOOL_NAME=\"&lt;POOL-NAME&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\nhelm repo update\nhelm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n    --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n    --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n    --set logicalVolume.pool_name=${POOL_NAME}\n</code></pre> Example output of the CSI driver deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/","title":"Install Simplyblock Storage Cluster","text":"<p>Installing simplyblock for production requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance, and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides two test scripts to automatically check your system's configuration. While those may not catch all edge cases, they can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configurations during the process.</p> Automatically check your configurations<pre><code># Configuration check for the control plane (management nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n\n# Configuration check for the storage plane (storage nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> <li>Secondary nodes are part of a specific storage cluster and enable automatic fail over for NVMe-oF connections. In a high-availability cluster, every primary storage node automatically provides a secondary storage node. </li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, and the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separate subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24 <p>Warning</p> <p>Simplyblock strongly recommends setting up individual networks for the storage plane and control plane traffic.  </p> <p>Recommendation</p> <p>Simplyblock recommends using AlmaLinux, Rocky, or Red Hat Enterprise Linux on Amazon EC2 machines. Amazon Linux does not support native NVMe-oF Multipathing. While it is possible to use DM-MPIO (device manager multipathing) usage is not as straight forward as possible.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#amazon-elastic-kubernetes-service-eks","title":"Amazon Elastic Kubernetes Service (EKS)","text":"<p>Info</p> <p>If simplyblock is to be installed into Amazon EKS, the Kubernetes documentation section has the necessary step-by-step guide.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets are available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Docker Swarm, by default, creates iptables entries open to the world. If no external firewall is available, the created iptables configuration needs to be restricted.</p> <p>The following script will create additional iptables rules prepended to Docker's forwarding rules and only enabling access from internal networks. This script should be stored in /usr/local/sbin/simplyblock-iptables.sh.</p> Configuration script for Iptables<pre><code>#!/usr/bin/env bash\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080:8890 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9090-9900 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre> <p>To automatically run this script whenever Docker is started or restarted, it must be attached to a Systemd service, stored as /etc/systemd/system/simplyblock-iptables.service.</p> Systemd script to set up Iptables<pre><code>[Unit]\nDescription=Simplyblock Iptables Restrictions for Docker \nAfter=docker.service\nBindsTo=docker.service\nReloadPropagatedFrom=docker.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/simplyblock-iptables.sh\nExecReload=/usr/local/sbin/simplyblock-iptables.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>After both files are stored in their respective locations, the bash script needs to be made executable, and the Systemd service needs to be enabled to start automatically.</p> Enabling service file<pre><code>chmod +x /usr/local/sbin/simplyblock-iptables.sh\nsystemctl enable simplyblock-iptables.service\nsystemctl start simplyblock-iptables.service\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) are installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>With all NVMe devices prepared and the NVMe/TCP driver loaded, the storage node software can be deployed.</p> <p>The actual deployment process happens in three steps: - Creating the storage node configuration - Deploy the first stage (the storage node API) - Deploy the second stage (add the storage node to the cluster), happening from a management node</p> <p>The configuration process creates the configuration file, which contains all the assignments of NVMe devices, NICs, and potentially available NUMA nodes. By default, simplyblock will configure one storage node per NUMA node.</p> Configure the storage node<pre><code>sudo sbctl storage-node configure \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-size &lt;MAX_PROVISIONING_CAPACITY&gt;\n</code></pre> Example output of storage node configure<pre><code>[demo@demo-3 ~]# sudo sbctl sn configure --nodes-per-socket=2 --max-lvol=50 --max-size=1T\n2025-05-14 10:40:17,460: INFO: 0000:00:04.0 is already bound to nvme.\n0000:00:1e.0\n0000:00:1e.0\n0000:00:1f.0\n0000:00:1f.0\n0000:00:1e.0\n0000:00:1f.0\n2025-05-14 10:40:17,841: INFO: JSON file successfully written to /etc/simplyblock/sn_config_file\n2025-05-14 10:40:17,905: INFO: JSON file successfully written to /etc/simplyblock/system_info\nTrue\n</code></pre> <p>A full set of the parameters for the configure subcommand can be found in the CLI reference. </p> <p>After the configuration has been created, the first stage deployment can be executed </p> Deploy the storage node<pre><code>sudo sbctl storage-node deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbctl storage-node deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p> <p>When all storage nodes are added, it's finally time to activate the storage plane.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step, after all nodes are added to the storage cluster, is to activate the storage plane.</p> Storage cluster activation<pre><code>sudo sbctl cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this, and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbctl cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre> <p>Now that the cluster is ready, it is time to install the Kubernetes CSI Driver or learn how to use the simplyblock storage cluster to manually provision logical volumes.</p>"},{"location":"deployments/aws-ec2/prerequisites/","title":"Prerequisites","text":"<p>When installing simplyblock control planes and storage planes, a number of prerequisites are important to understand.</p> <p>Simplyblock uses Docker Swarm for the control plane cluster. In case of a bare metal or virtualized installation, it will also use Docker Swarm for the storage plane. Hence, Docker has to be installed.</p> <p>Furthermore, simplyblock requires installing the <code>sbctl</code> command line tool. This tool is written in Python. Therefore, Python (3.5 or later) has to be installed. Likewise, pip, the Python package manager, has to be installed with version 20 or later.</p> <p>To install <code>sbctl</code> run:</p> <pre><code>sudo pip install sbctl --upgrade\n</code></pre>"},{"location":"deployments/aws-ec2/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/aws-ec2/prerequisites/#node-instance-type","title":"Node Instance Type","text":"<p>Simplyblock recommends pre-tested and verified instance types. Those instance types are:</p> <ul> <li>i3en.6xlarge</li> <li>i4i.8xlarge</li> </ul>"},{"location":"deployments/aws-ec2/prerequisites/#network-configuration","title":"Network Configuration","text":"<p>Simplyblock requires a number of network ports to be available from different networks. The configuration of the required network ports are provided in the installation documentation.</p> <p>Additionally, IPv6 must be disabled on all nodes running simplyblock.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"},{"location":"deployments/aws-ec2/prerequisites/#aws-networks","title":"AWS Networks","text":"<p>Specifically for AWS, simplyblock strongly advises using individual networks for the control plane and storage plane.</p> <p>For access to the Cluster Management API, simplyblock recommends using an AWS load balancer as a front instead of making the API available directly.</p> <p>Warning</p> <p>Amazon Linux 2 and Amazon Linux 2023 do not support NVMe over Fabrics Multipathing right now!</p>"},{"location":"deployments/aws-ec2/prerequisites/#careful-with-up-to-gbits-instances","title":"Careful with \"Up To GBit/s Instances\"","text":"<p>AWS heavily discounts instances with \"up to GBit/s\" network bandwidth. These instances use a credit-based system to calculate the currently available network bandwidth. These instances initially offer good throughput until the credits are used up, which happens very quickly.</p> <p>In addition to the limitation on the maximum available bandwidth, AWS heavily limits the number of packets per second.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends using EC2 instances with dedicated bandwidth. Generally recommended Amazon EC2 instances are i3en.6xlarge and i4i.8xlarge.</p>"},{"location":"deployments/aws-ec2/prerequisites/#network-ports-for-control-plane","title":"Network Ports for Control Plane","text":"Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP"},{"location":"deployments/aws-ec2/prerequisites/#network-ports-for-storage-plane","title":"Network Ports for Storage Plane","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP"},{"location":"deployments/aws-ec2/prerequisites/#storage-configuration","title":"Storage Configuration","text":"<p>Simplyblock has certain requirements in terms of storage. While the most important facts are provided in the installation section, here are things to consider.</p>"},{"location":"deployments/aws-ec2/prerequisites/#root-volume","title":"Root Volume","text":"<p>The volume mounted as the root directory has to provide at least 35GiB of free capacity. More free space is recommended, especially for control plane nodes, which collect logs and the cluster state.</p>"},{"location":"deployments/aws-ec2/prerequisites/#nvme-devices","title":"NVMe Devices","text":"<p>NVMe devices used for simplyblock should ALWAYS be formatted using the <code>nvme</code> command line tool before adding them to a simplyblock storage node. Failing to do so can negatively impact storage performance and lead to data corruption or even data loss in case of a sudden power outage.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> CLI is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The command output should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/baremetal/","title":"Bare Metal or VM-based","text":"<p>Bare metal and virtualized installations are installations on physically dedicated hosts or virtual machines. These physical or virtual machines provide a Red Hat-based Linux installation.</p> <ul> <li> <p> Prerequisites</p> <p> Node Sizing  Prerequisites</p> </li> <li> <p> Simplyblock Installation</p> <p> Install Simplyblock</p> </li> <li> <p> Kubernetes CSI Driver Installation</p> <p> Install Kubernetes CSI Driver</p> </li> <li> <p> Caching Node Installation</p> <p> Install Caching Nodes</p> </li> </ul>"},{"location":"deployments/baremetal/data-migration/","title":"Data Migration","text":"<p>When migrating existing data to simplyblock, the process can be performed at the block level or the file system level, depending on the source system and migration requirements. Because Simplyblock provides logical Volumes (LVs) as virtual block devices, data can be migrated using standard block device cloning tools such as <code>dd</code>, as well as file-based tools like <code>rsync</code> after the block device has been formatted.</p> <p>Therefore, sata migration to simplyblock is a straightforward process using common block-level and file-level tools. For full disk cloning, <code>dd</code> and similar utilities are effective. For selective file migrations, <code>rsync</code> provides flexibility and reliability. Proper planning and validation of available storage capacity are essential to ensure successful and complete data transfers.</p>"},{"location":"deployments/baremetal/data-migration/#block-level-migration-using-dd","title":"Block-Level Migration Using <code>dd</code>","text":"<p>A block-level copy duplicates the entire content of a source block device, including partition tables, file systems, and data. This method is ideal when migrating entire disks or volumes.</p> Creating a block-level clone of a block device<pre><code>dd if=/dev/source-device of=/dev/simplyblock-device bs=4M status=progress\n</code></pre> <ul> <li><code>if=</code> specifies the input (source) device.</li> <li><code>of=</code> specifies the output (Simplyblock Logical Volume) device.</li> <li><code>bs=4M</code> sets the block size for efficiency.</li> <li><code>status=progress</code> provides real-time progress updates.</li> </ul> <p>Info</p> <p>Ensure that the simplyblock logical volume is at least as large as the source device to prevent data loss.</p>"},{"location":"deployments/baremetal/data-migration/#alternative-block-level-cloning-tools","title":"Alternative Block-Level Cloning Tools","text":"<p>Other block-level tools such as <code>Clonezilla</code>, <code>partclone</code>, or <code>dcfldd</code> may also be used for disk duplication, depending on the specific environment and desired features like compression or network transfer.</p>"},{"location":"deployments/baremetal/data-migration/#file-level-migration-using-rsync","title":"File-Level Migration Using <code>rsync</code>","text":"<p>For scenarios where only file contents need to be migrated (for example, after creating a new file system on a simplyblock logical volume), <code>rsync</code> is a reliable tool.</p> <ol> <li> <p>First, format the Simplyblock Logical Volume:    </p>Format the simplyblock block device with ext4<pre><code>mkfs.ext4 /dev/simplyblock-device\n</code></pre> </li> <li> <p>Mount the Logical Volume:    </p>Mount the block device<pre><code>mount /dev/simplyblock-device /mnt/simplyblock\n</code></pre> </li> <li> <p>Use <code>rsync</code> to copy files from the source directory:    </p>Synchronize the source disks content using rsync<pre><code>rsync -avh --progress /source/data/ /mnt/simplyblock/\n</code></pre> <ul> <li><code>-a</code> preserves permissions, timestamps, and symbolic links.</li> <li><code>-v</code> provides verbose output.</li> <li><code>-h</code> makes output human-readable.</li> <li><code>--progress</code> shows transfer progress.</li> </ul> </li> </ol>"},{"location":"deployments/baremetal/data-migration/#minimal-downtime-migration-strategy","title":"Minimal-Downtime Migration Strategy","text":"<p>An alternative, but more complex solution enables minimal downtime. This option utilizes the Linux <code>dm</code> (Device Mapper) subsystem.</p> <p>Using the Device Mapper, the current and new block devices will be moved into a RAID-1 and synchronized (re-silvered) in the background.  This solution requires two minimal downtimes to create and remount the devices.</p> <p>Warning</p> <p>This method is quite involved, requires a lot of steps, and can lead to data loss in case of wrong commands or parameters. It should only be used by advanced users that understand the danger of the commands below. Furthermore, this migration method MUST NOT be used for boot devices!</p> <p>In this walkthrough, we assume the new simplyblock logical volume is already connected to the system.</p>"},{"location":"deployments/baremetal/data-migration/#preparation","title":"Preparation","text":"<p>To successfully execute this data migration, a few values are required. First of all, the two device names of the currently used and new device need to be collected.</p> <p>This can be done by executing the command <code>lsblk</code> to list all attached block devices.</p> lsblk provides information about all attached block devices<pre><code>lsblk\n</code></pre> <p>In this example, sda is the boot device which hosts the operating system, while sdb is the currently used block device and nvme0n1 is the newly attached simplyblock logical volume. The latter two should be noted down.</p> <p>Danger</p> <p>It is important to understand the difference between the currently used and the new device. Using them in the wrong order in the following steps will cause any or all data to be lost!</p> Find the source and target block devices using lsblk<pre><code>[root@demo ~]# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0   25G  0 disk\n\u251c\u2500sda1                      8:1    0    1G  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0    2G  0 part  /boot\n\u2514\u2500sda3                      8:3    0 21.9G  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0   11G  0 lvm   /\nsdb                         8:16   0   25G  0 disk\n\u2514\u2500sdb1                      8:17   0   25G  0 part  /data/pg\nsr0                        11:0    1 57.4M  0 rom\nnvme0n1                   259:0    0   25G  0 disk\n</code></pre> <p>Next up the cluster size of the current device is required. The value must be set on the RAID to-be-created. It needs to be noted down.</p> Find the block size of the source filesystem<pre><code>tune2fs -l /dev/sdb1 | grep -i 'block size'\n</code></pre> <p>In this example, the block size is 4 KiB (4096 bytes).</p> Example output of the block size<pre><code>[root@demo ~]# tune2fs -l /dev/sdb1 | grep -i 'block size'\nBlock size:               4096\n</code></pre> <p>Last, it is important to ensure that the new target device is at least as large or larger than the current device. <code>lsblk</code> can be used again to get the required numbers.</p> lsblk with byte sizes of the block devices<pre><code>lsblk -b\n</code></pre> <p>In this example, both devices are the same size, 26843545600 bytes in total disk capacity.</p> Example output of lsblk -b<pre><code>[root@demo ~]# lsblk -b\nNAME                      MAJ:MIN RM        SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0 26843545600  0 disk\n\u251c\u2500sda1                      8:1    0  1127219200  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0  2147483648  0 part  /boot\n\u2514\u2500sda3                      8:3    0 23566745600  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0 11781799936  0 lvm   /\nsdb                         8:16   0 26843545600  0 disk\n\u2514\u2500sdb1                      8:17   0 26843513344  0 part  /data/pg\nsr0                        11:0    1    60225536  0 rom\nnvme0n1                   259:0    0 26843545600  0 disk\n</code></pre>"},{"location":"deployments/baremetal/data-migration/#device-mapper-raid-setup","title":"Device Mapper RAID Setup","text":"<p>Danger</p> <p>From here on out, mistakes can cause any or all data to be lost! It is strongly recommended to only go further, if ensured that the values above are correct and after a full data backup is created. It is also recommended to test the backup before continuing. A failure to do so can cause issues in case it cannot be replayed.</p> <p>Now, it's time to create the temporary RAID for disk synchronization. Anything beyond this point is dangerous.</p> <p>Warning</p> <p>Any service accessing the current block device or any of its partitions need to be shutdown and the block device and its partitions need to be unmounted. It is required for the device to not be busy. </p>Example of PostgreSQL shutdown and partition unmount<pre><code>service postgresql stop\numount /data/pg\n</code></pre> Building a RAID-1 with mdadm<pre><code>mdadm --build --chunk=&lt;CHUNK_SIZE&gt; --level=1 \\\n    --raid-devices=2 --bitmap=none \\\n    &lt;RAID_NAME&gt; &lt;CURRENT_DEVICE_FILE&gt; missing\n</code></pre> <p>In this example, the RAID is created using the /dev/sdb device file and 4096 as the chunk size. The newly created RAID is called migration. The RAID-level is 1 (meaning, RAID-1) and it includes 2 devices. The missing at the end of the command is required to tell the device mapper that the second device of the RAID is missing for now. It will be added later.</p> Example output of a RAID-1 with mdadm<pre><code>[root@demo ~]# mdadm --build --chunk=4096 --level=1 --raid-devices=2 --bitmap=none migration /dev/sdb missing\nmdadm: array /dev/md/migration built and started.\n</code></pre> <p>To ensure that the RAID was created successfully, all device files with /dev/md* can be listed. In this case, /dev/md127 is the actual RAID device, while /dev/md/migration is the device mapper file.</p> Finding the new device mapper device files<pre><code>[root@demo ~]# ls /dev/md*\n/dev/md127  /dev/md127p1\n\n/dev/md:\nmigration  migration1\n</code></pre> <p>After the RAID device name is confirmed, the new RAID device can be mounted. In this example, the original block device was partitioned. Hence, the RAID device also has one partition /dev/md127p1. This is what needs to be mounted to the same mount point as the original disk before, /data/pg in this example.</p> Mount the new device mapper device file<pre><code>[root@demo ~]# mount /dev/md127p1 /data/pg/\n</code></pre> <p>Info</p> <p>All services that require access to the data can be started again. The RAID itself is still in a degraded state, but it provides the same data security as the original device.</p> <p>Now the second, new device must be added to the RAID setup to start the re-silvering (data synchronization) process. This is again done using <code>mdadm</code> tool.</p> Add the new simplyblock block device to RAID-1<pre><code>mdadm &lt;RAID_DEVICE_MAPPER_FILE&gt; --add &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>In the example, we add /dev/nvme0n1 (the simplyblock logical volume) to the RAID named \"migration.\"</p> Example output of mdadm --add<pre><code>[root@demo ~]# mdadm /dev/md/migration --add /dev/nvme0n1\nmdadm: added /dev/nvme0n1\n</code></pre> <p>After the device was added to the RAID setup, a background process is automatically started to synchronize the newly added device to the first device in the setup. This process is called re-silvering.</p> <p>Info</p> <p>While the devices are synchronized, the read and write performance may be impacted due to the additional I/O operations of the synchronization process. However, the process runs on a very low priority and shouldn't impact the live operation too extensively. For AWS users: if the migration uses an Amazon EBS volume as the source, ensure enough IOPS to cover live operation and migration.</p> <p>The synchronization process status can be monitored using one of two commands:</p> Check status of re-silvering<pre><code>mdadm -D &lt;RAID_DEVICE_FILE&gt;\ncat /proc/mdstat\n</code></pre> Example output of a status check via mdadm<pre><code>[root@demo ~]#mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean, degraded, recovering\n    Active Devices : 1\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 1\n\nConsistency Policy : resync\n\n    Rebuild Status : 98% complete\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      spare rebuilding   /dev/nvme0n1\n</code></pre> Example output of a status check via /proc/mdstat<pre><code>[root@demo ~]# cat /proc/mdstat \nPersonalities : [raid1] \nmd0 : active raid1 sdb[1] nvme0n1[0]\n      10484664 blocks super 1.2 [2/2] [UU]\n      [========&gt;............]  resync = 42.3% (4440832/10484664) finish=0.4min speed=201856K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>"},{"location":"deployments/baremetal/data-migration/#after-the-synchronization-is-done","title":"After the Synchronization is done","text":"<p>Eventually, the synchronization finishes. At this point, the two devices (original and new) are kept in sync by the device mapper system.</p> Example out of a finished synchronzation<pre><code>[root@demo ~]# mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean\n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      active sync   /dev/nvme0n1\n</code></pre> <p>To fully switch to the new simplyblock logical volume, a second, minimal, downtime is required.</p> <p>The RAID device needs to be unmounted and the device mapper stopped.</p> Stopping the device mapper RAID-1<pre><code>umount &lt;MOUNT_POINT&gt;\nmdadm --stop &lt;DEVICE_MAPPER_FILE&gt;\n</code></pre> <p>In this example /data/pg and /dev/md/migration are used.</p> Example output of a stopped RAID-1<pre><code>[root@demo ~]# umount /data/pg/\n[root@demo ~]# mdadm --stop /dev/md/migration\nmdadm: stopped /dev/md/migration\n</code></pre> <p>Now, the system should be restarted. If a system reboot takes too long and is out of the scope of the available maintenance window, a re-read of the partition tables can be forced.</p> Re-read partition table<pre><code>blockdev --rereadpt &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>After re-reading the partition table of a device, the partition should be recognized and visible.</p> Example output of re-reading the partition table<pre><code>[root@demo ~]# blockdev --rereadpt /dev/nvme0n1\n[root@demo ~]# ls /dev/nvme0n1p1\n/dev/nvme0n1p1\n</code></pre> <p>As a last step, the partition must be mounted to the same mount point as the RAID device before. If the mount is successful, the services can be started again.</p> Mounting the plain block device and restarting services<pre><code>[root@demo ~]# mount /dev/nvme0n1p1 /data/pg/\n[root@demo ~]# service postgresql start\n</code></pre>"},{"location":"deployments/baremetal/install-caching-nodes/","title":"Install Caching Nodes (Kubernetes)","text":"<p>Caching nodes are simplyblock storage containers that are co-located with the workloads on Kubernetes workers. They utilize directly attached NVMe disks on the worker nodes to provide an ultra-low-latency write-through cache for disaggregated or hybrid simplyblock clusters.</p>"},{"location":"deployments/baremetal/install-caching-nodes/#prerequisites","title":"Prerequisites","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>Caching nodes, like storage nodes, require huge page memory to hold the internal state. Huge pages should be 2MiB in size and a minimum of 4096 huge pages should be allocated at boot time of the operating system.</p> <pre><code>demo@worker-1 ~&gt; sudo sysctl -w vm.nr_hugepages=4096\n</code></pre> <p>Info</p> <p>To see how huge pages can be pre-reserved at boot time, see the node sizing documentation section on Huge Pages.</p> <pre><code>demo@worker-1 ~&gt; sudo systemctl restart kubelet\n</code></pre> <pre><code>demo@worker-1 ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n</code></pre> <pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi      0 (0%)    0 (0%)\n</code></pre> <pre><code>demo@worker-1 ~&gt; sudo yum install -y nvme-cli\ndemo@worker-1 ~&gt; sudo modprobe nvme-tcp\ndemo@worker-1 ~&gt; sudo modprobe nbd\n</code></pre> <pre><code>demo@demo ~&gt; kubectl label nodes worker-1.kubernetes-cluster.local type=simplyblock-cache\n</code></pre> <p>Afterward, the worker node can be described to check that the node-selector type is successfully set.</p> Example output of a successfully configured worker node<pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep simplyblock-cache\n                    type=simplyblock-cache\n</code></pre> Example Caching Node-enabled StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  type: cache\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock-csi/","title":"Install Simplyblock CSI","text":"<p>Simplyblock provides a seamless integration with Kubernetes through its Kubernetes CSI driver.</p>"},{"location":"deployments/baremetal/install-simplyblock-csi/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p> <p>To install the Simplyblock CSI Driver, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended. If a manual installation is preferred, see the CSI Driver Repository\u00a0\u29c9. </p> <p>Either way, the installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying the CSI driver is the control plane address. On a standard bare metal or virtualized installation, it is any of the API addresses. Meaning, if the primary management node has the IP of <code>192.168.10.1</code>, the control plane address is <code>http://192.168.0.1</code>. It is, however, recommended to front all management nodes with a load balancing proxy, such as HAProxy. In the latter case, the load balancer URL would be the address of the control plane.</p> <p>Anyhow, deploying the Simplyblock CSI Driver using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver.</p> Install Simplyblock's CSI Driver<pre><code>CLUSTER_UUID=\"&lt;UUID&gt;\"\nCLUSTER_SECRET=\"&lt;SECRET&gt;\"\nCNTR_ADDR=\"&lt;CONTROL-PLANE-ADDR&gt;\"\nPOOL_NAME=\"&lt;POOL-NAME&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\nhelm repo update\nhelm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n    --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n    --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n    --set logicalVolume.pool_name=${POOL_NAME}\n</code></pre> Example output of the CSI driver deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/","title":"Install Simplyblock Storage Cluster","text":"<p>Installing simplyblock for production requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance, and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides two test scripts to automatically check your system's configuration. While those may not catch all edge cases, they can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configurations during the process.</p> Automatically check your configurations<pre><code># Configuration check for the control plane (management nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n\n# Configuration check for the storage plane (storage nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> <li>Secondary nodes are part of a specific storage cluster and enable automatic fail over for NVMe-oF connections. In a high-availability cluster, every primary storage node automatically provides a secondary storage node. </li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, and the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/baremetal/install-simplyblock/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separate subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24 <p>Danger</p> <p>Simplyblock requires a fully redundant network interconnect, implemented via a solution such as LACP or Static LAG. Failing to provide that may cause data corruption or data loss in case of network issues. For more information see the Network Considerations section.</p>"},{"location":"deployments/baremetal/install-simplyblock/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/baremetal/install-simplyblock/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/baremetal/install-simplyblock/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets are available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/baremetal/install-simplyblock/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Docker Swarm, by default, creates iptables entries open to the world. If no external firewall is available, the created iptables configuration needs to be restricted.</p> <p>The following script will create additional iptables rules prepended to Docker's forwarding rules and only enabling access from internal networks. This script should be stored in /usr/local/sbin/simplyblock-iptables.sh.</p> Configuration script for Iptables<pre><code>#!/usr/bin/env bash\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080:8890 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9090-9900 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre> <p>To automatically run this script whenever Docker is started or restarted, it must be attached to a Systemd service, stored as /etc/systemd/system/simplyblock-iptables.service.</p> Systemd script to set up Iptables<pre><code>[Unit]\nDescription=Simplyblock Iptables Restrictions for Docker \nAfter=docker.service\nBindsTo=docker.service\nReloadPropagatedFrom=docker.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/simplyblock-iptables.sh\nExecReload=/usr/local/sbin/simplyblock-iptables.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>After both files are stored in their respective locations, the bash script needs to be made executable, and the Systemd service needs to be enabled to start automatically.</p> Enabling service file<pre><code>chmod +x /usr/local/sbin/simplyblock-iptables.sh\nsystemctl enable simplyblock-iptables.service\nsystemctl start simplyblock-iptables.service\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) are installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#nvme-device-preparation","title":"NVMe Device Preparation","text":"<p>Once the check is complete, the NVMe devices in each storage node can be prepared. To prevent data loss in case of a sudden power outage, NVMe devices need to be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If an NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/baremetal/install-simplyblock/#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>With all NVMe devices prepared and the NVMe/TCP driver loaded, the storage node software can be deployed.</p> <p>The actual deployment process happens in three steps: - Creating the storage node configuration - Deploy the first stage (the storage node API) - Deploy the second stage (add the storage node to the cluster), happening from a management node</p> <p>The configuration process creates the configuration file, which contains all the assignments of NVMe devices, NICs, and potentially available NUMA nodes. By default, simplyblock will configure one storage node per NUMA node.</p> Configure the storage node<pre><code>sudo sbctl storage-node configure \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-size &lt;MAX_PROVISIONING_CAPACITY&gt;\n</code></pre> Example output of storage node configure<pre><code>[demo@demo-3 ~]# sudo sbctl sn configure --nodes-per-socket=2 --max-lvol=50 --max-size=1T\n2025-05-14 10:40:17,460: INFO: 0000:00:04.0 is already bound to nvme.\n0000:00:1e.0\n0000:00:1e.0\n0000:00:1f.0\n0000:00:1f.0\n0000:00:1e.0\n0000:00:1f.0\n2025-05-14 10:40:17,841: INFO: JSON file successfully written to /etc/simplyblock/sn_config_file\n2025-05-14 10:40:17,905: INFO: JSON file successfully written to /etc/simplyblock/system_info\nTrue\n</code></pre> <p>A full set of the parameters for the configure subcommand can be found in the CLI reference. </p> <p>After the configuration has been created, the first stage deployment can be executed </p> Deploy the storage node<pre><code>sudo sbctl storage-node deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbctl storage-node deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p> <p>When all storage nodes are added, it's finally time to activate the storage plane.</p>"},{"location":"deployments/baremetal/install-simplyblock/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step, after all nodes are added to the storage cluster, is to activate the storage plane.</p> Storage cluster activation<pre><code>sudo sbctl cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this, and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbctl cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre> <p>Now that the cluster is ready, it is time to install the Kubernetes CSI Driver or learn how to use the simplyblock storage cluster to manually provision logical volumes.</p>"},{"location":"deployments/baremetal/prerequisites/","title":"Prerequisites","text":"<p>When installing simplyblock control planes and storage planes, a number of prerequisites are important to understand.</p> <p>Simplyblock uses Docker Swarm for the control plane cluster. In case of a bare metal or virtualized installation, it will also use Docker Swarm for the storage plane. Hence, Docker has to be installed.</p> <p>Furthermore, simplyblock requires installing the <code>sbctl</code> command line tool. This tool is written in Python. Therefore, Python (3.5 or later) has to be installed. Likewise, pip, the Python package manager, has to be installed with version 20 or later.</p> <p>To install <code>sbctl</code> run:</p> <pre><code>sudo pip install sbctl --upgrade\n</code></pre>"},{"location":"deployments/baremetal/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/baremetal/prerequisites/#network-configuration","title":"Network Configuration","text":"<p>Simplyblock requires a number of network ports to be available from different networks. The configuration of the required network ports are provided in the installation documentation.</p> <p>Additionally, IPv6 must be disabled on all nodes running simplyblock.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"},{"location":"deployments/baremetal/prerequisites/#network-ports-for-control-plane","title":"Network Ports for Control Plane","text":"Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP"},{"location":"deployments/baremetal/prerequisites/#network-ports-for-storage-plane","title":"Network Ports for Storage Plane","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP"},{"location":"deployments/baremetal/prerequisites/#storage-configuration","title":"Storage Configuration","text":"<p>Simplyblock has certain requirements in terms of storage. While the most important facts are provided in the installation section, here are things to consider.</p>"},{"location":"deployments/baremetal/prerequisites/#root-volume","title":"Root Volume","text":"<p>The volume mounted as the root directory has to provide at least 35GiB of free capacity. More free space is recommended, especially for control plane nodes, which collect logs and the cluster state.</p>"},{"location":"deployments/baremetal/prerequisites/#nvme-devices","title":"NVMe Devices","text":"<p>NVMe devices used for simplyblock should ALWAYS be formatted using the <code>nvme</code> command line tool before adding them to a simplyblock storage node. Failing to do so can negatively impact storage performance and lead to data corruption or even data loss in case of a sudden power outage.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> CLI is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/deployment-planning/","title":"Deployment Planning","text":"<p>Proper deployment planning is essential for ensuring the performance, scalability, and resilience of a simplyblock storage cluster.</p> <p>Before installation, key factors such as node sizing, storage capacity, and fault tolerance mechanisms should be carefully evaluated to match workload requirements. This section provides guidance on sizing management nodes and storage nodes, helping administrators allocate adequate CPU, memory, and disk resources for optimal cluster performance.</p> <p>Additionally, it explores selectable erasure coding schemes, detailing how different configurations impact storage efficiency, redundancy, and recovery performance. Other critical considerations, such as network infrastructure, high-availability strategies, and workload-specific optimizations, are also covered to assist in designing a simplyblock deployment that meets both operational and business needs.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/","title":"Erasure Coding Scheme","text":"<p>Choosing the appropriate erasure coding scheme is crucial when deploying a simplyblock storage cluster, as it directly impacts data redundancy, storage efficiency, and overall system performance. Simplyblock currently supports the following erasure coding schemes: 1+1, 2+1, 4+1, 1+2, 2+2, and 4+2. Understanding the trade-offs between redundancy and storage utilization will help determine the best option for your workload. All schemas have been performance-optimized by specialized algorithms. There is, however, a remaining capacity-to-performance trade-off.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#erasure-coding-schemes","title":"Erasure Coding Schemes","text":"<p>Erasure coding (EC) is a data protection mechanism that distributes data and parity across multiple storage nodes, allowing data recovery in case of hardware failures. The notation k+m represents:</p> <ul> <li>k: The number of data fragments.</li> <li>m: The number of parity fragments.</li> </ul> <p>If you need more information on erasure coding, see the dedicated concept page for erasure coding.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-11","title":"Scheme: 1+1","text":"<ul> <li>Description: In the 1+1 scheme, data is mirrored, effectively creating an exact copy of every data block.</li> <li>Redundancy Level: Can tolerate the failure of one storage node.</li> <li>Raw-to-Effective Ratio: 200%</li> <li>Available Storage Capacity: 50%</li> <li>Performance Considerations: Offers fast recovery and high read performance due to data mirroring.</li> <li>Best Use Cases:<ul> <li>Workloads requiring high availability and minimal recovery time.</li> <li>Applications where performance is prioritized over storage efficiency.</li> <li>Requires 3 or more nodes for full redundancy.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-21","title":"Scheme: 2+1","text":"<ul> <li>Description: In the 2+1 scheme, data is divided into two fragments with one parity fragment, offering a   balance between performance and storage efficiency.</li> <li>Redundancy Level: Can tolerate the failure of one storage node.</li> <li>Raw-to-Effective Ratio: 150%</li> <li>Available Storage Capacity: 66.6%</li> <li>Performance Considerations: For writes of 8K or higher, lower write amplification compared to 1+1, as data is distributed across multiple nodes. This typically results in similar or higher IOPS. However, for small random writes (4K), the write performance is worse than 1+1. Write latency is somewhat higher than with 1+1. Read performance is similar to 1+1, if local node affinity is disabled. With node affinity enabled, read performance is slightly worse (up to 25%). In a degraded state (one node offline / unavailable or failed disk), the performance is worse than with 1+1. Recovery time to full redundancy from single disk error is slightly higher than with 1+1.</li> <li>Best Use Cases:<ul> <li>Deployments where storage efficiency is relevant without significantly compromising performance.</li> <li>Requires 4 or more nodes for full redundancy.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-41","title":"Scheme: 4+1","text":"<ul> <li>Description: In the 4+1 scheme, data is divided into four fragments with one parity fragment, offering   optimal storage efficiency.</li> <li>Redundancy Level: Can tolerate the failure of one storage node.</li> <li>Raw-to-Effective Ratio: 125%</li> <li>Available Storage Capacity: 80%</li> <li>Performance Considerations: For writes of 16K or higher, lower write amplification compared to 2+1, as data is distributed across more nodes. This typically results in similar or higher write IOPS. However, for 4-8K random writes, the write performance is typically worse than 2+1. Write latency is somewhat similar to 2+1. Read performance is similar to 2+1, if local node affinity is disabled. With node affinity enabled, read performance is slightly worse (up to 13%). In a degraded state (one node offline / unavailable or failed disk), the performance is worse than with 2+1. Recovery time to full redundancy from single disk error is slightly higher than with 2+1.</li> <li>Best Use Cases:<ul> <li>Deployments where storage efficiency is a priority without significantly compromising performance.</li> <li>Requires 6 or more nodes for full redundancy.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-12","title":"Scheme: 1+2","text":"<ul> <li>Description: In the 1+2 scheme, data is replicated twice, effectively creating multiple copies of every data block.</li> <li>Redundancy Level: Can tolerate the failure of two storage nodes.</li> <li>Raw-to-Effective Ratio: 300%</li> <li>Available Storage Capacity: 33.3%</li> <li>Performance Considerations: Offers fast recovery and high read performance due to data replication, but write performance is lower than with 1+1 in all cases (~33%).</li> <li>Best Use Cases:<ul> <li>Workloads requiring high redundancy and minimal recovery time.</li> <li>Applications where performance is prioritized over storage efficiency.</li> <li>Requires 4 or more nodes for full redundancy.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-22","title":"Scheme: 2+2","text":"<ul> <li>Description: In the 2+2 scheme, data is divided into two fragments with two parity fragments, offering a great   balance between redundancy and storage efficiency.</li> <li>Redundancy Level: Can tolerate the failure of two storage nodes.</li> <li>Raw-to-Effective Ratio: 200%</li> <li>Available Storage Capacity: 50%</li> <li>Performance Considerations: Similar to 2+1, but with higher write latencies and lower effective write IOPS due to higher write amplification.</li> <li>Best Use Cases:<ul> <li>Deployments where high redundancy and storage efficiency is important without compromising redundancy.</li> <li>Applications that can tolerate slightly higher recovery times compared to 1+2.</li> <li>Requires 5 or more nodes for full redundancy.</li> </ul> </li> </ul> <p>### Scheme: 4+2</p> <ul> <li>Description: In the 4+2 scheme, data is divided into four fragments with two parity fragments, offering a great   balance between redundancy and storage efficiency.</li> <li>Redundancy Level: Can tolerate the failure of two storage nodes.</li> <li>Raw-to-Effective Ratio: 150%</li> <li>Available Storage Capacity: 66.6%</li> <li>Performance Considerations: Similar to 4+1, but with higher write latencies and lower effective write IOPS due to higher write amplification.</li> <li>Best Use Cases:<ul> <li>Deployments where high redundancy and storage efficiency is a priority.</li> <li>Requires 7 or more nodes in a cluster.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#choosing-the-scheme","title":"Choosing the Scheme","text":"<p>When selecting an erasure coding scheme for simplyblock, consider the following:</p> <ol> <li>Redundancy Requirements: If the priority is maximum data protection and quick recovery, 1+1 or 1+2 are ideal. For a    balance between protection and efficiency, 2+1 or 2+2 is preferred.</li> <li>Storage Capacity: 1+1 requires double the storage space, whereas 2+1 provides better storage efficiency. 1+2 requires triple the storage space, whereas 2+2 provides great storage efficiency and fault tolerance.</li> <li>Performance Needs: 1+1 and 2+2 offer faster reads and writes due to mirroring, while 2+1 and 2+2 reduce write amplification and optimize for storage usage.</li> <li>Cluster Size: Smaller clusters benefit from 1+1 or 1+2 due to its simplicity and faster rebuild times, whereas 2+1 and 2+2 are more effective in larger clusters.</li> <li>Recovery Time Objectives (RTOs): If minimizing downtime is critical, 1+1 and 1+2 offer near-instant recovery compared to 2+1 and 2+2 which require rebuilding of the lost data from parity information.</li> </ol>"},{"location":"deployments/deployment-planning/further-considerations/","title":"Further Considerations","text":""},{"location":"deployments/deployment-planning/further-considerations/#system-compatibility","title":"System Compatibility","text":"<p>Simplyblock contains two major components, the control plane and the storage plane.</p> <ul> <li>For the control plane, simplyblock requires x86-64 (AMD64 / Intel 64) compatible CPUs.</li> <li>For the storage plane, simplyblock supports x86-64 (AMD64 / Intel 64) or ARM64 (AArch64) compatible CPUs.</li> </ul> <p>Info</p> <p>A single storage plane cluster can be set up from both x86-64 and ARM64 CPUs. However, simplyblock recommends building a storage plane from a single CPU architecture. This is especially important to consider when running hyper-converged or in a hybrid setup.</p> <p>In terms of operating system, simplyblock requires a Red Hat-based Linux distribution version 9 (including Rocky Linux and Alma Linux) for the control plane nodes and disaggregated storage nodes (docker-swarm), and recommends a Linux kernel 5.9 or later. </p> <p>For storage nodes running in Kubernetes, any Linux distribution (RHEL-based, Debian-based, or Talos) can be used. For more information, see the supported Linux Distributions Matrix.</p> <p>For any client host, the <code>nvme-tcp</code> module must be loaded and support NVMe Multipathing\u00a0\u29c9.</p>"},{"location":"deployments/deployment-planning/further-considerations/#storage-considerations","title":"Storage Considerations","text":"<p>Simplyblock is an NVMe-first storage architecture and requires an NVMe device with exclusive access. Simplyblock does not support individual partitions but requires full and exclusive access to the physical or virtual NVMe device.</p>"},{"location":"deployments/deployment-planning/network-considerations/","title":"Network Considerations","text":"<p>Simplyblock is a distributed storage platform. Hence, it highly relies on a strong network infrastructure for the performance and reliability of its virtual block storage devices (logical volume). </p>"},{"location":"deployments/deployment-planning/network-considerations/#network-type","title":"Network Type","text":"<p>Protocol-wise, simplyblock implements NVMe over Fabrics (NVMe-oF), meaning that simplyblock does not require any specific network infrastructure, such as Fibre Channel or Infiniband, but works over any Ethernet interconnect.</p> <p>For data transmission, simplyblock provides NVMe over TCP (NVMe/TCP) and NVMe over RDMA over Converged Ethernet (NVMe/RoCE).</p>"},{"location":"deployments/deployment-planning/network-considerations/#network-infrastructure","title":"Network Infrastructure","text":"<p>In terms of bandwidth, simplyblock recommends at least 10GBit/s interconnects, but higher is better. Especially with a high number of cluster nodes and logical volumes, simplyblock can easily saturate 200 GBit/s and more interconnects.</p> <p>Recommendation</p> <p>Simplyblock recommends NVIDIA Mellanox network adapters. However, every network adapter, including virtual ones, will work. If using virtual machines, the physical network adapter should be made available to the VM using PCI-e passthrough (IOSRV).</p> <p>Additionally, simplyblock recommends a physically separated storage network or using a VLAN to create a virtually separated network. This can improve performance and minimize network contention.</p> <p>Recommendation</p> <p>If VLANs are used, prefer hardware-based VLANs configured in switches over a software-based VLAN with Linux bridges.</p>"},{"location":"deployments/deployment-planning/network-considerations/#network-configuration","title":"Network Configuration","text":"<p>Lastly, simplyblock requires a set of TCP/IP ports to be opened towards specific subnets. The installation prerequisites for the deployment model of your choice list the required ports. Simplyblock also provides a shell script to pre-test the most important requirements to ensure a smooth installation.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two separate NICs, one for the control plane traffic and one for the storage plane. These can be implemented via VLAN. However, we recommend port-based VLANs configured in the switch over virtual VLAN interfaces in Linux.</p> <p>Additionally, simplyblock strongly recommends designing any network interconnect as a fully redundant connection. All commonly found solutions to achieve that are supported, including but not limited to LACP and Static LAG configurations, stacked switches, and bonded NICs. Depending on the erasure coding schema chosen and the number of nodes in a cluster, simplyblock supports either single or concurrent dual-node outages, including network outages. If the network fails for more than one node (two nodes), this will cause a cluster-down and an I/O suspension event.</p>"},{"location":"deployments/deployment-planning/node-sizing/","title":"Node Sizing","text":"<p>When planning the deployment of a simplyblock cluster, it is essential to plan the sizing of the nodes. The sizing requirements are elaborated below, whether deployed on a private or public cloud, or inside and outside of Kubernetes.</p>"},{"location":"deployments/deployment-planning/node-sizing/#sizing-assumptions","title":"Sizing Assumptions","text":"<p>The following sizing information is meant for production environments.</p> <p>Warning</p> <p>If the sizing document discusses virtual CPUs (vCPUs), it means 0.5 physical CPUs. This corresponds to a typical hyper-threaded CPU core x86-64. This also relates to how AWS EC2 cores are measured.</p>"},{"location":"deployments/deployment-planning/node-sizing/#management-nodes","title":"Management Nodes","text":"<p>An appropriately sized management node cluster is required to ensure optimal performance and scalability. The management plane oversees critical functions such as cluster topology management, health monitoring, statistics collection, and automated maintenance tasks.</p> <p>The following hardware sizing specifications are recommended:</p> Hardware CPU Minimum 4 vCPUs, plus<ul><li>1 vCPU per 5 storage nodes</li><li>1 vCPU 500 logical volumes</li></ul> RAM Minimum 8 GiB, plus:<ul><li>1 GiB RAM per 5 storage nodes</li><li>1 GiB per 500 logical volumes</li></ul> Disk Minimum 35 GiB, plus:<ul><li>500 MiB per 100 cluster objects (storage nodes, devices, logical volumes, snapshots)</li></ul> Node type Bare metal or virtual machine with a supported Linux distribution Number of nodes For a production environment, a minimum of 3 management nodes is required."},{"location":"deployments/deployment-planning/node-sizing/#storage-nodes","title":"Storage Nodes","text":"<p>Warning</p> <p>A storage node is not equal to a physical or virtual host. For optimal performance, at least two storage nodes are deployed on two socket systems (one per NuMA socket), for optimal performance, even four storage nodes are recommended (2 per socket). </p> <p>A suitably sized storage node cluster is required to ensure optimal performance and scalability. Storage nodes are responsible for handling all I/O operations and data services for logical volumes and snapshots.</p> <p>The following hardware sizing specifications are recommended:</p> Hardware CPU Minimum 5 vCPU RAM Minimum 4 GiB Disk Minimum 10 GiB free space on boot volume"},{"location":"deployments/deployment-planning/node-sizing/#memory-requirements","title":"Memory Requirements","text":"<p>In addition to the above RAM requirements, the storage node requires additional memory based on the managed storage capacity.</p> <p>While a certain amount of RAM is pre-reserved for SPDK, another part is dynamically pre-allocated. Users should ensure that the full amount of required RAM is available (reserved) from the system as long as simplyblock is running.</p> <p>The exact amount of memory is calculated when adding or restarting a node based on two parameters:</p> <ul> <li>The maximum amount of storage available in the cluster</li> <li>The maximum amount of logical volumes that can be created on the node</li> </ul> Unit Memory Requirement Fixed amount 3 GiB Per logical volume 15 MiB % of max. utilized capacity on node 0.2% <p>Info</p> <p>Example: A node has 10 NVMe devices with 8TB each. The cluster has 3 nodes and a total capacity of 240 TB. Logical volumes are equally distributed across nodes, and it is planned to use up to 1,000 logical volumes on each node. Hence, the following formula: </p><pre><code>3 + (2 * 80) + (0.015 * 1000) = 164.5 GB\n</code></pre> <p>If not enough memory is available, the node will refuse to start. In this case, <code>/proc/meminfo</code> may be checked for total, reserved, and available system and huge page memory on a corresponding node. </p> <p>Info</p> <p>Part of the memory will be allocated as huge-page memory. In case of a high degree of memory fragmentation, a system may not be able to allocate enough of the huge-page memory even if there is enough system memory available. If the node fails to start up, a system reboot may ensure enough free memory.  </p> <p>The following command can be executed to temporarily allocate huge pages while the system is already running. It will allocate 8 GiB in huge pages. The number of huge pages must be adjusted depending on the requirements.</p> Allocate temporary huge pages<pre><code>sudo sysctl vm.nr_hugepages=4096\n</code></pre> <p>Since the allocation is temporary, it will disappear after a system reboot. It must be ensured that either the setting is reapplied after each system reboot or persisted to be automatically applied on system boot up.</p>"},{"location":"deployments/deployment-planning/node-sizing/#storage-planning","title":"Storage Planning","text":"<p>Simplyblock storage nodes require one or more NVMe devices to provide storage capacity to the distributed storage pool of a storage cluster.</p> <p>Furthermore, simplyblock storage nodes require one additional NVMe device with less capacity as a journaling device. The journaling device becomes part of the distributed record journal, keeping track of all changes before being persisted into their final position. This helps with write-performance and transactional behavior by using a write-ahead log structure and replaying the journal in case of an issue.</p> <p>Warning</p> <p>Simplyblock does not work with device partitions or claimed (mounted) devices. It must be ensured that all NVMe devices to be used by simplyblock are unmounted and not busy.</p> <p>Any partition must be removed from the NVMe devices prior to installing simplyblock. Furthermore, NVMe devices must be low-level formatted with a 4KB block size (lbaf: 12). More information can be found in NVMe Low-Level Format.</p>"},{"location":"deployments/deployment-planning/node-sizing/#caching-nodes-k8s-only","title":"Caching Nodes (K8s only)","text":"<p>In Kubernetes, simplyblock can be configured to deploy caching nodes. These nodes provide an ultra-low latency write-through cache to a disaggregated cluster, improving access latency substantially.</p> Hardware CPU Minimum 6 vCPU RAM Minimum 4 GiB"},{"location":"deployments/deployment-planning/numa-considerations/","title":"NUMA Considerations","text":"<p>Modern multi-socket servers use a memory architecture called NUMA (Non-Uniform Memory Access)\u00a0\u29c9. In a NUMA system, each CPU socket has its own local memory and I/O paths. Accessing local resources is faster than reaching across sockets to remote memory or devices.</p> <p>Linux is NUMA-aware, but performance can still degrade if workloads or I/O devices aren't correctly aligned with the CPU topology. This matters especially when working with high-throughput components like NVMe devices and network interface cards (NICs).</p> <p>Simplyblock highly recommends a NUMA-aware configuration in multi-socket systems. To ensure optimal performance, each CPU socket should have a dedicated NIC and dedicated NVMe devices. These NICs and NVMe devices must be installed in PCI-e slots that are physically connected to their respective CPUs. This setup ensures low-latency, high-bandwidth data paths between the backing storage devices, the storage software, and the network.</p>"},{"location":"deployments/deployment-planning/numa-considerations/#checking-numa-configuration","title":"Checking NUMA Configuration","text":"<p>Before configuring simplyblock, the system configuration should be checked for multiple NUMA nodes. This can be done using the <code>lscpu</code> tool.</p> How to check the NUMA configuration<pre><code>lscpu | grep -i numa\n</code></pre> Example output of the NUMA configuration<pre><code>root@demo:~# lscpu | grep -i numa\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-31\nNUMA node1 CPU(s):                    32-63\n</code></pre> <p>In the example above, the system has two NUMA nodes.</p> <p>Recommendation</p> <p>If the system consists of multiple NUMA nodes, it is recommended to configure simplyblock with multiple storage nodes per storage host. The number of storage nodes should match the number of NUMA nodes.</p>"},{"location":"deployments/deployment-planning/numa-considerations/#ensuring-numa-aware-devices","title":"Ensuring NUMA-Aware Devices","text":"<p>For optimal performance, there should be a similar number of NVMe devices per NUMA node. Additionally, it is recommended to provide one Ethernet NIC per NUMA node. </p> <p>To check the NUMA assignment of PCI-e devices, the <code>lspci</code> tool and a small script can be used.</p> Install pciutils which includes lspci<pre><code>yum install pciutils\n</code></pre> Small script to list all PCI-e devices and their NUMA nodes<pre><code>#!/bin/bash\n\nfor i in  /sys/class/*/*/device; do\n    pci=$(basename \"$(readlink $i)\")\n    if [ -e $i/numa_node ]; then\n        echo \"NUMA Node: `cat $i/numa_node` ($i): `lspci -s $pci`\" ;\n    fi\ndone | sort\n</code></pre>"},{"location":"deployments/deployment-planning/recommendations/","title":"System Recommendations","text":"<p>Simplyblock provides system recommendations for three general types of cluster setups. These types are a High Performance, a High Density, and one Standard setup, with the former offering the highest throughput and the lowest latency, the second providing the highest storage capacity, and the latter providing a great compromise between the other two.</p>"},{"location":"deployments/deployment-planning/recommendations/#standard-setup","title":"Standard Setup","text":"<p>Per storage server:</p> <ul> <li>2U server with 2 CPU sockets.</li> <li>Each socket is supplied with 128 GB RAM.</li> <li>Each CPU offers 32 physical cores.</li> <li>11-12x PCIe 4.0 NVMe devices, up to 8 TB each.</li> <li>Dual 100 GBit/s NIC per socket configured as a LAG. Preferably, Mellanox Connect-X for RoCEv2.</li> </ul> <p>One of the NVMe devices per CPU socket could be implemented as an SLC NVMe, while all others should be TLC.</p>"},{"location":"deployments/deployment-planning/recommendations/#high-performance-setup","title":"High-Performance Setup","text":"<p>Per storage server:</p> <ul> <li>1U server with 2 CPU sockets.</li> <li>Each socket is supplied with 128 GB of RAM.</li> <li>Each CPU offers 32 physical cores.</li> <li>4-5x PCIe 4.0 NVMe devices with 3-4 TB capacity each.</li> <li>Dual 100 GBit/s NIC per socket configured as a LAG. Preferably, Mellanox Connect-X for RoCEv2.</li> </ul> <p>One of the NVMe devices per CPU socket could be implemented as an SLC NVMe, while all others should be TLC.</p>"},{"location":"deployments/deployment-planning/recommendations/#high-density-setup","title":"High-Density Setup","text":"<p>Per storage server:</p> <ul> <li>2U server with 2 CPU sockets.</li> <li>Each socket is supplied with 128 GB of RAM.</li> <li>Each CPU offers 32 physical cores.</li> <li>2x PCIe 4.0 SLC NVMe devices with 1 TB capacity each.</li> <li>6x PCIe 4.0 TLC NVMe devices with 4 TB capacity each.</li> <li>16x PCIe 4.0 NVMe devices, up to 120 TB capacity each.</li> <li>Dual 100 GBit/s NIC per socket configured as a LAG. Preferably, Mellanox Connect-X for RoCEv2.</li> </ul>"},{"location":"deployments/deployment-planning/recommendations/#considerations","title":"Considerations","text":"<p>For throughput-heavy workloads, the network bandwidth is the bottleneck. Increasing the network bandwidth will provide better performance.</p> <p>For IOPS-heavy workloads, the CPU will be the bottleneck. Increasing the number of CPU cores will provide better performance.</p> <p>For volume encryption, simplyblock utilizes offloading of the AES encryption into hardware. Intel provides better offloading capabilities and performance than AMD.</p> <p>ARM64 CPUs are fully supported on storage nodes and recommended for their high CPU core count.</p>"},{"location":"deployments/deployment-planning/recommendations/#aws-amazon-ec2-recommendations","title":"AWS Amazon EC2 Recommendations","text":"<p>Simplyblock can work with local instance storage (local NVMe devices) and Amazon EBS volumes. For performance reasons, Amazon EBS is not recommended for high-performance clusters.</p> <p>Generally, with AWS, there are three considerations when selecting virtual machine types:</p> <ul> <li>Minimum requirements of vCPU and RAM</li> <li>Locally attached NVMe devices</li> <li>Network performance (dedicated and \"up to\")</li> </ul> <p>Based on those criteria, simplyblock commonly recommends the following virtual machine types for storage nodes:</p> VM Type vCPU(s) RAM Locally Attached Storage Network Performance i3en.2xlarge 8 64 GB 2x 2500 GB Up to 25 GBit/s i3en.6xlarge 24 192 GB 2x 7500 GB 25 GBit/s i3en.12xlarge 48 384 GB 4x 7500 GB 50 GBit/s i3en.24xlarge 96 768 GB 8x 7500 GB 100 GBit/s m5d.4xlarge 16 64 GB 2x 300 GB 10 GBit/s i4i.8xlarge 32 256 GB 2x 3750 GB 18.75 GBit/s i4i.12xlarge 48 384 GB 3x 3750 GB 28.12 GBit/s"},{"location":"deployments/deployment-planning/recommendations/#google-compute-engine-recommendations","title":"Google Compute Engine Recommendations","text":"<p>In GCP, physical hosts are highly-shared and sliced into virtual machines. This isn't only true for network CPU, RAM, and network bandwidth, but also virtualized NVMe devices. Google Compute Engine NVMe devices provide a specific number of queue pairs (logical connections between the virtual machine and physical NVMe device) depending on the size of the disk. Hence, separately attached NVMe devices are highly recommended to achieve the required number of queue pairs of simplyblock.</p> <p>Generally, with GCP, there are three considerations when selecting virtual machine types:</p> <ul> <li>Minimum requirements of vCPU and RAM</li> <li>The size of the locally attached NVMe devices (SSD Storage)</li> <li>Network performance</li> </ul> <p>Based on those criteria, simplyblock commonly recommends the following virtual machine types for storage nodes:</p> VM Type vCPU(s) RAM Additional Local SSD Storage Network Performance n2-standard-8 8 32 GB 2x 2500 GB 16 GBit/s n2-standard-16 16 64 GB 2x 2500 GB 32 GBit/s n2-standard-32 32 128 GB 4x 2500 GB 32 GBit/s n2-standard-48 48 192 GB 4x 2500 GB 50 GBit/s n2-standard-48 48 192 GB 4x 2500 GB 50 GBit/s n2-standard-64 64 256 GB 6x 2500 GB 75 GBit/s n2-standard-80 64 320 GB 8x 2500 GB 100 GBit/s"},{"location":"deployments/deployment-planning/recommendations/#attaching-an-additional-local-ssd-on-google-compute-engine","title":"Attaching an additional Local SSD on Google Compute Engine","text":"<p>The above recommended instance types do not provide NVMe storage by default. It has to specifically be added to the virtual machine at creation time. It cannot be changed after the virtual machine is created.</p> <p>To add additional Local SSD Storage to a virtual machine, the operating system section must be selected in the wizard, then \"Add local SSD\" must be clicked. Now an additional disk can be added.</p> <p>Warning</p> <p>It is important that NVMe is selected as the interface type. SCSI will not work!</p> <p></p>"},{"location":"deployments/gcp/","title":"Google Compute Engine (GCP)","text":"<p>An installation on GCP's Compute Engine is mostly comparable to a bare-metal or virtualized installation. There are, however, a few minor differences. Hence, we decided to give it a separate documentation section.</p> <p>An installation on Google's Kubernetes Engine (GKE) is comparable to a standard Kubernetes installation. The differences between a GKE installation and a basic Kubernetes.</p> <ul> <li> <p> Prerequisites</p> <p> Node Sizing  Prerequisites</p> </li> <li> <p> Simplyblock Installation</p> <p> Install Simplyblock</p> </li> <li> <p> Kubernetes CSI Driver Installation</p> <p> Install Kubernetes CSI Driver</p> </li> <li> <p> Caching Node Installation</p> <p> Install Caching Nodes</p> </li> </ul>"},{"location":"deployments/gcp/data-migration/","title":"Data Migration","text":"<p>When migrating existing data to simplyblock, the process can be performed at the block level or the file system level, depending on the source system and migration requirements. Because Simplyblock provides logical Volumes (LVs) as virtual block devices, data can be migrated using standard block device cloning tools such as <code>dd</code>, as well as file-based tools like <code>rsync</code> after the block device has been formatted.</p> <p>Therefore, sata migration to simplyblock is a straightforward process using common block-level and file-level tools. For full disk cloning, <code>dd</code> and similar utilities are effective. For selective file migrations, <code>rsync</code> provides flexibility and reliability. Proper planning and validation of available storage capacity are essential to ensure successful and complete data transfers.</p>"},{"location":"deployments/gcp/data-migration/#block-level-migration-using-dd","title":"Block-Level Migration Using <code>dd</code>","text":"<p>A block-level copy duplicates the entire content of a source block device, including partition tables, file systems, and data. This method is ideal when migrating entire disks or volumes.</p> Creating a block-level clone of a block device<pre><code>dd if=/dev/source-device of=/dev/simplyblock-device bs=4M status=progress\n</code></pre> <ul> <li><code>if=</code> specifies the input (source) device.</li> <li><code>of=</code> specifies the output (Simplyblock Logical Volume) device.</li> <li><code>bs=4M</code> sets the block size for efficiency.</li> <li><code>status=progress</code> provides real-time progress updates.</li> </ul> <p>Info</p> <p>Ensure that the simplyblock logical volume is at least as large as the source device to prevent data loss.</p>"},{"location":"deployments/gcp/data-migration/#alternative-block-level-cloning-tools","title":"Alternative Block-Level Cloning Tools","text":"<p>Other block-level tools such as <code>Clonezilla</code>, <code>partclone</code>, or <code>dcfldd</code> may also be used for disk duplication, depending on the specific environment and desired features like compression or network transfer.</p>"},{"location":"deployments/gcp/data-migration/#file-level-migration-using-rsync","title":"File-Level Migration Using <code>rsync</code>","text":"<p>For scenarios where only file contents need to be migrated (for example, after creating a new file system on a simplyblock logical volume), <code>rsync</code> is a reliable tool.</p> <ol> <li> <p>First, format the Simplyblock Logical Volume:    </p>Format the simplyblock block device with ext4<pre><code>mkfs.ext4 /dev/simplyblock-device\n</code></pre> </li> <li> <p>Mount the Logical Volume:    </p>Mount the block device<pre><code>mount /dev/simplyblock-device /mnt/simplyblock\n</code></pre> </li> <li> <p>Use <code>rsync</code> to copy files from the source directory:    </p>Synchronize the source disks content using rsync<pre><code>rsync -avh --progress /source/data/ /mnt/simplyblock/\n</code></pre> <ul> <li><code>-a</code> preserves permissions, timestamps, and symbolic links.</li> <li><code>-v</code> provides verbose output.</li> <li><code>-h</code> makes output human-readable.</li> <li><code>--progress</code> shows transfer progress.</li> </ul> </li> </ol>"},{"location":"deployments/gcp/data-migration/#minimal-downtime-migration-strategy","title":"Minimal-Downtime Migration Strategy","text":"<p>An alternative, but more complex solution enables minimal downtime. This option utilizes the Linux <code>dm</code> (Device Mapper) subsystem.</p> <p>Using the Device Mapper, the current and new block devices will be moved into a RAID-1 and synchronized (re-silvered) in the background.  This solution requires two minimal downtimes to create and remount the devices.</p> <p>Warning</p> <p>This method is quite involved, requires a lot of steps, and can lead to data loss in case of wrong commands or parameters. It should only be used by advanced users that understand the danger of the commands below. Furthermore, this migration method MUST NOT be used for boot devices!</p> <p>In this walkthrough, we assume the new simplyblock logical volume is already connected to the system.</p>"},{"location":"deployments/gcp/data-migration/#preparation","title":"Preparation","text":"<p>To successfully execute this data migration, a few values are required. First of all, the two device names of the currently used and new device need to be collected.</p> <p>This can be done by executing the command <code>lsblk</code> to list all attached block devices.</p> lsblk provides information about all attached block devices<pre><code>lsblk\n</code></pre> <p>In this example, sda is the boot device which hosts the operating system, while sdb is the currently used block device and nvme0n1 is the newly attached simplyblock logical volume. The latter two should be noted down.</p> <p>Danger</p> <p>It is important to understand the difference between the currently used and the new device. Using them in the wrong order in the following steps will cause any or all data to be lost!</p> Find the source and target block devices using lsblk<pre><code>[root@demo ~]# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0   25G  0 disk\n\u251c\u2500sda1                      8:1    0    1G  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0    2G  0 part  /boot\n\u2514\u2500sda3                      8:3    0 21.9G  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0   11G  0 lvm   /\nsdb                         8:16   0   25G  0 disk\n\u2514\u2500sdb1                      8:17   0   25G  0 part  /data/pg\nsr0                        11:0    1 57.4M  0 rom\nnvme0n1                   259:0    0   25G  0 disk\n</code></pre> <p>Next up the cluster size of the current device is required. The value must be set on the RAID to-be-created. It needs to be noted down.</p> Find the block size of the source filesystem<pre><code>tune2fs -l /dev/sdb1 | grep -i 'block size'\n</code></pre> <p>In this example, the block size is 4 KiB (4096 bytes).</p> Example output of the block size<pre><code>[root@demo ~]# tune2fs -l /dev/sdb1 | grep -i 'block size'\nBlock size:               4096\n</code></pre> <p>Last, it is important to ensure that the new target device is at least as large or larger than the current device. <code>lsblk</code> can be used again to get the required numbers.</p> lsblk with byte sizes of the block devices<pre><code>lsblk -b\n</code></pre> <p>In this example, both devices are the same size, 26843545600 bytes in total disk capacity.</p> Example output of lsblk -b<pre><code>[root@demo ~]# lsblk -b\nNAME                      MAJ:MIN RM        SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0 26843545600  0 disk\n\u251c\u2500sda1                      8:1    0  1127219200  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0  2147483648  0 part  /boot\n\u2514\u2500sda3                      8:3    0 23566745600  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0 11781799936  0 lvm   /\nsdb                         8:16   0 26843545600  0 disk\n\u2514\u2500sdb1                      8:17   0 26843513344  0 part  /data/pg\nsr0                        11:0    1    60225536  0 rom\nnvme0n1                   259:0    0 26843545600  0 disk\n</code></pre>"},{"location":"deployments/gcp/data-migration/#device-mapper-raid-setup","title":"Device Mapper RAID Setup","text":"<p>Danger</p> <p>From here on out, mistakes can cause any or all data to be lost! It is strongly recommended to only go further, if ensured that the values above are correct and after a full data backup is created. It is also recommended to test the backup before continuing. A failure to do so can cause issues in case it cannot be replayed.</p> <p>Now, it's time to create the temporary RAID for disk synchronization. Anything beyond this point is dangerous.</p> <p>Warning</p> <p>Any service accessing the current block device or any of its partitions need to be shutdown and the block device and its partitions need to be unmounted. It is required for the device to not be busy. </p>Example of PostgreSQL shutdown and partition unmount<pre><code>service postgresql stop\numount /data/pg\n</code></pre> Building a RAID-1 with mdadm<pre><code>mdadm --build --chunk=&lt;CHUNK_SIZE&gt; --level=1 \\\n    --raid-devices=2 --bitmap=none \\\n    &lt;RAID_NAME&gt; &lt;CURRENT_DEVICE_FILE&gt; missing\n</code></pre> <p>In this example, the RAID is created using the /dev/sdb device file and 4096 as the chunk size. The newly created RAID is called migration. The RAID-level is 1 (meaning, RAID-1) and it includes 2 devices. The missing at the end of the command is required to tell the device mapper that the second device of the RAID is missing for now. It will be added later.</p> Example output of a RAID-1 with mdadm<pre><code>[root@demo ~]# mdadm --build --chunk=4096 --level=1 --raid-devices=2 --bitmap=none migration /dev/sdb missing\nmdadm: array /dev/md/migration built and started.\n</code></pre> <p>To ensure that the RAID was created successfully, all device files with /dev/md* can be listed. In this case, /dev/md127 is the actual RAID device, while /dev/md/migration is the device mapper file.</p> Finding the new device mapper device files<pre><code>[root@demo ~]# ls /dev/md*\n/dev/md127  /dev/md127p1\n\n/dev/md:\nmigration  migration1\n</code></pre> <p>After the RAID device name is confirmed, the new RAID device can be mounted. In this example, the original block device was partitioned. Hence, the RAID device also has one partition /dev/md127p1. This is what needs to be mounted to the same mount point as the original disk before, /data/pg in this example.</p> Mount the new device mapper device file<pre><code>[root@demo ~]# mount /dev/md127p1 /data/pg/\n</code></pre> <p>Info</p> <p>All services that require access to the data can be started again. The RAID itself is still in a degraded state, but it provides the same data security as the original device.</p> <p>Now the second, new device must be added to the RAID setup to start the re-silvering (data synchronization) process. This is again done using <code>mdadm</code> tool.</p> Add the new simplyblock block device to RAID-1<pre><code>mdadm &lt;RAID_DEVICE_MAPPER_FILE&gt; --add &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>In the example, we add /dev/nvme0n1 (the simplyblock logical volume) to the RAID named \"migration.\"</p> Example output of mdadm --add<pre><code>[root@demo ~]# mdadm /dev/md/migration --add /dev/nvme0n1\nmdadm: added /dev/nvme0n1\n</code></pre> <p>After the device was added to the RAID setup, a background process is automatically started to synchronize the newly added device to the first device in the setup. This process is called re-silvering.</p> <p>Info</p> <p>While the devices are synchronized, the read and write performance may be impacted due to the additional I/O operations of the synchronization process. However, the process runs on a very low priority and shouldn't impact the live operation too extensively. For AWS users: if the migration uses an Amazon EBS volume as the source, ensure enough IOPS to cover live operation and migration.</p> <p>The synchronization process status can be monitored using one of two commands:</p> Check status of re-silvering<pre><code>mdadm -D &lt;RAID_DEVICE_FILE&gt;\ncat /proc/mdstat\n</code></pre> Example output of a status check via mdadm<pre><code>[root@demo ~]#mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean, degraded, recovering\n    Active Devices : 1\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 1\n\nConsistency Policy : resync\n\n    Rebuild Status : 98% complete\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      spare rebuilding   /dev/nvme0n1\n</code></pre> Example output of a status check via /proc/mdstat<pre><code>[root@demo ~]# cat /proc/mdstat \nPersonalities : [raid1] \nmd0 : active raid1 sdb[1] nvme0n1[0]\n      10484664 blocks super 1.2 [2/2] [UU]\n      [========&gt;............]  resync = 42.3% (4440832/10484664) finish=0.4min speed=201856K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>"},{"location":"deployments/gcp/data-migration/#after-the-synchronization-is-done","title":"After the Synchronization is done","text":"<p>Eventually, the synchronization finishes. At this point, the two devices (original and new) are kept in sync by the device mapper system.</p> Example out of a finished synchronzation<pre><code>[root@demo ~]# mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean\n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      active sync   /dev/nvme0n1\n</code></pre> <p>To fully switch to the new simplyblock logical volume, a second, minimal, downtime is required.</p> <p>The RAID device needs to be unmounted and the device mapper stopped.</p> Stopping the device mapper RAID-1<pre><code>umount &lt;MOUNT_POINT&gt;\nmdadm --stop &lt;DEVICE_MAPPER_FILE&gt;\n</code></pre> <p>In this example /data/pg and /dev/md/migration are used.</p> Example output of a stopped RAID-1<pre><code>[root@demo ~]# umount /data/pg/\n[root@demo ~]# mdadm --stop /dev/md/migration\nmdadm: stopped /dev/md/migration\n</code></pre> <p>Now, the system should be restarted. If a system reboot takes too long and is out of the scope of the available maintenance window, a re-read of the partition tables can be forced.</p> Re-read partition table<pre><code>blockdev --rereadpt &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>After re-reading the partition table of a device, the partition should be recognized and visible.</p> Example output of re-reading the partition table<pre><code>[root@demo ~]# blockdev --rereadpt /dev/nvme0n1\n[root@demo ~]# ls /dev/nvme0n1p1\n/dev/nvme0n1p1\n</code></pre> <p>As a last step, the partition must be mounted to the same mount point as the RAID device before. If the mount is successful, the services can be started again.</p> Mounting the plain block device and restarting services<pre><code>[root@demo ~]# mount /dev/nvme0n1p1 /data/pg/\n[root@demo ~]# service postgresql start\n</code></pre>"},{"location":"deployments/gcp/install-caching-nodes/","title":"Install Caching Nodes (Kubernetes)","text":"<p>Caching nodes are simplyblock storage containers co-located with the workloads on Kubernetes workers. They utilize directly attached NVMe disks on the worker nodes to provide an ultra-low-latency write-through cache for disaggregated or hybrid simplyblock clusters.</p>"},{"location":"deployments/gcp/install-caching-nodes/#prerequisites","title":"Prerequisites","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>Caching nodes, like storage nodes, require huge page memory to hold the internal state. Huge pages should be 2MiB in size, and a minimum of 4096 huge pages should be allocated at boot time of the operating system.</p> <pre><code>demo@worker-1 ~&gt; sudo sysctl -w vm.nr_hugepages=4096\n</code></pre> <p>Info</p> <p>To see how huge pages can be pre-reserved at boot time, see the node sizing documentation section on Huge Pages.</p> <pre><code>demo@worker-1 ~&gt; sudo systemctl restart kubelet\n</code></pre> <pre><code>demo@worker-1 ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n</code></pre> <pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi      0 (0%)    0 (0%)\n</code></pre> <pre><code>demo@worker-1 ~&gt; sudo yum install -y nvme-cli\ndemo@worker-1 ~&gt; sudo modprobe nvme-tcp\ndemo@worker-1 ~&gt; sudo modprobe nbd\n</code></pre> <pre><code>demo@demo ~&gt; kubectl label nodes worker-1.kubernetes-cluster.local type=simplyblock-cache\n</code></pre> <p>Afterward, the worker node can be described to check that the node-selector type is successfully set.</p> Example output of a successfully configured worker node<pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep simplyblock-cache\n                    type=simplyblock-cache\n</code></pre> Example Caching Node-enabled StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  type: cache\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock-csi/","title":"Install Simplyblock CSI","text":"<p>Simplyblock provides a seamless integration with Kubernetes through its Kubernetes CSI driver.</p>"},{"location":"deployments/gcp/install-simplyblock-csi/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p> <p>To install the Simplyblock CSI Driver, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended. If a manual installation is preferred, see the CSI Driver Repository\u00a0\u29c9.</p> <p>Either way, the installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying the CSI driver is the control plane address. It is recommended to front the simplyblock API with an AWS load balancer service. Hence, your control plane address is the \"public\" endpoint of this load balancer.</p> <p>Anyhow, deploying the Simplyblock CSI Driver using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver.</p> Install Simplyblock's CSI Driver<pre><code>CLUSTER_UUID=\"&lt;UUID&gt;\"\nCLUSTER_SECRET=\"&lt;SECRET&gt;\"\nCNTR_ADDR=\"&lt;CONTROL-PLANE-ADDR&gt;\"\nPOOL_NAME=\"&lt;POOL-NAME&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm\nhelm repo update\nhelm install -n simplyblock-csi --create-namespace simplyblock-csi simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n    --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n    --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n    --set logicalVolume.pool_name=${POOL_NAME}\n</code></pre> Example output of the CSI driver deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\n...Successfully got an update from the \"kasten\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock-csi --create-namespace simplyblock-csi simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock-csi\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock-csi get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock-csi get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/","title":"Install Simplyblock Storage Cluster","text":"<p>Installing simplyblock for production requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance, and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides two test scripts to automatically check your system's configuration. While those may not catch all edge cases, they can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configurations during the process.</p> Automatically check your configurations<pre><code># Configuration check for the control plane (management nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n\n# Configuration check for the storage plane (storage nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> <li>Secondary nodes are part of a specific storage cluster and enable automatic fail over for NVMe-oF connections. In a high-availability cluster, every primary storage node automatically provides a secondary storage node. </li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, and the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/gcp/install-simplyblock/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separate subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24 <p>Warning</p> <p>Simplyblock strongly recommends setting up individual networks for the storage plane and control plane traffic.  </p>"},{"location":"deployments/gcp/install-simplyblock/#google-kubernetes-engine-gke","title":"Google Kubernetes Engine (GKE)","text":"<p>Info</p> <p>If simplyblock is to be installed into Google Kubernetes Engine (GKE), the Kubernetes documentation section has the necessary step-by-step guide.</p>"},{"location":"deployments/gcp/install-simplyblock/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/gcp/install-simplyblock/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/gcp/install-simplyblock/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets are available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/gcp/install-simplyblock/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Docker Swarm, by default, creates iptables entries open to the world. If no external firewall is available, the created iptables configuration needs to be restricted.</p> <p>The following script will create additional iptables rules prepended to Docker's forwarding rules and only enabling access from internal networks. This script should be stored in /usr/local/sbin/simplyblock-iptables.sh.</p> Configuration script for Iptables<pre><code>#!/usr/bin/env bash\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080:8890 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9090-9900 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre> <p>To automatically run this script whenever Docker is started or restarted, it must be attached to a Systemd service, stored as /etc/systemd/system/simplyblock-iptables.service.</p> Systemd script to set up Iptables<pre><code>[Unit]\nDescription=Simplyblock Iptables Restrictions for Docker \nAfter=docker.service\nBindsTo=docker.service\nReloadPropagatedFrom=docker.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/simplyblock-iptables.sh\nExecReload=/usr/local/sbin/simplyblock-iptables.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>After both files are stored in their respective locations, the bash script needs to be made executable, and the Systemd service needs to be enabled to start automatically.</p> Enabling service file<pre><code>chmod +x /usr/local/sbin/simplyblock-iptables.sh\nsystemctl enable simplyblock-iptables.service\nsystemctl start simplyblock-iptables.service\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) are installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/gcp/install-simplyblock/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/gcp/install-simplyblock/#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>With all NVMe devices prepared and the NVMe/TCP driver loaded, the storage node software can be deployed.</p> <p>The actual deployment process happens in three steps: - Creating the storage node configuration - Deploy the first stage (the storage node API) - Deploy the second stage (add the storage node to the cluster), happening from a management node</p> <p>The configuration process creates the configuration file, which contains all the assignments of NVMe devices, NICs, and potentially available NUMA nodes. By default, simplyblock will configure one storage node per NUMA node.</p> Configure the storage node<pre><code>sudo sbctl storage-node configure \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-size &lt;MAX_PROVISIONING_CAPACITY&gt;\n</code></pre> Example output of storage node configure<pre><code>[demo@demo-3 ~]# sudo sbctl sn configure --nodes-per-socket=2 --max-lvol=50 --max-size=1T\n2025-05-14 10:40:17,460: INFO: 0000:00:04.0 is already bound to nvme.\n0000:00:1e.0\n0000:00:1e.0\n0000:00:1f.0\n0000:00:1f.0\n0000:00:1e.0\n0000:00:1f.0\n2025-05-14 10:40:17,841: INFO: JSON file successfully written to /etc/simplyblock/sn_config_file\n2025-05-14 10:40:17,905: INFO: JSON file successfully written to /etc/simplyblock/system_info\nTrue\n</code></pre> <p>A full set of the parameters for the configure subcommand can be found in the CLI reference. </p> <p>After the configuration has been created, the first stage deployment can be executed </p> Deploy the storage node<pre><code>sudo sbctl storage-node deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbctl storage-node deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p> <p>When all storage nodes are added, it's finally time to activate the storage plane.</p>"},{"location":"deployments/gcp/install-simplyblock/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step, after all nodes are added to the storage cluster, is to activate the storage plane.</p> Storage cluster activation<pre><code>sudo sbctl cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this, and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbctl cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre> <p>Now that the cluster is ready, it is time to install the Kubernetes CSI Driver or learn how to use the simplyblock storage cluster to manually provision logical volumes.</p>"},{"location":"deployments/gcp/prerequisites/","title":"Prerequisites","text":"<p>When installing simplyblock control planes and storage planes, a number of prerequisites are important to understand.</p> <p>Simplyblock uses Docker Swarm for the control plane cluster. In case of a bare metal or virtualized installation, it will also use Docker Swarm for the storage plane. Hence, Docker has to be installed.</p> <p>Furthermore, simplyblock requires installing the <code>sbctl</code> command line tool. This tool is written in Python. Therefore, Python (3.5 or later) has to be installed. Likewise, pip, the Python package manager, has to be installed with version 20 or later.</p> <p>To install <code>sbctl</code> run:</p> <pre><code>sudo pip install sbctl --upgrade\n</code></pre>"},{"location":"deployments/gcp/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/gcp/prerequisites/#node-instance-type","title":"Node Instance Type","text":"<p>Simplyblock recommends pre-tested and verified instance types. Those instance types are:</p> <ul> <li>i3en.6xlarge</li> <li>i4i.8xlarge</li> </ul>"},{"location":"deployments/gcp/prerequisites/#network-configuration","title":"Network Configuration","text":"<p>Simplyblock requires a number of network ports to be available from different networks. The configuration of the required network ports are provided in the installation documentation.</p> <p>Additionally, IPv6 must be disabled on all nodes running simplyblock.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"},{"location":"deployments/gcp/prerequisites/#google-compute-engine-networks","title":"Google Compute Engine Networks","text":"<p>Specifically for GCP, simplyblock strongly advises using individual networks for the control plane and storage plane.</p> <p>For access to the Cluster Management API, simplyblock recommends using an GCP load balancer as a front instead of making the API available directly.</p>"},{"location":"deployments/gcp/prerequisites/#network-ports-for-control-plane","title":"Network Ports for Control Plane","text":"Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP"},{"location":"deployments/gcp/prerequisites/#network-ports-for-storage-plane","title":"Network Ports for Storage Plane","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP"},{"location":"deployments/gcp/prerequisites/#storage-configuration","title":"Storage Configuration","text":"<p>Simplyblock has certain requirements in terms of storage. While the most important facts are provided in the installation section, here are things to consider.</p>"},{"location":"deployments/gcp/prerequisites/#root-volume","title":"Root Volume","text":"<p>The volume mounted as the root directory has to provide at least 35GiB of free capacity. More free space is recommended, especially for control plane nodes, which collect logs and the cluster state.</p>"},{"location":"deployments/kubernetes/","title":"Kubernetes","text":"<p>Installing simplyblock into and using it with Kubernetes requires two or more components to be installed. The number of components depends on your deployment strategy and requirements.</p> <p>For Kubernetes-related installations, simplyblock provides three deployment models: hyper-converged (also known as co-located), disaggregated, and a hybrid model which combines the best of the former two.</p>"},{"location":"deployments/kubernetes/#prerequisites","title":"Prerequisites","text":"<p>Before starting with the installation of simplyblock, make yourself familiar with the requirements and prerequisites of simplyblock. You can find all necessary information under the Prerequisites section specific to Kubernetes deployments.</p>"},{"location":"deployments/kubernetes/#installation","title":"Installation","text":"<p>After making sure that all requirements are fulfilled, you can start with the installation. Follow the necessary section depending on your chosen deployment model:</p> <ul> <li>Hyper-Converged Setup</li> <li>Disaggregated Setup</li> <li>Hybrid Setup</li> </ul> <p>In either case, you start with installing the control plane, before going over to the actual storage cluster and the Kubernetes CSI driver.</p> <p>As a last step, you may want to install caching nodes on your Kubernetes workers to improve access latency. See the installation steps in the Install Caching Nodes section.</p>"},{"location":"deployments/kubernetes/data-migration/","title":"Data Migration","text":"<p>When migrating existing data to simplyblock, the process can be performed at the block level or the file system level, depending on the source system and migration requirements. Because Simplyblock provides logical Volumes (LVs) as virtual block devices, data can be migrated using standard block device cloning tools such as <code>dd</code>, as well as file-based tools like <code>rsync</code> after the block device has been formatted.</p> <p>Therefore, sata migration to simplyblock is a straightforward process using common block-level and file-level tools. For full disk cloning, <code>dd</code> and similar utilities are effective. For selective file migrations, <code>rsync</code> provides flexibility and reliability. Proper planning and validation of available storage capacity are essential to ensure successful and complete data transfers.</p>"},{"location":"deployments/kubernetes/data-migration/#block-level-migration-using-dd","title":"Block-Level Migration Using <code>dd</code>","text":"<p>A block-level copy duplicates the entire content of a source block device, including partition tables, file systems, and data. This method is ideal when migrating entire disks or volumes.</p> Creating a block-level clone of a block device<pre><code>dd if=/dev/source-device of=/dev/simplyblock-device bs=4M status=progress\n</code></pre> <ul> <li><code>if=</code> specifies the input (source) device.</li> <li><code>of=</code> specifies the output (Simplyblock Logical Volume) device.</li> <li><code>bs=4M</code> sets the block size for efficiency.</li> <li><code>status=progress</code> provides real-time progress updates.</li> </ul> <p>Info</p> <p>Ensure that the simplyblock logical volume is at least as large as the source device to prevent data loss.</p>"},{"location":"deployments/kubernetes/data-migration/#alternative-block-level-cloning-tools","title":"Alternative Block-Level Cloning Tools","text":"<p>Other block-level tools such as <code>Clonezilla</code>, <code>partclone</code>, or <code>dcfldd</code> may also be used for disk duplication, depending on the specific environment and desired features like compression or network transfer.</p>"},{"location":"deployments/kubernetes/data-migration/#file-level-migration-using-rsync","title":"File-Level Migration Using <code>rsync</code>","text":"<p>For scenarios where only file contents need to be migrated (for example, after creating a new file system on a simplyblock logical volume), <code>rsync</code> is a reliable tool.</p> <ol> <li> <p>First, format the Simplyblock Logical Volume:    </p>Format the simplyblock block device with ext4<pre><code>mkfs.ext4 /dev/simplyblock-device\n</code></pre> </li> <li> <p>Mount the Logical Volume:    </p>Mount the block device<pre><code>mount /dev/simplyblock-device /mnt/simplyblock\n</code></pre> </li> <li> <p>Use <code>rsync</code> to copy files from the source directory:    </p>Synchronize the source disks content using rsync<pre><code>rsync -avh --progress /source/data/ /mnt/simplyblock/\n</code></pre> <ul> <li><code>-a</code> preserves permissions, timestamps, and symbolic links.</li> <li><code>-v</code> provides verbose output.</li> <li><code>-h</code> makes output human-readable.</li> <li><code>--progress</code> shows transfer progress.</li> </ul> </li> </ol>"},{"location":"deployments/kubernetes/data-migration/#minimal-downtime-migration-strategy","title":"Minimal-Downtime Migration Strategy","text":"<p>An alternative, but more complex solution enables minimal downtime. This option utilizes the Linux <code>dm</code> (Device Mapper) subsystem.</p> <p>Using the Device Mapper, the current and new block devices will be moved into a RAID-1 and synchronized (re-silvered) in the background.  This solution requires two minimal downtimes to create and remount the devices.</p> <p>Warning</p> <p>This method is quite involved, requires a lot of steps, and can lead to data loss in case of wrong commands or parameters. It should only be used by advanced users that understand the danger of the commands below. Furthermore, this migration method MUST NOT be used for boot devices!</p> <p>In this walkthrough, we assume the new simplyblock logical volume is already connected to the system.</p>"},{"location":"deployments/kubernetes/data-migration/#preparation","title":"Preparation","text":"<p>To successfully execute this data migration, a few values are required. First of all, the two device names of the currently used and new device need to be collected.</p> <p>This can be done by executing the command <code>lsblk</code> to list all attached block devices.</p> lsblk provides information about all attached block devices<pre><code>lsblk\n</code></pre> <p>In this example, sda is the boot device which hosts the operating system, while sdb is the currently used block device and nvme0n1 is the newly attached simplyblock logical volume. The latter two should be noted down.</p> <p>Danger</p> <p>It is important to understand the difference between the currently used and the new device. Using them in the wrong order in the following steps will cause any or all data to be lost!</p> Find the source and target block devices using lsblk<pre><code>[root@demo ~]# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0   25G  0 disk\n\u251c\u2500sda1                      8:1    0    1G  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0    2G  0 part  /boot\n\u2514\u2500sda3                      8:3    0 21.9G  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0   11G  0 lvm   /\nsdb                         8:16   0   25G  0 disk\n\u2514\u2500sdb1                      8:17   0   25G  0 part  /data/pg\nsr0                        11:0    1 57.4M  0 rom\nnvme0n1                   259:0    0   25G  0 disk\n</code></pre> <p>Next up the cluster size of the current device is required. The value must be set on the RAID to-be-created. It needs to be noted down.</p> Find the block size of the source filesystem<pre><code>tune2fs -l /dev/sdb1 | grep -i 'block size'\n</code></pre> <p>In this example, the block size is 4 KiB (4096 bytes).</p> Example output of the block size<pre><code>[root@demo ~]# tune2fs -l /dev/sdb1 | grep -i 'block size'\nBlock size:               4096\n</code></pre> <p>Last, it is important to ensure that the new target device is at least as large or larger than the current device. <code>lsblk</code> can be used again to get the required numbers.</p> lsblk with byte sizes of the block devices<pre><code>lsblk -b\n</code></pre> <p>In this example, both devices are the same size, 26843545600 bytes in total disk capacity.</p> Example output of lsblk -b<pre><code>[root@demo ~]# lsblk -b\nNAME                      MAJ:MIN RM        SIZE RO TYPE  MOUNTPOINTS\nsda                         8:0    0 26843545600  0 disk\n\u251c\u2500sda1                      8:1    0  1127219200  0 part  /boot/efi\n\u251c\u2500sda2                      8:2    0  2147483648  0 part  /boot\n\u2514\u2500sda3                      8:3    0 23566745600  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 252:0    0 11781799936  0 lvm   /\nsdb                         8:16   0 26843545600  0 disk\n\u2514\u2500sdb1                      8:17   0 26843513344  0 part  /data/pg\nsr0                        11:0    1    60225536  0 rom\nnvme0n1                   259:0    0 26843545600  0 disk\n</code></pre>"},{"location":"deployments/kubernetes/data-migration/#device-mapper-raid-setup","title":"Device Mapper RAID Setup","text":"<p>Danger</p> <p>From here on out, mistakes can cause any or all data to be lost! It is strongly recommended to only go further, if ensured that the values above are correct and after a full data backup is created. It is also recommended to test the backup before continuing. A failure to do so can cause issues in case it cannot be replayed.</p> <p>Now, it's time to create the temporary RAID for disk synchronization. Anything beyond this point is dangerous.</p> <p>Warning</p> <p>Any service accessing the current block device or any of its partitions need to be shutdown and the block device and its partitions need to be unmounted. It is required for the device to not be busy. </p>Example of PostgreSQL shutdown and partition unmount<pre><code>service postgresql stop\numount /data/pg\n</code></pre> Building a RAID-1 with mdadm<pre><code>mdadm --build --chunk=&lt;CHUNK_SIZE&gt; --level=1 \\\n    --raid-devices=2 --bitmap=none \\\n    &lt;RAID_NAME&gt; &lt;CURRENT_DEVICE_FILE&gt; missing\n</code></pre> <p>In this example, the RAID is created using the /dev/sdb device file and 4096 as the chunk size. The newly created RAID is called migration. The RAID-level is 1 (meaning, RAID-1) and it includes 2 devices. The missing at the end of the command is required to tell the device mapper that the second device of the RAID is missing for now. It will be added later.</p> Example output of a RAID-1 with mdadm<pre><code>[root@demo ~]# mdadm --build --chunk=4096 --level=1 --raid-devices=2 --bitmap=none migration /dev/sdb missing\nmdadm: array /dev/md/migration built and started.\n</code></pre> <p>To ensure that the RAID was created successfully, all device files with /dev/md* can be listed. In this case, /dev/md127 is the actual RAID device, while /dev/md/migration is the device mapper file.</p> Finding the new device mapper device files<pre><code>[root@demo ~]# ls /dev/md*\n/dev/md127  /dev/md127p1\n\n/dev/md:\nmigration  migration1\n</code></pre> <p>After the RAID device name is confirmed, the new RAID device can be mounted. In this example, the original block device was partitioned. Hence, the RAID device also has one partition /dev/md127p1. This is what needs to be mounted to the same mount point as the original disk before, /data/pg in this example.</p> Mount the new device mapper device file<pre><code>[root@demo ~]# mount /dev/md127p1 /data/pg/\n</code></pre> <p>Info</p> <p>All services that require access to the data can be started again. The RAID itself is still in a degraded state, but it provides the same data security as the original device.</p> <p>Now the second, new device must be added to the RAID setup to start the re-silvering (data synchronization) process. This is again done using <code>mdadm</code> tool.</p> Add the new simplyblock block device to RAID-1<pre><code>mdadm &lt;RAID_DEVICE_MAPPER_FILE&gt; --add &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>In the example, we add /dev/nvme0n1 (the simplyblock logical volume) to the RAID named \"migration.\"</p> Example output of mdadm --add<pre><code>[root@demo ~]# mdadm /dev/md/migration --add /dev/nvme0n1\nmdadm: added /dev/nvme0n1\n</code></pre> <p>After the device was added to the RAID setup, a background process is automatically started to synchronize the newly added device to the first device in the setup. This process is called re-silvering.</p> <p>Info</p> <p>While the devices are synchronized, the read and write performance may be impacted due to the additional I/O operations of the synchronization process. However, the process runs on a very low priority and shouldn't impact the live operation too extensively. For AWS users: if the migration uses an Amazon EBS volume as the source, ensure enough IOPS to cover live operation and migration.</p> <p>The synchronization process status can be monitored using one of two commands:</p> Check status of re-silvering<pre><code>mdadm -D &lt;RAID_DEVICE_FILE&gt;\ncat /proc/mdstat\n</code></pre> Example output of a status check via mdadm<pre><code>[root@demo ~]#mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean, degraded, recovering\n    Active Devices : 1\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 1\n\nConsistency Policy : resync\n\n    Rebuild Status : 98% complete\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      spare rebuilding   /dev/nvme0n1\n</code></pre> Example output of a status check via /proc/mdstat<pre><code>[root@demo ~]# cat /proc/mdstat \nPersonalities : [raid1] \nmd0 : active raid1 sdb[1] nvme0n1[0]\n      10484664 blocks super 1.2 [2/2] [UU]\n      [========&gt;............]  resync = 42.3% (4440832/10484664) finish=0.4min speed=201856K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>"},{"location":"deployments/kubernetes/data-migration/#after-the-synchronization-is-done","title":"After the Synchronization is done","text":"<p>Eventually, the synchronization finishes. At this point, the two devices (original and new) are kept in sync by the device mapper system.</p> Example out of a finished synchronzation<pre><code>[root@demo ~]# mdadm -D /dev/md127\n/dev/md127:\n           Version :\n     Creation Time : Sat Mar 15 17:24:17 2025\n        Raid Level : raid1\n        Array Size : 26214400 (25.00 GiB 26.84 GB)\n     Used Dev Size : 26214400 (25.00 GiB 26.84 GB)\n      Raid Devices : 2\n     Total Devices : 2\n\n             State : clean\n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       2     259        0        1      active sync   /dev/nvme0n1\n</code></pre> <p>To fully switch to the new simplyblock logical volume, a second, minimal, downtime is required.</p> <p>The RAID device needs to be unmounted and the device mapper stopped.</p> Stopping the device mapper RAID-1<pre><code>umount &lt;MOUNT_POINT&gt;\nmdadm --stop &lt;DEVICE_MAPPER_FILE&gt;\n</code></pre> <p>In this example /data/pg and /dev/md/migration are used.</p> Example output of a stopped RAID-1<pre><code>[root@demo ~]# umount /data/pg/\n[root@demo ~]# mdadm --stop /dev/md/migration\nmdadm: stopped /dev/md/migration\n</code></pre> <p>Now, the system should be restarted. If a system reboot takes too long and is out of the scope of the available maintenance window, a re-read of the partition tables can be forced.</p> Re-read partition table<pre><code>blockdev --rereadpt &lt;NEW_DEVICE_FILE&gt;\n</code></pre> <p>After re-reading the partition table of a device, the partition should be recognized and visible.</p> Example output of re-reading the partition table<pre><code>[root@demo ~]# blockdev --rereadpt /dev/nvme0n1\n[root@demo ~]# ls /dev/nvme0n1p1\n/dev/nvme0n1p1\n</code></pre> <p>As a last step, the partition must be mounted to the same mount point as the RAID device before. If the mount is successful, the services can be started again.</p> Mounting the plain block device and restarting services<pre><code>[root@demo ~]# mount /dev/nvme0n1p1 /data/pg/\n[root@demo ~]# service postgresql start\n</code></pre>"},{"location":"deployments/kubernetes/install-caching-nodes/","title":"Install Caching Nodes","text":"<p>Caching nodes are simplyblock storage containers co-located with the workloads on Kubernetes workers. They utilize directly attached NVMe disks on the worker nodes to provide an ultra-low-latency write-through cache for disaggregated or hybrid simplyblock clusters.</p>"},{"location":"deployments/kubernetes/install-caching-nodes/#prerequisites","title":"Prerequisites","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>Caching nodes, like storage nodes, require huge page memory to hold the internal state. Huge pages should be 2MiB in size, and a minimum of 4096 huge pages should be allocated at boot time of the operating system.</p> <pre><code>demo@worker-1 ~&gt; sudo sysctl -w vm.nr_hugepages=4096\n</code></pre> <p>Info</p> <p>To see how huge pages can be pre-reserved at boot time, see the node sizing documentation section on Huge Pages.</p> <pre><code>demo@worker-1 ~&gt; sudo systemctl restart kubelet\n</code></pre> <pre><code>demo@worker-1 ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n</code></pre> <pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep hugepages-2Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi      0 (0%)    0 (0%)\n</code></pre> <pre><code>demo@worker-1 ~&gt; sudo yum install -y nvme-cli\ndemo@worker-1 ~&gt; sudo modprobe nvme-tcp\ndemo@worker-1 ~&gt; sudo modprobe nbd\n</code></pre> <pre><code>demo@demo ~&gt; kubectl label nodes worker-1.kubernetes-cluster.local type=simplyblock-cache\n</code></pre> <p>Afterward, the worker node can be described to check that the node-selector type is successfully set.</p> Example output of a successfully configured worker node<pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | grep simplyblock-cache\n                    type=simplyblock-cache\n</code></pre> Example Caching Node-enabled StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  type: cache\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/","title":"Prerequisites","text":"<p>Before deploying simplyblock on a Kubernetes cluster, it is essential to ensure that the environment meets all necessary infrastructure, software, and configuration requirements. Proper planning and preparation will help guarantee a smooth installation, optimal performance, and long-term stability of the simplyblock storage system.</p> <p>This section outlines the key hardware and software prerequisites, including supported Kubernetes versions, required resources for management and storage nodes, necessary permissions, network configurations, and storage prerequisites.</p> <p>Verifying these requirements before installation will help avoid compatibility issues and ensure that Simplyblock integrates seamlessly with your Kubernetes deployment.</p>"},{"location":"deployments/kubernetes/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/kubernetes/prerequisites/#control-plane","title":"Control Plane","text":"<p>Minimum 2 physical cores Minimum 8GiB 35 GiB unused disk space Supported Linux distribution 3 management nodes</p> Create Management Cluster<pre><code>sbctl cluster create\nsbctl cluster list\n</code></pre> Sample output control plane creation<pre><code>[demo@demo ~]# sudo sbctl cluster create --ifname=ens18 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.151\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655 # (1)\n</code></pre> <ol> <li> This is the cluster id: 7bef076c-82b7-46a5-9f30-8c938b30e655</li> </ol> <p>Returns cluster-id</p> Example output for listing available clusters<pre><code>[demo@demo ~]# sudo sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status  |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n| 7bef076c-82b7-46a5-9f30-8c938b30e655 | nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655 | ha      | False | 3          | 10             | 1x1 | unready |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/#get-cluster-secret","title":"Get Cluster Secret","text":"<p>sbctl cluster get-secret </p> Example output get cluster secret<pre><code>[demo@demo ~]# sudo sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0 # (1)\n</code></pre> <ol> <li> This is the cluster secret: e8SQ1ElMm8Y9XIwyn8O0</li> </ol>"},{"location":"deployments/kubernetes/prerequisites/#control-plane-secondaries","title":"Control Plane Secondaries","text":"Adding a management node to the control plane<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\nsudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\nsbctl mgmt add 192.168.10.151 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 ens18\nsbctl mgmt add &lt;CTR_PLANE_PRI_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> Example output joining a control plane cluster<pre><code>[root@vm12 ~]# sbctl mgmt add 192.168.10.151 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 ens18\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.152\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/#network-configuration","title":"Network Configuration","text":""},{"location":"deployments/kubernetes/prerequisites/#disable-ipv6","title":"Disable IPv6","text":"Permanently disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Defined networks:</p> <ul> <li>internal: The subnet for communication with the control plane</li> <li>storage: The subnet for communication with and between the storage plane</li> <li>loadbalancer: The subnet between the load balancer and the control plane</li> <li>management: Valid IPs or IP ranges for direct management access</li> </ul> Direction Source or target nw ports protocol ingress management 80 tcp ingress mgmt 3000 tcp ingress mgmt 9000 tcp egress all all all Service Direction Source Network Port Protocol(s) API (HTTPS) ingress loadbalancer 80 TCP SSH ingress management 22 TCP Grafana ingress loadbalancer 3000 TCP Graylog ingress loadbalancer 9000 TCP Docker Swarm ingress storage, internal 2375 TCP Docker Swarm ingress storage, internal 2377 TCP Docker Swarm ingress storage, internal 4789 UDP Docker Swarm ingress storage, internal 7946 TCP / UDP Graylog ingress storage, internal 12201 TCP / UDP FoundationDB ingress storage, internal 4800 TCP FoundationDB ingress storage 4500 TCP Cluster Control ingress storage 4420 TCP Cluster Control ingress storage 9090-9099 TCP Service Direction Target Network Port Protocol(s) All traffic egress [0.0.0.0/0] ALL ALL"},{"location":"deployments/kubernetes/prerequisites/#storage-plane","title":"Storage Plane","text":"Format the NVMe devices<pre><code>lsblk\nnvme id-ns /dev/nvmeXnY\nnvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/#kubernetes-requirements","title":"Kubernetes Requirements","text":"<ul> <li>Kubernetes v1.25 or higher</li> <li>Privileged container</li> </ul>"},{"location":"deployments/kubernetes/talos/","title":"Talos Prerequisites","text":"<p>Talos Linux\u00a0\u29c9 is a minimal Linux distribution optimized for Kubernetes. Built as an immutable distribution image, it provides a minimal attack surface but requires some changes to the image to run simplyblock.</p> <p>Simplyblock requires a set of additional Linux kernel modules, as well as tools being available in the Talos image. That means that a custom Talos image has to be built to run simplyblock. The following section explains the required changes to make Talos compliant.</p>"},{"location":"deployments/kubernetes/talos/#required-kernel-modules-worker-node","title":"Required Kernel Modules (Worker Node)","text":"<p>On Kubernetes worker nodes, simplyblock requires few kernel modules to be loaded.</p> Content of kernel-module-config.yaml<pre><code>machine:\n  kernel:\n    modules:\n      - name: nbd \n      - name: uio_pci_generic\n      - name: vfio_pci\n      - name: vfio_iommu_type1\n</code></pre>"},{"location":"deployments/kubernetes/talos/#huge-pages-reservations","title":"Huge Pages Reservations","text":"<p>Simplyblock requires huge pages memory to operate. The storage engine expects to find huge pages of 2 MiB page size. The required amount of huge pages depends on a number of factors.</p> <p>To apply the change to Talos' worker nodes, a YAML configuration file with the following content is required. The number of pages is to be replaced with the number calculated above.</p> Content of huge-pages-config.yaml<pre><code>machine:\n  sysctls:\n     vm.nr_hugepages: \"&lt;number-of-pages&gt;\"\n</code></pre> <p>To activate the huge pages, the <code>talosctl</code> command should be used.</p> Enable Huge Pages in Talos<pre><code>demo@demo ~&gt; talosctl apply-config --nodes &lt;worker_node_ip&gt; \\\n    --file huge-pages-config.yaml -m reboot\ndemo@demo ~&gt; talosctl service kubelet restart --nodes &lt;worker_node_ip&gt;\n</code></pre>"},{"location":"deployments/kubernetes/talos/#required-talos-permissions","title":"Required Talos Permissions","text":"<p>Simyplyblock's CSI driver requires connecting NVMe over Fabrics devices, as well as mounting and formatting them. Therefore, the CSI driver has to run as a privileged container. Hence, Talos must be configured to start the simplyblock's CSI driver in privileged mode. </p> <p>Talos allows overriding the pod security admission settings at a Kubernetes namespace level. To enable privileged mode and grant the required access to the simplyblock CSI driver, a specific simplyblock namespace with the appropriate security exemptions must be created:</p> Content of simplyblock-namespace.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: simplyblock\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/enforce-version: latest\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/audit-version: latest\n    pod-security.kubernetes.io/warn: privileged\n    pod-security.kubernetes.io/warn-version: latest\n</code></pre> <p>To enable the required permissions, apply the namespace configuration using <code>kubectl</code>.</p> Enabled privileged mode for simplyblock<pre><code>demo@demo ~&gt; kubectl apply -f simplyblock-namespace.yaml\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/","title":"Install Simplyblock","text":"<p>Simplyblock is a highly flexible solution when it comes to Kubernetes-related deployment strategies. Each of the deployment methods have their own pros and cons. </p> <ul> <li> <p> Hyper-Converged</p> <p></p><p>A hyper-converged storage cluster provides the best resource usage by combining storage and compute into shared hardware. Running hyper-converged enables data affinity (data locality) for ultra-low latency.</p><p>  Learn about Hyper-Converged  Hyper-Converged Installation</p> </li> <li> <p> Disaggregated</p> <p></p><p>A disaggregated storage cluster provides the best independent scalability between storage and compute. Running a separate storage cluster enables resource efficiency and simplified management.</p><p>  Learn about Disaggregated  Disaggregated Installation</p> </li> <li> <p> Hybrid</p> <p></p><p>A hybrid storage cluster combines the best of both worlds. Using a disaggregated storage cluster for infinite scalability and joining it with a hyper-converged part for the lowest latency and highest throughput workloads.</p><p>  Hybrid Installation</p> </li> </ul>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/","title":"Disaggregated Setup","text":"<p>A disaggregated setup on Kubernetes is very similar to a bare-metal or virtualized installation.</p> <p>Danger</p> <p>Simplyblock requires a fully redundant network interconnect, implemented via a solution such as LACP or Static LAG. Failing to provide that may cause data corruption or data loss in case of network issues. For more information see the Network Considerations section.</p> <p>Installing simplyblock for production requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance, and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides two test scripts to automatically check your system's configuration. While those may not catch all edge cases, they can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configurations during the process.</p> Automatically check your configuration<pre><code># Configuration check for the control plane (management nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n\n# Configuration check for the storage plane (storage nodes)\ncurl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, and the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separate subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets are available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Docker Swarm, by default, creates iptables entries open to the world. If no external firewall is available, the created iptables configuration needs to be restricted.</p> <p>The following script will create additional iptables rules prepended to Docker's forwarding rules and only enabling access from internal networks. This script should be stored in /usr/local/sbin/simplyblock-iptables.sh.</p> Configuration script for Iptables<pre><code>#!/usr/bin/env bash\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080:8890 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9090-9900 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre> <p>To automatically run this script whenever Docker is started or restarted, it must be attached to a Systemd service, stored as /etc/systemd/system/simplyblock-iptables.service.</p> Systemd script to set up Iptables<pre><code>[Unit]\nDescription=Simplyblock Iptables Restrictions for Docker \nAfter=docker.service\nBindsTo=docker.service\nReloadPropagatedFrom=docker.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/simplyblock-iptables.sh\nExecReload=/usr/local/sbin/simplyblock-iptables.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>After both files are stored in their respective locations, the bash script needs to be made executable, and the Systemd service needs to be enabled to start automatically.</p> Enabling service file<pre><code>chmod +x /usr/local/sbin/simplyblock-iptables.sh\nsystemctl enable simplyblock-iptables.service\nsystemctl start simplyblock-iptables.service\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) are installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#nvme-device-preparation","title":"NVMe Device Preparation","text":"<p>Once the check is complete, the NVMe devices in each storage node can be prepared. To prevent data loss in case of a sudden power outage, NVMe devices need to be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If an NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>With all NVMe devices prepared and the NVMe/TCP driver loaded, the storage node software can be deployed.</p> <p>The actual deployment process happens in three steps: - Creating the storage node configuration - Deploy the first stage (the storage node API) - Deploy the second stage (add the storage node to the cluster), happening from a management node</p> <p>The configuration process creates the configuration file, which contains all the assignments of NVMe devices, NICs, and potentially available NUMA nodes. By default, simplyblock will configure one storage node per NUMA node.</p> Configure the storage node<pre><code>sudo sbctl storage-node configure \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-size &lt;MAX_PROVISIONING_CAPACITY&gt;\n</code></pre> Example output of storage node configure<pre><code>[demo@demo-3 ~]# sudo sbctl sn configure --nodes-per-socket=2 --max-lvol=50 --max-size=1T\n2025-05-14 10:40:17,460: INFO: 0000:00:04.0 is already bound to nvme.\n0000:00:1e.0\n0000:00:1e.0\n0000:00:1f.0\n0000:00:1f.0\n0000:00:1e.0\n0000:00:1f.0\n2025-05-14 10:40:17,841: INFO: JSON file successfully written to /etc/simplyblock/sn_config_file\n2025-05-14 10:40:17,905: INFO: JSON file successfully written to /etc/simplyblock/system_info\nTrue\n</code></pre> <p>A full set of the parameters for the configure subcommand can be found in the CLI reference. </p> <p>After the configuration has been created, the first stage deployment can be executed </p> Deploy the storage node<pre><code>sudo sbctl storage-node deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbctl storage-node deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p> <p>When all storage nodes are added, it's finally time to activate the storage plane.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step, after all nodes are added to the storage cluster, is to activate the storage plane.</p> Storage cluster activation<pre><code>sudo sbctl cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this, and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbctl cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#simplyblock-csi-driver-installation","title":"Simplyblock CSI Driver Installation","text":"<p>Simplyblock provides a seamless integration with Kubernetes through its Kubernetes CSI driver.</p> <p>To install the Simplyblock CSI Driver, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended. If a manual installation is preferred, see the CSI Driver Repository\u00a0\u29c9.</p> <p>Either way, the installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying the CSI driver is the control plane address. On a standard bare metal or virtualized installation, it is any of the API addresses. Meaning, if the primary management node has the IP of <code>192.168.10.1</code>, the control plane address is <code>http://192.168.0.1</code>. It is, however, recommended to front all management nodes with a load-balancing proxy, such as HAProxy. In the latter case, the load balancer URL would be the address of the control plane.</p> <p>Anyhow, deploying the Simplyblock CSI Driver using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver.</p> Install Simplyblock's CSI Driver<pre><code>CLUSTER_UUID=\"&lt;cluster_uuid&gt;\"\nCLUSTER_SECRET=\"&lt;cluster_secret&gt;\"\nCNTR_ADDR=\"&lt;control_plane_api_addr&gt;\"\nPOOL_NAME=\"&lt;pool_name&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\nhelm repo update\nhelm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n    --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n    --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n    --set logicalVolume.pool_name=${POOL_NAME}\n</code></pre> Example output of the CSI driver deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/","title":"Hybrid Setup","text":""},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separate subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#storage-plane-installation","title":"Storage Plane Installation","text":""},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":""},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#storage-plane-installation_1","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets are available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#firewall-configuration-sp_1","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP FoundationDB ingress control 4500 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP Greylog ingress control 12202 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Greylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Docker Swarm, by default, creates iptables entries open to the world. If no external firewall is available, the created iptables configuration needs to be restricted.</p> <p>The following script will create additional iptables rules prepended to Docker's forwarding rules and only enabling access from internal networks. This script should be stored in /usr/local/sbin/simplyblock-iptables.sh.</p> Configuration script for Iptables<pre><code>#!/usr/bin/env bash\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080:8890 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9090-9900 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre> <p>To automatically run this script whenever Docker is started or restarted, it must be attached to a Systemd service, stored as /etc/systemd/system/simplyblock-iptables.service.</p> Systemd script to set up Iptables<pre><code>[Unit]\nDescription=Simplyblock Iptables Restrictions for Docker \nAfter=docker.service\nBindsTo=docker.service\nReloadPropagatedFrom=docker.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/simplyblock-iptables.sh\nExecReload=/usr/local/sbin/simplyblock-iptables.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>After both files are stored in their respective locations, the bash script needs to be made executable, and the Systemd service needs to be enabled to start automatically.</p> Enabling service file<pre><code>chmod +x /usr/local/sbin/simplyblock-iptables.sh\nsystemctl enable simplyblock-iptables.service\nsystemctl start simplyblock-iptables.service\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) are installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-sn.sh | bash\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#nvme-device-preparation","title":"NVMe Device Preparation","text":"<p>Once the check is complete, the NVMe devices in each storage node can be prepared. To prevent data loss in case of a sudden power outage, NVMe devices need to be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If an NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>With all NVMe devices prepared and the NVMe/TCP driver loaded, the storage node software can be deployed.</p> <p>The actual deployment process happens in three steps: - Creating the storage node configuration - Deploy the first stage (the storage node API) - Deploy the second stage (add the storage node to the cluster), happening from a management node</p> <p>The configuration process creates the configuration file, which contains all the assignments of NVMe devices, NICs, and potentially available NUMA nodes. By default, simplyblock will configure one storage node per NUMA node.</p> Configure the storage node<pre><code>sudo sbctl storage-node configure \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-size &lt;MAX_PROVISIONING_CAPACITY&gt;\n</code></pre> Example output of storage node configure<pre><code>[demo@demo-3 ~]# sudo sbctl sn configure --nodes-per-socket=2 --max-lvol=50 --max-size=1T\n2025-05-14 10:40:17,460: INFO: 0000:00:04.0 is already bound to nvme.\n0000:00:1e.0\n0000:00:1e.0\n0000:00:1f.0\n0000:00:1f.0\n0000:00:1e.0\n0000:00:1f.0\n2025-05-14 10:40:17,841: INFO: JSON file successfully written to /etc/simplyblock/sn_config_file\n2025-05-14 10:40:17,905: INFO: JSON file successfully written to /etc/simplyblock/system_info\nTrue\n</code></pre> <p>A full set of the parameters for the configure subcommand can be found in the CLI reference. </p> <p>After the configuration has been created, the first stage deployment can be executed </p> Deploy the storage node<pre><code>sudo sbctl storage-node deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbctl storage-node deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p> <p>When all storage nodes are added, it's finally time to activate the storage plane.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step, after all nodes are added to the storage cluster, is to activate the storage plane.</p> Storage cluster activation<pre><code>sudo sbctl cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this, and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbctl cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#hyper-converged-storage-node-installation","title":"Hyper-Converged Storage Node Installation","text":""},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#nvme-over-fabrics-modules_1","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#collect-required-cluster-details","title":"Collect Required Cluster Details","text":"<p>To install the simplyblock in Kubernetes, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended.  The installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying simplyblock is the control plane address. This is any of the API addresses of a management node. Meaning, if the primary management node has the IP of <code>192.168.10.1</code>, the control plane address is <code>http://192.168.0.1</code>. It is, however, recommended to front all management nodes with a load balancing proxy, such as HAproxy. In the latter case, the load balancer URL would be the address of the control plane.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#installing-the-helm-charts","title":"Installing the Helm Charts","text":"<p>Anyhow, deploying simplyblock using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver. In addition to the storage nodes, this will also install the Simplyblock CSI driver for seamless integration with the Kubernetes CSI persistent storage subsystem.</p> <p>To enable Kubernetes to decide where to install storage nodes, the helm chart uses a Kubernetes node label. This can be used to mark only specific nodes to act as storage nodes, or to use all nodes for the hyper-converged or hybrid setup. </p> Label the Kubernetes Worker Node<pre><code>kubectl label nodes &lt;NODE_NAME&gt; type=simplyblock-storage-plane\n</code></pre> <p>Warning</p> <p>The label must be applied to all nodes that operate as part of the storage plane.</p> <p>After labeling the nodes, the Helm chart can be deployed.</p> Install the helm chart<pre><code>CLUSTER_UUID=\"&lt;UUID&gt;\"\nCLUSTER_SECRET=\"&lt;SECRET&gt;\"\nCNTR_ADDR=\"&lt;CONTROL-PLANE-ADDR&gt;\"\nPOOL_NAME=\"&lt;POOL-NAME&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\nhelm repo add simplyblock-controller https://install.simplyblock.io/helm/controller\nhelm repo update\n\n# Install Simplyblock CSI Driver and Storage Node API\nhelm install -n simplyblock \\\n    --create-namespace simplyblock \\\n    simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=&lt;CLUSTER_UUID&gt; \\\n    --set csiConfig.simplybk.ip=&lt;CNTR_ADDR&gt; \\\n    --set csiSecret.simplybk.secret=&lt;CLUSTER_SECRET&gt; \\\n    --set logicalVolume.pool_name=&lt;POOL_NAME&gt; \\\n    --set storagenode.create=true\n</code></pre> Example output of the Simplyblock Kubernetes deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo add simplyblock-controller https://install.simplyblock.io/helm/controller\n\"simplyblock-controller\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\n...Successfully got an update from the \"simplyblock-controller\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre> <p>When the storage cluster nodes are deployed, it is recommended to apply CPU core isolation for highest performance to the Kubernetes worker nodes that act as storage node hosts.</p> <p>During the installation of the simplyblock controller, a configuration file with the system configuration has been created. To apply core isolation to the Kubernetes worker, an SSH login to the worker node is required.</p> <p>After logging in, tuned must be installed if not already available. This can be installed via one of the following commands:</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>yum install tuned\n</code></pre> <pre><code>apt install tuned\n</code></pre> <p>Info</p> <p>On Amazon Linux, the tuned package is not available. Please use the AlmaLinux version instead.</p> <pre><code>yum install https://repo.almalinux.org/almalinux/9/BaseOS/x86_64/os/Packages/tuned-2.25.1-1.el9.noarch.rpm\n</code></pre> <p>Notes</p> <p>Potentially, the number of CPU cores, assigned to simplyblock, should be adjusted. This is especially true for hyper-converged setups. Simplyblock, by default, will take all CPUs but 20% for itself. To change number of CPUs, the Change the number of CPUs section explains the necessary steps which should be executed before following up here.</p> <p>Following the installation of tuned, the tuning profile file must be created. The following snippet automates the creation based on the generated configuration file.</p> Generate the core isolation tuning profile<pre><code>sudo -i\nSIMPLYBLOCK_CONFIG=\"/var/simplyblock/sn_config_file\"\npip install -y yq jq\nISOLATED=$(yq '.isolated_cores' ${SIMPLYBLOCK_CONFIG} | jq -r '. | join(\",\")'); echo \"isolcpus=${ISOLATED}\"\nmkdir -p /etc/tuned/realtime\ncat &lt;&lt; EOF &gt; /etc/tuned/realtime/tuned.conf\n[main]\ninclude=latency-performance\n[bootloader]\ncmdline_add=isolcpus={$ISOLATED} nohz_full={$ISOLATED} rcu_nocbs={$ISOLATED}\nEOF\n</code></pre> <p>Now the profile file must be applied and the worker node restarted.</p> <p>Info</p> <p>Remember to drain potentially remaining services on the Kubernetes worker node before rebooting.</p> Apply the profile and reboot<pre><code>sudo systemctl enable tuned\nsudo systemctl start tuned\nsudo tuned-adm profile realtime\nsudo reboot \n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#install-the-storage-nodes","title":"Install the Storage Nodes","text":"<p>Last but not least, install the actual storage nodes into Kubernetes via Helm.</p> <pre><code>helm install -n simplyblock \\\n    simplyblock-controller/sb-controller \\\n    --set storagenode.create=true\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#changing-the-number-of-utilized-cpu-cores","title":"Changing the Number of Utilized CPU Cores","text":"<p>Info</p> <p>The following section is optional and only required if additional services share the same machine, as happens in a hyper-converged setup.</p> <p>By default, simplyblock assumes that the whole host is available to it and will configure itself to use everything but 20% of the host. In hyper-converged setups, this assumption is not true and the number of utilized CPU cores must be adjusted.</p> <p>To adjust the number of CPU cores, an SSH login to the Kubernetes worker node is required. After logging in, the configuration file at /var/simplyblock/sn_config_file must be updated.</p> Open the configuration file in VI<pre><code>vi /var/simplyblock/sn_config_file\n</code></pre> <p>Inside the configuration file, the cpu_mask value must be updated to represent the number and assignment of cores to be used by simplyblock. To create the required CPU mask, the CPU Mask Calculator can be used. </p> Updating the CPU Mask configuration<pre><code>{\n    \"nodes\": [\n        {\n            \"socket\": 0,\n            \"cpu_mask\": \"0xfffbffc\",\n            \"isolated\": [\n                2,\n                3,\n                4,\n                5,\n                ...\n            ]\n        }\n    ]\n}\n</code></pre> <p>After saving the file and exiting vi, the new configuration must be applied. For simplicity, this shell script at GitHub\u00a0\u29c9 automates the creation and submission of the Kubernetes job.</p> Apply the configuration change<pre><code>curl -s -L https://raw.githubusercontent.com/simplyblock-io/simplyblock-csi/refs/heads/master/scripts/config-gen-upgrade.sh | bash\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/","title":"Hyper-Converged Setup","text":""},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>The following is a list of all ports (TCP and UDP) required to operate as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) ICMP ingress control - ICMP Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Greylog ingress storage, control 12201 TCP / UDP Greylog ingress storage, control 12202 TCP Greylog ingress storage, control 13201 TCP Greylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080-8890 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbctl</code>. It's built in Python and requires Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbctl</code> command line interface can be installed. Upgrading the CLI later on uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbctl --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbctl</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbctl</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -s -L https://install.simplyblock.io/scripts/prerequisites-cp.sh | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbctl cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbctl cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbctl cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbctl</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbctl --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbctl mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbctl mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>Caching nodes, like storage nodes, require huge page memory to hold the internal state. Huge pages should be 2MiB in size, and a minimum of 4096 huge pages should be allocated at boot time of the operating system.</p> <pre><code>demo@worker-1 ~&gt; sudo sysctl -w vm.nr_hugepages=4096\n</code></pre> <p>Info</p> <p>To see how huge pages can be pre-reserved at boot time, see the node sizing documentation section on Huge Pages.</p> <pre><code>demo@worker-1 ~&gt; sudo systemctl restart kubelet\n</code></pre> <pre><code>demo@worker-1 ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | \\\n    grep hugepages-2Mi\n</code></pre> <pre><code>demo@demo ~&gt; kubectl describe node worker-1.kubernetes-cluster.local | \\\n    grep hugepages-2Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi:      9440Mi\n  hugepages-2Mi      0 (0%)    0 (0%)\n</code></pre> <pre><code>demo@worker-1 ~&gt; sudo yum install -y nvme-cli\ndemo@worker-1 ~&gt; sudo modprobe nvme-tcp\ndemo@worker-1 ~&gt; sudo modprobe nbd\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"Service Direction Source / Target Network Port(s) Protocol(s) ICMP ingress control - ICMP Storage node API ingress storage 5000 TCP spdk-http-proxy ingress storage, control 8080-8180 TCP hublvol-nvmf-subsys-port ingress storage, control 9030-9059 TCP internal-nvmf-subsys-port ingress storage, control 9060-9099 TCP lvol-nvmf-subsys-port ingress storage, control 9100-9200 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#storage-node-installation","title":"Storage Node Installation","text":""},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#nvme-over-fabrics-modules","title":"NVMe over Fabrics Modules","text":"<p>Simplyblock is built upon the NVMe over Fabrics standard and uses NVMe over TCP (NVMe/TCP) by default.</p> <p>While the driver is part of the Linux kernel with kernel versions 5.x and later, it is not enabled by default. Hence, when using simplyblock, the driver needs to be loaded.</p> Loading the NVMe/TCP driver<pre><code>modprobe nvme-tcp\n</code></pre> <p>When loading the NVMe/TCP driver, the NVMe over Fabrics driver automatically get loaded to, as the former depends on its provided foundations.</p> <p>It is possible to check for successful loading of both drivers with the following command:</p> Checking the drivers being loaded<pre><code>lsmod | grep 'nvme_'\n</code></pre> <p>The response should list the drivers as nvme_tcp and nvme_fabrics as seen in the following example:</p> Example output of the driver listing<pre><code>[demo@demo ~]# lsmod | grep 'nvme_'\nnvme_tcp               57344  0\nnvme_keyring           16384  1 nvme_tcp\nnvme_fabrics           45056  1 nvme_tcp\nnvme_core             237568  3 nvme_tcp,nvme,nvme_fabrics\nnvme_auth              28672  1 nvme_core\nt10_pi                 20480  2 sd_mod,nvme_core\n</code></pre> <p>To make the driver loading persistent and survive system reboots, it has to be configured to be loaded at system startup time. This can be achieved by either adding it to /etc/modules (Debian / Ubuntu) or creating a config file under /etc/modules-load.d/ (Red Hat / Alma / Rocky).</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> <p>After rebooting the system, the driver should be loaded automatically. It can be checked again via the above provided <code>lsmod</code> command.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#collect-required-cluster-details","title":"Collect Required Cluster Details","text":"<p>To install the simplyblock in Kubernetes, a Helm chart is provided. While it can be installed manually, the Helm chart is strongly recommended.  The installation requires a few values to be available.</p> <p>First, we need the unique cluster id. Note down the cluster UUID of the cluster to access.</p> Retrieving the Cluster UUID<pre><code>sudo sbctl cluster list\n</code></pre> <p>An example of the output is below.</p> Example output of a cluster listing<pre><code>[demo@demo ~]# sbctl cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n| 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | nqn.2023-02.io.simplyblock:4502977c-ae2d-4046-a8c5-ccc7fa78eb9a | ha      | False | 1          | 4             | 1x1 | active |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+--------+\n</code></pre> <p>In addition, we need the cluster secret. Note down the cluster secret.</p> Retrieve the Cluster Secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_UUID&gt;\n</code></pre> <p>Retrieving the cluster secret will look somewhat like that.</p> Example output of retrieving a cluster secret<pre><code>[demo@demo ~]# sbctl cluster get-secret 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\noal4PVNbZ80uhLMah2Bs\n</code></pre> <p>Additionally, a storage pool is required. If a pool already exists, it can be reused. Otherwise, creating a storage pool can be created as following:</p> Create a Storage Pool<pre><code>sbctl pool add &lt;POOL_NAME&gt; &lt;CLUSTER_UUID&gt;\n</code></pre> <p>The last line of a successful storage pool creation returns the new pool id.</p> Example output of creating a storage pool<pre><code>[demo@demo ~]# sbctl pool add test 4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\n2025-03-05 06:36:06,093: INFO: Adding pool\n2025-03-05 06:36:06,098: INFO: {\"cluster_id\": \"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Pool\", \"message\": \"Pool created test\", \"caused_by\": \"cli\"}\n2025-03-05 06:36:06,100: INFO: Done\nad35b7bb-7703-4d38-884f-d8e56ffdafc6 # &lt;- Pool Id\n</code></pre> <p>The last item necessary before deploying simplyblock is the control plane address. This is any of the API addresses of a management node. Meaning, if the primary management node has the IP of <code>192.168.10.1</code>, the control plane address is <code>http://192.168.0.1</code>. It is, however, recommended to front all management nodes with a load balancing proxy, such as HAproxy. In the latter case, the load balancer URL would be the address of the control plane.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#installing-the-helm-charts","title":"Installing the Helm Charts","text":"<p>Anyhow, deploying simplyblock using the provided helm chart comes down to providing the four necessary values, adding the helm chart repository, and installing the driver. In addition to the storage nodes, this will also install the Simplyblock CSI driver for seamless integration with the Kubernetes CSI persistent storage subsystem.</p> <p>To enable Kubernetes to decide where to install storage nodes, the helm chart uses a Kubernetes node label. This can be used to mark only specific nodes to act as storage nodes, or to use all nodes for the hyper-converged or hybrid setup. </p> Label the Kubernetes Worker Node<pre><code>kubectl label nodes &lt;NODE_NAME&gt; type=simplyblock-storage-plane\n</code></pre> <p>Warning</p> <p>The label must be applied to all nodes that operate as part of the storage plane.</p> <p>After labeling the nodes, the Helm chart can be deployed.</p> Install the helm chart<pre><code>CLUSTER_UUID=\"&lt;UUID&gt;\"\nCLUSTER_SECRET=\"&lt;SECRET&gt;\"\nCNTR_ADDR=\"&lt;CONTROL-PLANE-ADDR&gt;\"\nPOOL_NAME=\"&lt;POOL-NAME&gt;\"\nhelm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\nhelm repo add simplyblock-controller https://install.simplyblock.io/helm/controller\nhelm repo update\n\n# Install Simplyblock CSI Driver and Storage Node API\nhelm install -n simplyblock \\\n    --create-namespace simplyblock \\\n    simplyblock-csi/spdk-csi \\\n    --set csiConfig.simplybk.uuid=&lt;CLUSTER_UUID&gt; \\\n    --set csiConfig.simplybk.ip=&lt;CNTR_ADDR&gt; \\\n    --set csiSecret.simplybk.secret=&lt;CLUSTER_SECRET&gt; \\\n    --set logicalVolume.pool_name=&lt;POOL_NAME&gt; \\\n    --set storagenode.create=true\n</code></pre> Example output of the Simplyblock Kubernetes deployment<pre><code>demo@demo ~&gt; export CLUSTER_UUID=\"4502977c-ae2d-4046-a8c5-ccc7fa78eb9a\"\ndemo@demo ~&gt; export CLUSTER_SECRET=\"oal4PVNbZ80uhLMah2Bs\"\ndemo@demo ~&gt; export CNTR_ADDR=\"http://192.168.10.1/\"\ndemo@demo ~&gt; export POOL_NAME=\"test\"\ndemo@demo ~&gt; helm repo add simplyblock-csi https://install.simplyblock.io/helm/csi\n\"simplyblock-csi\" has been added to your repositories\ndemo@demo ~&gt; helm repo add simplyblock-controller https://install.simplyblock.io/helm/controller\n\"simplyblock-controller\" has been added to your repositories\ndemo@demo ~&gt; helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"simplyblock-csi\" chart repository\n...Successfully got an update from the \"simplyblock-controller\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\ndemo@demo ~&gt; helm install -n simplyblock --create-namespace simplyblock simplyblock-csi/spdk-csi \\\n  --set csiConfig.simplybk.uuid=${CLUSTER_UUID} \\\n  --set csiConfig.simplybk.ip=${CNTR_ADDR} \\\n  --set csiSecret.simplybk.secret=${CLUSTER_SECRET} \\\n  --set logicalVolume.pool_name=${POOL_NAME}\nNAME: simplyblock-csi\nLAST DEPLOYED: Wed Mar  5 15:06:02 2025\nNAMESPACE: simplyblock\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe Simplyblock SPDK Driver is getting deployed to your cluster.\n\nTo check CSI SPDK Driver pods status, please run:\n\n  kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\ndemo@demo ~&gt; kubectl --namespace=simplyblock get pods --selector=\"release=simplyblock-csi\" --watch\nNAME                   READY   STATUS    RESTARTS   AGE\nspdkcsi-controller-0   6/6     Running   0          30s\nspdkcsi-node-tzclt     2/2     Running   0          30s\n</code></pre> <p>When the storage cluster nodes are deployed, it is recommended to apply CPU core isolation for highest performance to the Kubernetes worker nodes that act as storage node hosts.</p> <p>During the installation of the simplyblock controller, a configuration file with the system configuration has been created. To apply core isolation to the Kubernetes worker, an SSH login to the worker node is required.</p> <p>After logging in, tuned must be installed if not already available. This can be installed via one of the following commands:</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>yum install tuned\n</code></pre> <pre><code>apt install tuned\n</code></pre> <p>Info</p> <p>On Amazon Linux, the tuned package is not available. Please use the AlmaLinux version instead.</p> <pre><code>yum install https://repo.almalinux.org/almalinux/9/BaseOS/x86_64/os/Packages/tuned-2.25.1-1.el9.noarch.rpm\n</code></pre> <p>Notes</p> <p>Potentially, the number of CPU cores, assigned to simplyblock, should be adjusted. This is especially true for hyper-converged setups. Simplyblock, by default, will take all CPUs but 20% for itself. To change number of CPUs, the Change the number of CPUs section explains the necessary steps which should be executed before following up here.</p> <p>Following the installation of tuned, the tuning profile file must be created. The following snippet automates the creation based on the generated configuration file.</p> Generate the core isolation tuning profile<pre><code>sudo -i\nSIMPLYBLOCK_CONFIG=\"/var/simplyblock/sn_config_file\"\npip install -y yq jq\nISOLATED=$(yq '.isolated_cores' ${SIMPLYBLOCK_CONFIG} | jq -r '. | join(\",\")'); echo \"isolcpus=${ISOLATED}\"\nmkdir -p /etc/tuned/realtime\ncat &lt;&lt; EOF &gt; /etc/tuned/realtime/tuned.conf\n[main]\ninclude=latency-performance\n[bootloader]\ncmdline_add=isolcpus={$ISOLATED} nohz_full={$ISOLATED} rcu_nocbs={$ISOLATED}\nEOF\n</code></pre> <p>Now the profile file must be applied and the worker node restarted.</p> <p>Info</p> <p>Remember to drain potentially remaining services on the Kubernetes worker node before rebooting.</p> Apply the profile and reboot<pre><code>sudo systemctl enable tuned\nsudo systemctl start tuned\nsudo tuned-adm profile realtime\nsudo reboot \n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#install-the-storage-nodes","title":"Install the Storage Nodes","text":"<p>Last but not least, install the actual storage nodes into Kubernetes via Helm.</p> <pre><code>helm install -n simplyblock \\\n    simplyblock-controller/sb-controller \\\n    --set storagenode.create=true\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#changing-the-number-of-utilized-cpu-cores","title":"Changing the Number of Utilized CPU Cores","text":"<p>Info</p> <p>The following section is optional and only required if additional services share the same machine, as happens in a hyper-converged setup.</p> <p>By default, simplyblock assumes that the whole host is available to it and will configure itself to use everything but 20% of the host. In hyper-converged setups, this assumption is not true and the number of utilized CPU cores must be adjusted.</p> <p>To adjust the number of CPU cores, an SSH login to the Kubernetes worker node is required. After logging in, the configuration file at /var/simplyblock/sn_config_file must be updated.</p> Open the configuration file in VI<pre><code>vi /var/simplyblock/sn_config_file\n</code></pre> <p>Inside the configuration file, the cpu_mask value must be updated to represent the number and assignment of cores to be used by simplyblock. To create the required CPU mask, the CPU Mask Calculator can be used. </p> Updating the CPU Mask configuration<pre><code>{\n    \"nodes\": [\n        {\n            \"socket\": 0,\n            \"cpu_mask\": \"0xfffbffc\",\n            \"isolated\": [\n                2,\n                3,\n                4,\n                5,\n                ...\n            ]\n        }\n    ]\n}\n</code></pre> <p>After saving the file and exiting vi, the new configuration must be applied. For simplicity, this shell script at GitHub\u00a0\u29c9 automates the creation and submission of the Kubernetes job.</p> Apply the configuration change<pre><code>curl -s -L https://raw.githubusercontent.com/simplyblock-io/simplyblock-csi/refs/heads/master/scripts/config-gen-upgrade.sh | bash\n</code></pre>"},{"location":"deployments/proxmox/","title":"Proxmox Integration","text":"<p>Proxmox Virtual Environment (Proxmox VE) is an open-source server virtualization platform that integrates KVM-based virtual machines and LXC containers with a web-based management interface.</p> <p>Simplyblock seamlessly integrates with Proxmox through its storage plugin. The storage plugin enables the automatic provisioning of storage volumes for Proxmox's KVM virtual machines and LXC containers. Simplyblock is fully integrated into the Proxmox user interface.</p> <p>After being deployed, virtual machine and container images can be provisioned to simplyblock logical volumes, inheriting all performance and reliability characteristics. Volumes provisioned using the simplyblock Proxmox integration are automatically managed and provided to the hypervisor in an ad-hoc fashion. The Proxmox UI and command line interface can manage the volume lifecycle.</p>"},{"location":"deployments/proxmox/#install-simplyblock-for-proxmox","title":"Install Simplyblock for Proxmox","text":"<p>Simplyblock's Proxmox storage plugin can be installed from the simplyblock apt repository. To register the simplyblock apt repository, simplyblock offers a script to handle the repository registration automatically.</p> <p>Info</p> <p>All the following commands require root permissions for execution. It is recommended to log in as root or open a root shell using <code>sudo su</code>. </p> Automatically register the Simplyblock Debian Repository<pre><code>curl https://install.simplyblock.io/install-debian-repository | bash\n</code></pre> <p>If a manual registration is preferred, the repository public key must be downloaded and made available to apt. This key is used for signature verification.</p> Install the Simplyblock Public Key<pre><code>curl -o /etc/apt/keyrings/simplyblock.gpg https://install.simplyblock.io/simplyblock.key\n</code></pre> <p>Afterward, the repository needs to be registered for apt itself. The following line registers the apt repository.</p> Register the Simplyblock Debian Repository<pre><code>echo 'deb [signed-by=/etc/apt/keyrings/simplyblock.gpg] https://install.simplyblock.io/debian stable main' | \\\n    tee /etc/apt/sources.list.d/simplyblock.list\n</code></pre>"},{"location":"deployments/proxmox/#install-the-simplyblock-proxmox-package","title":"Install the Simplyblock-Proxmox Package","text":"<p>After the registration of the repository, an <code>apt update</code> will refresh all available package information and make the <code>simplyblock-proxmox</code> package available. The update must not show any errors related to the simplyblock apt repository.</p> <p>With the updated repository information, an <code>apt install simplyblock-proxmox</code> installed the simplyblock storage plugin.</p> Install the Simplyblock Proxmox Integration<pre><code>apt update\napt install simplyblock-proxmox\n</code></pre> <p>Now, register a simplyblock storage pool with Proxmox. The new Proxmox storage can have an arbitrary name and multiple simplyblock storage pools can be registered as long as their Proxmox names are different.</p> Enable Simplyblock as a Storage Provider<pre><code>pvesm add simplyblock &lt;NAME&gt; \\\n    --entrypoint=&lt;CONTROL_PLANE_ADDR&gt; \\\n    --cluster=&lt;CLUSTER_ID&gt; \\\n    --secret=&lt;CLUSTER_SECRET&gt; \\\n    --pool=&lt;STORAGE_POOL_NAME&gt;\n</code></pre> Parameter Description NAME The name of the storage pool in Proxmox. CONTROL_PLANE_ADDR The api address of the simplyblock control plane. CLUSTER_ID The simplyblock storage cluster id. The cluster id can be found using <code>sbctl cluster lust</code>. CLUSTER_SECRET The simplyblock storage cluster secret. The cluster secret can be retrieved using <code>sbctl cluster get-secret</code>. STORAGE_POOL_NAME The simplyblock storage pool name to attach. <p>In the Proxmox user interface, a storage of type simplyblock is now available.</p> <p></p> <p>The hypervisor is now configured and can use a simplyblock storage cluster as a storage backend.</p>"},{"location":"important-notes/","title":"Important Notes","text":"<p>Simplyblock is a high-performance yet reliable distributed block storage optimized for Kubernetes that is compatible with any bare metal and virtualized Linux environments. It also provides integrations with other environments, such as Proxmox.</p> <p>To enable the successful operation of your new simplyblock cluster, this section defines some initial conventions and terminology when working with this documentation.</p>"},{"location":"important-notes/acronyms/","title":"Acronyms & Abbreviations","text":"Acronym or Abbreviation Explanation API Application Programming Interface AWS Amazon Web Services CIDR Classless Inter-Domain Routing CLI Command Line Interface COW Copy On Write CP Control Plane CSI Container Storage Interface DMA Direct Memory Access EA Erasure Coding HA High Availability HTTP Hypertext Transfer Protocol ID Identifier IO Input-Output IOMMU Input-Output Memory Management Unit IP Internet Protocol K8s Kubernetes LV Logical Volume MFT Maximum Tolerable Failure NIC Network Interface Card NQN NVMe Qualified Name NVMe Non-Volatile Memory Express NVMe-oF NVMe over Fabrics NVMe/RoCE NVMe over RDMA on Converged Ethernet NVMe/TCP NVMe over TCP OS Operating System PV Persistent Volume PVC Persistent Volume Claim QOS Quality of Service RAID Redundant Array of Independent Disks RDMA Remote Direct Memory Access ROW Redirect On Write ROX Read Only Many RWO Read Write Once RWX Read Write Many SC Storage Class SDK Software Development Kit SDS Software Defined Storage SP Storage Plane SPDK Storage Performance Development Kit SSD Solid State Drive SSL Secure Socket Layer TCP Transmission Control Protocol TLS Transport Layer Security UDP User Datagram Protocol UUID Universally Unique Identifier VM Virtual Machine"},{"location":"important-notes/contributing/","title":"Contributing","text":""},{"location":"important-notes/contributing/#contributing-to-simplyblock-documentation","title":"Contributing to Simplyblock Documentation","text":""},{"location":"important-notes/contributing/#overview","title":"Overview","text":"<p>Simplyblock's documentation is publicly available, and we welcome contributions from the community to improve clarity, fix errors, and enhance the overall quality of our documentation. While simplyblock itself is not open source, our documentation is publicly hosted  GitHub\u00a0\u29c9. We encourage users to provide feedback, report typos, suggest improvements, and submit fixes for documentation inconsistencies.</p>"},{"location":"important-notes/contributing/#how-to-contribute","title":"How to Contribute","text":"<p>The simplyblock documentation is built using mkdocs\u00a0\u29c9, specifically using the mkdocs-material\u00a0\u29c9 variant.</p> <p>Changes to the documentation can be made by changing or adding the necessary Markdown files.</p>"},{"location":"important-notes/contributing/#1-provide-feedback-or-report-issues","title":"1. Provide Feedback or Report Issues","text":"<p>If you notice any inaccuracies, typos, missing information, or outdated content, you can submit an issue on our GitHub repository:</p> <ol> <li>Navigate to the Simplyblock Documentation GitHub Repository\u00a0\u29c9.</li> <li>Click on the Issues tab.</li> <li>Click New Issue and provide a clear description of the problem or suggestion.</li> <li>Submit the issue, and our team will review it.</li> </ol>"},{"location":"important-notes/contributing/#2-make-edits-and-submit-a-pull-request-pr","title":"2. Make Edits and Submit a Pull Request (PR)","text":"<p>If you'd like to make direct changes to the documentation, follow these steps:</p> <ol> <li> <p>Fork the Repository</p> </li> <li> <p>Visit Simplyblock Documentation GitHub\u00a0\u29c9 and click Fork to create   your own copy of the repository.</p> </li> <li> <p>Clone the Repository</p> </li> <li> <p>Clone your fork to your local machine:   </p><pre><code>git clone https://github.com/YOUR_USERNAME/documentation.git\ncd documentation\n</code></pre> </li> <li> <p>Create a New Branch</p> </li> <li> <p>Always create a new branch for your changes:   </p><pre><code>git checkout -b update-docs\n</code></pre> </li> <li> <p>Make Changes</p> </li> <li> <p>Edit the relevant Markdown (<code>.md</code>) files using a text editor or IDE. The documentation files can be found in the   <code>/docs</code> directory.</p> </li> <li> <p>Ensure that formatting follows existing conventions.</p> </li> <li> <p>Commit and Push Your Changes</p> </li> <li> <p>Commit your changes with a clear message:   </p><pre><code>git commit -m \"Fix typo in installation guide\"\n</code></pre> </li> <li> <p>Push the changes to your fork:   </p><pre><code>git push origin update-docs\n</code></pre> </li> <li> <p>Create a Pull Request (PR)</p> </li> <li> <p>Navigate to the original simplyblock documentation repository.</p> </li> <li>Click New Pull Request and select your branch.</li> <li>Provide a concise description of the changes and submit the PR.</li> <li>Our team will review and merge accepted contributions.</li> </ol>"},{"location":"important-notes/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<ul> <li>Ensure all content remains clear, concise, and professional.</li> <li>Follow Markdown syntax conventions used throughout the documentation.</li> <li>Keep changes focused on documentation improvements (not product functionality).</li> <li>Be respectful and constructive in all discussions and contributions.</li> </ul>"},{"location":"important-notes/contributing/#getting-in-touch","title":"Getting in Touch","text":"<p>If you have questions about contributing, feel free to open an issue or contact us via the simplyblock support channels.</p>"},{"location":"important-notes/documentation-conventions/","title":"Documentation Conventions","text":""},{"location":"important-notes/documentation-conventions/#feature-stages","title":"Feature Stages","text":"<p>Features in simplyblock are released when reaching general availability. However, sometimes, features are made available earlier to receive feedback from testers. Those features must be explicitly enabled and are marked in the documentation accordingly. Features without a specific label are considered ready for production.</p> <p>The documentation uses the following feature stage labels:</p> <ul> <li>General Availability: This is the default stage if nothing else is defined for the feature. In this stage, the   feature is considered ready for production.</li> <li>Technical Preview: The feature is provided for testing and feedback acquisition. It is not regarded as stable   or complete. Breaking changes may occur, which could break backward compatibility. Features   in this stage are not considered ready for production. Features in this stage need to   be specifically enabled before use.</li> </ul>"},{"location":"important-notes/documentation-conventions/#admonitions-call-outs","title":"Admonitions (Call-Outs)","text":""},{"location":"important-notes/documentation-conventions/#notes","title":"Notes","text":"<p>Notes include additional information that may be interesting but not crucial.</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#recommendations","title":"Recommendations","text":"<p>Recommendations include best practices and recommendations.</p> <p>Recommendation</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#infos","title":"Infos","text":"<p>Information boxes include background and links to additional information.</p> <p>Info</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#warnings","title":"Warnings","text":"<p>Warnings contain crucial information that should be considered before proceeding.</p> <p>Warning</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#dangers","title":"Dangers","text":"<p>Dangers contain crucial information that can lead to harmful consequences, such as data loss and irreversible damage.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/known-issues/","title":"Known Issues","text":""},{"location":"important-notes/known-issues/#kubernetes","title":"Kubernetes","text":"<ul> <li>Currently, it is not possible to resize a logical volume clone. The resize command does not fail and the new size    is shown by <code>lsblk</code>. But when remounting the filesystem with the option to resize, it fails.</li> </ul>"},{"location":"important-notes/known-issues/#aws","title":"AWS","text":"<ul> <li>During a VPC peering connection, all possible CIDRs from the request's VPC should be added to the route table.   Be aware that there might be more than one CIDR to be added.</li> </ul>"},{"location":"important-notes/known-issues/#control-plane","title":"Control Plane","text":"<ul> <li>In a highly available control plane cluster, Graylog currently runs only on the primary (first) node. Availability   is lost in case of an outage of the primary control plane node.</li> </ul>"},{"location":"important-notes/known-issues/#storage-plane","title":"Storage Plane","text":"<ul> <li>During background deletion of large volumes, if the storage node at which the deleted volume resides goes offline   or becomes unreachable before the delete completes, possible storage garbage may be left.</li> </ul>"},{"location":"important-notes/terminology/","title":"Terminology","text":""},{"location":"important-notes/terminology/#storage-related-terms","title":"Storage Related Terms","text":""},{"location":"important-notes/terminology/#storage-cluster","title":"Storage Cluster","text":"<p>A simplyblock storage cluster is a group of interconnected storage nodes that work together to provide a scalable, fault-tolerant, and high-performance storage system. Unlike traditional single-node storage solutions, storage clusters distribute data across multiple nodes, ensuring redundancy, load balancing, and resilience against hardware failures. To optimize data availability and efficiency, these clusters can be configured using different architectures, including replication and erasure coding. Storage clusters are commonly used in cloud storage, high-performance computing (HPC), and enterprise data centers, enabling seamless scalability and improved data accessibility across distributed environments.</p>"},{"location":"important-notes/terminology/#storage-node","title":"Storage Node","text":"<p>A storage node in a simplyblock distributed storage cluster is a physical or virtual machine that contributes storage resources to the cluster. It provides a portion of the overall storage capacity and participates in the data distribution, redundancy, and retrieval processes. In simplyblock, each logical volume is attached to particular primary and secondary storage nodes via the nmvf protocol. The nodes run the in-memory data services for this volume on the hot data path and provide access to underlying data. The data stored on such a volume is distributed within the cluster following a defined placement logic. </p>"},{"location":"important-notes/terminology/#storage-pool","title":"Storage Pool","text":"<p>A storage pool in simplyblock groups logical volumes and assigns them optional quotas (caps) of capacity, IOPS, and  read-write throughput. Storage pools are defined on a cluster level and can span logical volumes across multiple storage nodes. Therefore, storage pools implement a tenant concept.   </p>"},{"location":"important-notes/terminology/#storage-device","title":"Storage Device","text":"<p>A storage device is a physical or virtualized NVMe drive in simplyblock, but not a partition. It is identified by its PCIe address and serial number. Simplyblock currently supports a wide range of different types of NVMe drives with varying characteristics of performance, features, and capacities. </p>"},{"location":"important-notes/terminology/#nvme-non-volatile-memory-express","title":"NVMe (Non-Volatile Memory Express)","text":"<p>NVMe (Non-Volatile Memory Express) is a high-performance storage protocol explicitly designed for flash-based storage devices like SSDs, leveraging the PCIe (Peripheral Component Interconnect Express) interface for ultra-low latency and high throughput. Unlike traditional protocols such as SATA or SAS, NVMe takes advantage of parallelism and multiple queues, significantly improving data transfer speeds and reducing CPU overhead. It is widely used in enterprise storage, cloud computing, and high-performance computing (HPC) environments, where speed and efficiency are critical. NVMe is also the foundation for NVMe-over-Fabrics (NVMe-oF), which extends its benefits across networked storage systems, enhancing scalability and flexibility in distributed environments.</p>"},{"location":"important-notes/terminology/#nvme-of-nvme-over-fabrics","title":"NVMe-oF (NVMe over Fabrics)","text":"<p>NVMe-oF (NVMe over Fabrics) is an extension of the NVMe (Non-Volatile Memory Express) protocol that enables high-performance, low-latency access to remote NVMe storage devices over network fabrics such as TCP, RDMA (RoCE, iWARP), and Fibre Channel (FC). Unlike traditional networked storage protocols, NVMe-oF maintains the efficiency and parallelism of direct-attached NVMe storage while allowing disaggregation of compute and storage resources. This architecture improves scalability, resource utilization, and flexibility in cloud, enterprise, and high-performance computing (HPC) environments. NVMe-oF is a key technology in modern software-defined and disaggregated storage infrastructures, providing fast and efficient remote storage access.</p>"},{"location":"important-notes/terminology/#nvmetcp-nvme-over-tcp","title":"NVMe/TCP (NVMe over TCP)","text":"<p>NVMe/TCP (NVMe over TCP) is a transport protocol that extends NVMe-over-Fabrics (NVMe-oF) using standard TCP/IP networks to enable high-performance, low-latency access to remote NVMe storage. By leveraging existing Ethernet infrastructure, NVMe/TCP eliminates the need for specialized networking hardware such as RDMA (RoCE or iWARP) or Fibre Channel (FC), making it a cost-effective and easily deployable solution for cloud, enterprise, and data center storage environments. It maintains the efficiency of NVMe, providing scalable, high-throughput, and low-latency remote storage access while ensuring broad compatibility with modern network architectures.</p>"},{"location":"important-notes/terminology/#nvmeroce-nvme-over-rdma-over-converged-ethernet","title":"NVMe/RoCE (NVMe over RDMA over Converged Ethernet)","text":"<p>NVMe/RoCE (NVMe over RoCE) is a high-performance storage transport protocol that extends NVMe-over-Fabrics (NVMe-oF) using RDMA over Converged Ethernet (RoCE) to enable ultra-low-latency and high-throughput access to remote NVMe storage devices. By leveraging Remote Direct Memory Access (RDMA), NVMe/RoCE bypasses the CPU for data transfers, reducing latency and improving efficiency compared to traditional TCP-based storage protocols. This makes it ideal for high-performance computing (HPC), enterprise storage, and latency-sensitive applications such as financial trading and AI workloads. NVMe/RoCE requires lossless Ethernet networking and specialized NICs to fully utilize its performance advantages.</p>"},{"location":"important-notes/terminology/#multipathing","title":"Multipathing","text":"<p>Multipathing is a storage networking technique that enables multiple physical paths between a compute system and a storage device to improve redundancy, load balancing, and fault tolerance. Multipathing enhances performance and reliability by using multiple connections, ensuring continuous access to storage even if one path fails. It is commonly implemented in Fibre Channel (FC), iSCSI, and NVMe-oF (including NVMe/TCP and NVMe/RoCE) environments, where high availability and optimized data transfer are critical.</p>"},{"location":"important-notes/terminology/#management-node","title":"Management Node","text":"<p>A management node is a containerized component that orchestrates, monitors, and controls the distributed storage cluster. It forms part of the control plane, managing cluster-wide configurations, provisioning logical volumes, handling metadata operations, and ensuring overall system health. Management nodes facilitate communication between storage nodes and client applications, enforcing policies such as access control, data placement, and fault tolerance. They also provide an interface for administrators to interact with the storage system via the Simplyblock CLI or API, enabling seamless deployment, scaling, and maintenance of the storage infrastructure.</p>"},{"location":"important-notes/terminology/#distributed-erasure-coding","title":"Distributed Erasure Coding","text":"<p>Distributed Erasure coding is a data protection technique used in distributed storage systems to provide fault tolerance and redundancy while minimizing storage overhead. It works by breaking data into k data fragments and generating m parity fragments using mathematical algorithms. These k + m fragments are then distributed across multiple storage nodes, allowing the system to reconstruct lost or corrupted data from any k available fragments. Compared to traditional replication, erasure coding offers greater storage efficiency while maintaining high availability, making it ideal for cloud storage, object storage, and high-performance computing (HPC) environments where durability and cost-effectiveness are critical.</p> <p>Simplyblock supports all combinations of k = 1,2,4 and m = 1,2. The erasure coding implementation uses highly performance-optimized algorithms specific to the selected schema.</p>"},{"location":"important-notes/terminology/#replication","title":"Replication","text":"<p>Replication in storage is the process of creating and maintaining identical copies of data across multiple storage devices or nodes to ensure fault tolerance, high availability, and disaster recovery. Replication can occur synchronously, where data is copied in real-time to ensure consistency, or asynchronously, where updates are delayed to optimize performance. It is commonly used in distributed storage systems, cloud storage, and database management to protect against hardware failures and data loss. By maintaining redundant copies, replication enhances data resilience, load balancing, and accessibility, making it a fundamental technique for enterprise and cloud-scale storage solutions. Simplyblock supports synchronous replication.</p>"},{"location":"important-notes/terminology/#raid-redundant-array-of-independent-disks","title":"RAID (Redundant Array of Independent Disks)","text":"<p>RAID (Redundant Array of Independent Disks) is a data storage technology that combines multiple physical drives into a single logical unit to improve performance, fault tolerance, or both. RAID configurations vary based on their purpose: RAID 0 (striping) enhances speed but offers no redundancy, RAID 1 (mirroring) duplicates data for high availability, and RAID 5, 6, and 10 use combinations of striping and parity to balance performance and fault tolerance. RAID is widely used in enterprise storage, servers, and high-performance computing to protect against drive failures and optimize data access. It can be implemented in hardware controllers or software-defined storage solutions, depending on system requirements.</p>"},{"location":"important-notes/terminology/#quality-of-service","title":"Quality of Service","text":"<p>Quality of Service (QoS) refers to the ability to define and enforce performance guarantees for storage workloads by controlling key metrics such as IOPS (Input/Output Operations Per Second), throughput, and latency. QoS ensures that different applications receive appropriate levels of performance, preventing resource contention in multi-tenant environments. By setting limits and priorities for Logical Volumes (LVs), Simplyblock allows administrators to allocate storage resources efficiently, ensuring critical workloads maintain consistent performance even under high demand. This capability is essential for optimizing storage operations, improving reliability, and meeting service-level agreements (SLAs) in distributed cloud-native environments. In simplyblock, it is possible to limit (cap) IOPS or throughput of individual logical volumes or entire storage pools, and additionally to create QoS classes and provide a fair  relative resource allocation (IOPS and/or throughput) to each class. Logical volumes can be assigned to classes.</p>"},{"location":"important-notes/terminology/#spdk-storage-performance-development-kit","title":"SPDK (Storage Performance Development Kit)","text":"<p>Storage Performance Development Kit (SPDK) is an open-source set of libraries and tools designed to optimize high-performance, low-latency storage applications by bypassing traditional kernel-based I/O processing. SPDK leverages user-space and polled-mode drivers to eliminate context switching and interrupts, significantly reducing CPU overhead and improving throughput. It is particularly suited for NVMe storage, NVMe-over-Fabrics (NVMe-oF), and iSCSI target acceleration, making it a key technology in software-defined storage solutions. By providing a highly efficient framework for storage processing, SPDK enables modern storage architectures to achieve high IOPS, reduced latency, and better resource utilization in cloud and enterprise environments.</p>"},{"location":"important-notes/terminology/#volume-snapshot","title":"Volume Snapshot","text":"<p>A volume snapshot is a point-in-time copy of a storage volume, file system, or virtual machine that captures its state without duplicating the entire data set. Snapshots enable rapid data recovery, backup, and versioning by preserving only the changes made since the last snapshot, often using copy-on-write (COW) or redirect-on-write (ROW) techniques to minimize storage overhead. They are commonly used in enterprise storage, cloud environments, and virtualized systems to ensure data consistency, quick rollback capabilities, and protection against accidental deletions or system failures. Unlike full backups, snapshots are lightweight and allow near-instantaneous recovery of data.</p>"},{"location":"important-notes/terminology/#volume-clone","title":"Volume Clone","text":"<p>A volume clone is an exact, fully independent copy of a storage volume, virtual machine, or dataset that can be used for testing, development, backup, or deployment purposes. Unlike snapshots, which capture a point-in-time state and depend on the original data, a clone is a complete duplication that can operate separately without relying on the source. Cloning is commonly used in enterprise storage, cloud environments, and containerized applications to create quick, reproducible environments for workloads without affecting the original data. Storage systems often use thin cloning to optimize space by sharing unchanged data blocks between the original and the clone, reducing storage overhead. COW is widely implemented in storage virtualization and containerized environments, enabling fast, space-efficient backups, cloning, and data protection while maintaining high system performance.</p>"},{"location":"important-notes/terminology/#cow-copy-on-write","title":"CoW (Copy-on-Write)","text":"<p>Copy-on-Write (COW) is an efficient data management technique used in snapshots, cloning, and memory management to optimize storage usage and performance. Instead of immediately duplicating data, COW defers copying until a modification is made, ensuring that only changed data blocks are written to a new location. This approach minimizes storage overhead, speeds up snapshot creation, and reduces unnecessary data duplication.</p> <p></p>"},{"location":"important-notes/terminology/#kubernetes-related-terms","title":"Kubernetes Related Terms","text":""},{"location":"important-notes/terminology/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes (K8s)\u00a0\u29c9 is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications across clusters of machines. Initially developed by Google and now maintained by the Cloud Native Computing Foundation (CNCF)\u00a0\u29c9, Kubernetes provides a robust framework for load balancing, self-healing, storage orchestration, and automated rollouts and rollbacks. It manages application workloads using Pods, Deployments, Services, and Persistent Volumes (PVs), ensuring scalability and resilience. By abstracting underlying infrastructure, Kubernetes enables organizations to efficiently run containerized applications across on-premises, cloud, and hybrid environments, making it a cornerstone of modern cloud-native computing.</p>"},{"location":"important-notes/terminology/#kubernetes-csi-container-storage-interface","title":"Kubernetes CSI (Container Storage Interface)","text":"<p>The Kubernetes Container Storage Interface (CSI)\u00a0\u29c9 is a standardized API enabling external storage providers to integrate their storage solutions with Kubernetes. CSI allows Kubernetes to dynamically provision, attach, mount, and manage Persistent Volumes (PVs) across different storage backends without requiring changes to the Kubernetes core. Using a CSI driver, storage vendors can offer block and file storage to Kubernetes workloads, supporting advanced features like snapshotting, cloning, and volume expansion. CSI enhances Kubernetes\u2019 flexibility by enabling seamless integration with cloud, on-premises, and software-defined storage solutions, making it the de facto method for managing storage in containerized environments.</p>"},{"location":"important-notes/terminology/#pod","title":"Pod","text":"<p>A Pod in Kubernetes is the smallest and most basic deployable unit, representing a single instance of a running process in a cluster. A Pod can contain one or multiple containerized applications that share networking, storage, and runtime configurations, enabling efficient communication and resource sharing. Kubernetes schedules and manages Pods, ensuring they are deployed on suitable worker nodes based on resource availability and constraints. Since Pods are ephemeral, they are often managed by higher-level controllers like Deployments, StatefulSets, or DaemonSets to maintain availability and scalability. Pods facilitate scalable, resilient, and cloud-native application deployments across diverse infrastructure environments.</p>"},{"location":"important-notes/terminology/#persistent-volume","title":"Persistent Volume","text":"<p>A Persistent Volume (PV) is a cluster-wide Kubernetes storage resource that provides durable and independent storage for Pods allow data to persist beyond the lifecycle of individual containers. Unlike ephemeral storage, which is tied to a Pod\u2019s runtime, a PV is provisioned either statically by an administrator or dynamically using StorageClasses. Applications request storage by creating Persistent Volume Claims (PVCs), which Kubernetes binds to an available PV based on capacity and access requirements. Persistent Volumes support different access modes, such as ReadWriteOnce ( RWO), ReadOnlyMany (ROX), and ReadWriteMany (RWX), and are backed by various storage solutions, including local disks, network-attached storage (NAS), and cloud-based storage services.</p>"},{"location":"important-notes/terminology/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>A Persistent Volume Claim (PVC) is a request for Kubernetes storage made by a Pod, allowing it to dynamically or statically access a Persistent Volume (PV). PVCs specify storage requirements such as size, access mode (ReadWriteOnce, ReadOnlyMany, or ReadWriteMany), and storage class. Kubernetes automatically binds a PVC to a suitable PV based on these criteria, abstracting the underlying storage details from applications. This separation enables dynamic storage provisioning, ensuring that Pods can seamlessly consume persistent storage resources without needing direct knowledge of the storage infrastructure. When a PVC is deleted, its associated PV handling depends on its reclaim policy (Retain, Recycle, or Delete), determining whether the storage is preserved, cleared, or removed.</p>"},{"location":"important-notes/terminology/#storage-class","title":"Storage Class","text":"<p>A StorageClass is a Kubernetes abstraction that defines different types of storage available within a cluster, enabling dynamic provisioning of Persistent Volumes (PVs). It allows administrators to specify storage requirements such as performance characteristics, replication policies, and backend storage providers (e.g., cloud block storage, network file systems, or distributed storage systems). Each StorageClass includes a provisioner, which determines how volumes are created and parameters that define specific configurations for the underlying storage system. By referencing a StorageClass in a Persistent Volume Claim (PVC), users can automatically provision storage that meets their application's needs without manually pre-allocating PVs, streamlining storage management in cloud-native environments.</p>"},{"location":"important-notes/terminology/#network-related-terms","title":"Network Related Terms","text":""},{"location":"important-notes/terminology/#tcp-transmission-control-protocol","title":"TCP (Transmission Control Protocol)","text":"<p>Transmission Control Protocol (TCP) is a core communication protocol in the Internet Protocol (IP) suite that ensures reliable, ordered, and error-checked data delivery between devices over a network. TCP operates at the transport layer and establishes a connection-oriented communication channel using a three-way handshake process to synchronize data exchange. It segments large data streams into smaller packets, ensures their correct sequencing, and retransmits lost packets to maintain data integrity. TCP is widely used in applications requiring stable and accurate data transmission, such as web browsing, email, and file transfers, making it a fundamental protocol for modern networked systems.</p>"},{"location":"important-notes/terminology/#udp-user-datagram-protocol","title":"UDP (User Datagram Protocol)","text":"<p>User Datagram Protocol (UDP) is a lightweight, connectionless communication protocol in the Internet Protocol (IP) suite that enables fast, low-latency data transmission without guaranteeing delivery, order, or error correction. Unlike Transmission Control Protocol (TCP), UDP does not establish a connection before sending data, making it more efficient for applications prioritizing speed over reliability. It is commonly used in real-time communications, streaming services, online gaming, and DNS lookups, where occasional data loss is acceptable in exchange for reduced latency and overhead.</p>"},{"location":"important-notes/terminology/#ip-internet-protocol-ipv4-ipv6","title":"IP (Internet Protocol), IPv4, IPv6","text":"<p>Internet Protocol (IP) is the fundamental networking protocol that enables devices to communicate over the Internet and private networks by assigning unique IP addresses to each device. Operating at the network layer of the Internet Protocol suite, IP is responsible for routing and delivering data packets from a source to a destination based on their addresses. It functions in a connectionless manner, meaning each packet is sent independently and may take different paths to reach its destination. IP exists in two primary versions: IPv4, which uses 32-bit addresses, and IPv6, which uses 128-bit addresses for expanded address space. IP works alongside transport layer protocols like TCP and UDP to ensure effective data transmission across networks.</p>"},{"location":"important-notes/terminology/#netmask","title":"Netmask","text":"<p>A netmask is a numerical value used in IP networking to define a subnet's range of IP addresses. It works by masking a portion of an IP address to distinguish the network part from the host part. A netmask consists of a series of binary ones (1s) followed by zeros (0s), where the ones represent the network portion and the zeros indicate the host portion. Common netmasks include 255.255.255.0 (/24) for standard subnets and 255.255.0.0 (/16) for larger networks. Netmasks are essential in subnetting, routing, and IP address allocation, ensuring efficient traffic management and communication within networks.</p>"},{"location":"important-notes/terminology/#cidr-classless-inter-domain-routing","title":"CIDR (Classless Inter-Domain Routing)","text":"<p>Classless Inter-Domain Routing (CIDR) is a method for allocating and managing IP addresses more efficiently than the traditional class-based system. CIDR uses variable-length subnet masking (VLSM) to define IP address ranges with flexible subnet sizes, reducing wasted addresses and improving routing efficiency. CIDR notation represents an IP address followed by a slash (/) and a number indicating the number of significant bits in the subnet mask (e.g., <code>192.168.1.0/24</code> means the first 24 bits define the network, leaving 8 bits for host addresses). Widely used in modern networking and the internet, CIDR helps optimize IP address distribution and enhance routing aggregation, reducing the size of global routing tables.</p>"},{"location":"important-notes/terminology/#hyper-converged","title":"Hyper-Converged","text":"<p>Hyper-converged refers to an IT infrastructure model that integrates compute, storage, and networking into a single, software-defined system. Unlike traditional architectures that rely on separate hardware components for each function, hyper-converged infrastructure (HCI) leverages virtualization and centralized management to streamline operations, improve scalability, and reduce complexity. This approach enhances performance, fault tolerance, and resource efficiency by distributing workloads across multiple nodes, allowing seamless scaling by adding more nodes. HCI is widely used in cloud environments, virtual desktop infrastructure (VDI), and enterprise data centers for its ease of deployment, automation capabilities, and cost-effectiveness.</p>"},{"location":"important-notes/terminology/#disaggregated","title":"Disaggregated","text":"<p>Disaggregated refers to an IT architecture approach where compute, storage, and networking resources are separated into independent components rather than tightly integrated within the same physical system. In disaggregated storage, for example, storage resources are managed independently of compute nodes, allowing for flexible scaling, improved resource utilization, and reduced hardware dependencies. This contrasts with traditional or hyper-converged architectures, where these resources are combined. Disaggregated architectures are widely used in cloud computing, high-performance computing (HPC), and modern data centers to enhance scalability, cost-efficiency, and operational flexibility while optimizing performance for dynamic workloads.</p>"},{"location":"maintenance-operations/","title":"Operations","text":"<p>Ensuring data resilience and maintaining cluster health are critical aspects of managing a simplyblock storage deployment. This section covers best practices for backing up and restoring individual volumes or entire clusters, helping organizations safeguard their data against failures, corruption, or accidental deletions.</p> <p>Additionally, simplyblock provides comprehensive monitoring capabilities using built-in Prometheus and Grafana for real-time visualization of cluster health, I/O statistics, and performance metrics.</p> <p>For organizations leveraging third-party monitoring solutions, simplyblock supports integration with Datadog, AppDynamics, Dynatrace, and other observability platforms, enabling centralized performance tracking and alerting.</p> <p>This section details how to configure and use these monitoring tools, ensuring optimal performance, early issue detection, and proactive storage management in cloud-native and enterprise environments.</p>"},{"location":"maintenance-operations/cluster-upgrade/","title":"Upgrading a Cluster","text":"<p>Simplyblock clusters consist of two independent parts: a control plane with management nodes, and a storage plane with storage nodes. A single control plane can be used to manage for multiple storage planes.</p> <p>The control plane and storage planes can be updated independently. It is, however, not recommended to run an upgraded control plane without upgrading the storage planes.</p> <p>Recommendation</p> <p>If multiple storage planes are connected to a single control plane, it is recommended to upgrade the control plane first.</p> <p>Upgrading the control plane and storage cluster is an online operation and does not require downtime. Planning an upgrade as part of a maintenance window is recommended, though.</p>"},{"location":"maintenance-operations/cluster-upgrade/#upgrading-a-control-plane","title":"Upgrading a Control Plane","text":"<p>This section outlines the process of upgrading the control plane. An upgrade introduces new versions of the management and monitoring services.</p> <p>To upgrade a control plane, the following command must be executed:</p> <pre><code>sudo sbctl cluster update &lt;CLUSTER_ID&gt; --cp-only=true --restart=true\n</code></pre> <p>After issuing the command, the individual management services will be upgraded and restarted on all management nodes. </p>"},{"location":"maintenance-operations/cluster-upgrade/#upgrading-a-storage-plane","title":"Upgrading a Storage Plane","text":"<p>This section outlines the process of upgrading the storage plane, which is essential for maintaining data integrity, performance, and compatibility with newer system components. A well-executed upgrade ensures continued reliability and access to the latest features and fixes.</p> <p>To upgrade a storage plane, the following command must be executed:</p> <pre><code>sudo sbctl cluster update &lt;CLUSTER_ID&gt;\n</code></pre>"},{"location":"maintenance-operations/find-secondary-node/","title":"Finding the Secondary Node","text":"<p>Simplyblock, in high-availability mode, creates two connections per logical volume: a primary and a secondary connection.</p> <p>The secondary connection will be used in case of issues or failures of the primary storage node which owns the logical volume.</p> <p>For debugging purposes, sometimes it is useful to find out which host is used as the secondary for a specific primary storage node. This can be achieved using the command line tool <code>sbctl</code> by asking for the details of the primary storage node and grepping for the secondary id.</p> Find secondary for a primary<pre><code>sbctl storage-node get &lt;NODE_ID&gt; | grep secondary_node_id\n</code></pre>"},{"location":"maintenance-operations/migrating-storage-node/","title":"Migrating a Storage Node","text":"<p>Simplyblock storage clusters are designed as always-on. That means that a storage node migration is an online operation that doesn't require explicit maintenance windows or storage downtime.</p>"},{"location":"maintenance-operations/migrating-storage-node/#storage-node-migration","title":"Storage Node Migration","text":"<p>Migrating a storage node is a three-step process. First, the new storage node will be pre-deployed, then the old node will be restarted with the new node address, and finally, the new storage node will become the primary storage node.</p> <p>Warning</p> <p>Between each process step, it is required to wait for storage node migration tasks to complete. Otherwise, there may have an impact on the system's performance or, worse, may lead to data loss.</p> <p>As part of the process, the existing storage node id will be moved to the new host machine. All logical volumes allocated on the old storage node will be moved to the new storage node and will automatically be reconnected.</p>"},{"location":"maintenance-operations/migrating-storage-node/#first-stage-storage-node-deployment","title":"First-Stage Storage Node Deployment","text":"<p>To install the first stage of a storage node, the installation guide for the selected environment should be followed.</p> <p>The process will diverge after executing the initial deployment command <code>sbctl storage-node deploy</code>. If the command finishes successfully, resume from the next section of this page.</p> <ul> <li>Kubernetes</li> <li>Bare Metal or Virtualized Linux</li> <li>AWS EC2</li> </ul>"},{"location":"maintenance-operations/migrating-storage-node/#restart-old-storage-node","title":"Restart Old Storage Node","text":"<p>To start the migration process of logical volumes, the old storage node needs to be restarted with the new storage node's API address.</p> <p>In this example, it is assumed that the new storage node's IP address is 192.168.10.100. The IP address must be changed according to the real-world setup.</p> <p>Danger</p> <p>Providing the wrong IP address can lead to service interruption and data loss.</p> <p>To restart the node, the following command must be run:</p> Restarting a storage node to initiate the migration<pre><code>sbctl storage-node restart &lt;NODE_ID&gt; --node-addr=&lt;NEW_NODE_IP&gt;:5000\n</code></pre> <p>Warning</p> <p>The parameter <code>--node-addr</code> expects the API endpoint of the new storage node. This API is reachable on port 5000. It must be ensured that the given parameter is the new IP address and the port, separated by a colon.</p> Example output of the node restart<pre><code>demo@cp-1 ~&gt; sbctl storage-node restart 788c3686-9d75-4392-b0ab-47798fd4a3c1 --node-addr 192.168.10.64:5000\n2025-04-02 13:24:26,785: INFO: Restarting storage node\n2025-04-02 13:24:26,796: INFO: Setting node state to restarting\n2025-04-02 13:24:26,807: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"StorageNode\", \"message\": \"Storage node status changed from: unreachable to: in_restart\", \"caused_by\": \"monitor\"}\n2025-04-02 13:24:26,812: INFO: Sending event updates, node: 788c3686-9d75-4392-b0ab-47798fd4a3c1, status: in_restart\n2025-04-02 13:24:26,843: INFO: Sending to: f4b37b6c-6e36-490f-adca-999859747eb4\n2025-04-02 13:24:26,859: INFO: Sending to: 71c31962-7313-4317-8330-9f09a3e77a72\n2025-04-02 13:24:26,870: INFO: Sending to: 93a812f9-2981-4048-a8fa-9f39f562f1aa\n2025-04-02 13:24:26,893: INFO: Restarting on new node with ip: 192.168.10.64:5000\n2025-04-02 13:24:27,037: INFO: Restarting Storage node: 192.168.10.64\n2025-04-02 13:24:27,097: INFO: Restarting SPDK\n...\n2025-04-02 13:24:40,012: INFO: creating subsystem nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:13945596-4fbc-46a5-bbb1-ebe4d3e2af26\n2025-04-02 13:24:40,025: INFO: creating subsystem nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:2c593f82-d96c-4eb7-8d1c-30c534f6592d\n2025-04-02 13:24:40,037: INFO: creating subsystem nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:e3d2d790-4d14-4875-a677-0776335e4588\n2025-04-02 13:24:40,048: INFO: creating subsystem nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:1086d1bf-e77f-4ddf-b374-3575cfd68d30\n2025-04-02 13:24:40,414: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"StorageNode\", \"message\": \"Port blocked: 9091\", \"caused_by\": \"cli\"}\n2025-04-02 13:24:40,494: INFO: Add BDev to subsystem\n2025-04-02 13:24:40,495: INFO: 1\n2025-04-02 13:24:40,495: INFO: adding listener for nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:13945596-4fbc-46a5-bbb1-ebe4d3e2af26 on IP 10.10.10.64\n2025-04-02 13:24:40,499: INFO: Add BDev to subsystem\n2025-04-02 13:24:40,499: INFO: 1\n2025-04-02 13:24:40,500: INFO: adding listener for nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:e3d2d790-4d14-4875-a677-0776335e4588 on IP 10.10.10.64\n2025-04-02 13:24:40,503: INFO: Add BDev to subsystem\n2025-04-02 13:24:40,504: INFO: 1\n2025-04-02 13:24:40,504: INFO: adding listener for nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:2c593f82-d96c-4eb7-8d1c-30c534f6592d on IP 10.10.10.64\n2025-04-02 13:24:40,507: INFO: Add BDev to subsystem\n2025-04-02 13:24:40,508: INFO: 1\n2025-04-02 13:24:40,509: INFO: adding listener for nqn.2023-02.io.simplyblock:a84537e2-62d8-4ef0-b2e4-8462b9e8ea96:lvol:1086d1bf-e77f-4ddf-b374-3575cfd68d30 on IP 10.10.10.64\n2025-04-02 13:24:41,861: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"StorageNode\", \"message\": \"Port allowed: 9091\", \"caused_by\": \"cli\"}\n2025-04-02 13:24:41,894: INFO: Done\nSuccess\n</code></pre>"},{"location":"maintenance-operations/migrating-storage-node/#make-new-storage-node-primary","title":"Make new Storage Node Primary","text":"<p>After the migration has successfully finished, the new storage node must be made the primary storage node for the owned set of logical volumes.</p> <p>This can be initiated using the following command:</p> Make the new storage node the primary<pre><code>sbctl storage-node make-primary &lt;NODE_ID&gt;\n</code></pre> <p>The following is the example output.</p> Example output of primary change<pre><code>demo@cp-1 ~&gt; sbctl storage-node make-primary 788c3686-9d75-4392-b0ab-47798fd4a3c1\n2025-04-02 13:25:02,220: INFO: Adding device 65965029-4ab3-44b9-a9d4-29550e6c14ae\n2025-04-02 13:25:02,251: INFO: bdev already exists alceml_65965029-4ab3-44b9-a9d4-29550e6c14ae\n2025-04-02 13:25:02,252: INFO: bdev already exists alceml_65965029-4ab3-44b9-a9d4-29550e6c14ae_PT\n2025-04-02 13:25:02,266: INFO: subsystem already exists True\n2025-04-02 13:25:02,267: INFO: bdev already added to subsys alceml_65965029-4ab3-44b9-a9d4-29550e6c14ae_PT\n2025-04-02 13:25:02,285: INFO: Setting device online\n2025-04-02 13:25:02,301: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"NVMeDevice\", \"message\": \"Device created: 65965029-4ab3-44b9-a9d4-29550e6c14ae\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,305: INFO: Make other nodes connect to the node devices\n2025-04-02 13:25:02,383: INFO: Connecting to node 71c31962-7313-4317-8330-9f09a3e77a72\n2025-04-02 13:25:02,384: INFO: bdev found remote_alceml_197c2d40-d39a-4a10-84eb-41c68a6834c7_qosn1\n2025-04-02 13:25:02,385: INFO: bdev found remote_alceml_5202854e-e3b3-4063-b6b9-9a83c1bbefe9_qosn1\n2025-04-02 13:25:02,386: INFO: bdev found remote_alceml_15c5f6de-63b6-424c-b4c0-49c3169c0135_qosn1\n2025-04-02 13:25:02,386: INFO: Connecting to node 93a812f9-2981-4048-a8fa-9f39f562f1aa\n2025-04-02 13:25:02,439: INFO: Connecting to node f4b37b6c-6e36-490f-adca-999859747eb4\n2025-04-02 13:25:02,440: INFO: bdev found remote_alceml_0544ef17-6130-4a79-8350-536c51a30303_qosn1\n2025-04-02 13:25:02,441: INFO: bdev found remote_alceml_e9d69493-1ce8-4386-af1a-8bd4feec82c6_qosn1\n2025-04-02 13:25:02,442: INFO: bdev found remote_alceml_5cc0aed8-f579-4a4c-9c31-04fb8d781af8_qosn1\n2025-04-02 13:25:02,443: INFO: Connecting to node 93a812f9-2981-4048-a8fa-9f39f562f1aa\n2025-04-02 13:25:02,493: INFO: Connecting to node f4b37b6c-6e36-490f-adca-999859747eb4\n2025-04-02 13:25:02,494: INFO: bdev found remote_alceml_0544ef17-6130-4a79-8350-536c51a30303_qosn1\n2025-04-02 13:25:02,494: INFO: bdev found remote_alceml_e9d69493-1ce8-4386-af1a-8bd4feec82c6_qosn1\n2025-04-02 13:25:02,495: INFO: bdev found remote_alceml_5cc0aed8-f579-4a4c-9c31-04fb8d781af8_qosn1\n2025-04-02 13:25:02,495: INFO: Connecting to node 71c31962-7313-4317-8330-9f09a3e77a72\n2025-04-02 13:25:02,496: INFO: bdev found remote_alceml_197c2d40-d39a-4a10-84eb-41c68a6834c7_qosn1\n2025-04-02 13:25:02,496: INFO: bdev found remote_alceml_5202854e-e3b3-4063-b6b9-9a83c1bbefe9_qosn1\n2025-04-02 13:25:02,497: INFO: bdev found remote_alceml_15c5f6de-63b6-424c-b4c0-49c3169c0135_qosn1\n2025-04-02 13:25:02,667: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 773ae420-3491-4ea6-aaf4-b7b1103132f6\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,675: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 95eaf69f-6926-454e-a023-8d9341f7c4c6\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,682: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 0a0f7942-46d7-46b2-9dc6-c5787bc3691e\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,690: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 0f10c95e-937b-4e9b-99ca-e13815ae3578\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,698: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: fb36c4c7-d128-4a43-894f-50fb406bab30\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,707: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: d5480f1f-e113-49ab-8c9d-3663e7ba512b\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,717: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 8e910437-7957-4701-b626-5dffce0284dc\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,727: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 919fceb4-ee48-4c72-96b0-a4367b8d0f67\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,737: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: da076017-c0ba-4e5b-8bcd-7748fa56305e\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,748: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: fa43687f-33ff-486d-8460-2b07bbc18cff\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,757: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: e53431ce-c7c9-40a9-8e11-4dafefce79d8\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,768: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: 38e320ca-1fd1-4f8e-9ef1-2defa50f1d22\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,813: INFO: Adding device 7e5145e7-d8fc-4d60-8af1-3f5015cb3021\n2025-04-02 13:25:02,837: INFO: bdev already exists alceml_7e5145e7-d8fc-4d60-8af1-3f5015cb3021\n2025-04-02 13:25:02,837: INFO: bdev already exists alceml_7e5145e7-d8fc-4d60-8af1-3f5015cb3021_PT\n2025-04-02 13:25:02,851: INFO: subsystem already exists True\n2025-04-02 13:25:02,852: INFO: bdev already added to subsys alceml_7e5145e7-d8fc-4d60-8af1-3f5015cb3021_PT\n2025-04-02 13:25:02,879: INFO: Setting device online\n2025-04-02 13:25:02,893: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"NVMeDevice\", \"message\": \"Device created: 7e5145e7-d8fc-4d60-8af1-3f5015cb3021\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:02,897: INFO: Make other nodes connect to the node devices\n2025-04-02 13:25:02,968: INFO: Connecting to node 71c31962-7313-4317-8330-9f09a3e77a72\n2025-04-02 13:25:02,969: INFO: bdev found remote_alceml_197c2d40-d39a-4a10-84eb-41c68a6834c7_qosn1\n2025-04-02 13:25:02,970: INFO: bdev found remote_alceml_5202854e-e3b3-4063-b6b9-9a83c1bbefe9_qosn1\n2025-04-02 13:25:02,971: INFO: bdev found remote_alceml_15c5f6de-63b6-424c-b4c0-49c3169c0135_qosn1\n2025-04-02 13:25:02,971: INFO: Connecting to node 93a812f9-2981-4048-a8fa-9f39f562f1aa\n...\n2025-04-02 13:25:10,255: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: a4692e1d-a527-44f7-8a86-28060eb466cf\", \"caused_by\": \"cli\"}\n2025-04-02 13:25:10,277: INFO: {\"cluster_id\": \"a84537e2-62d8-4ef0-b2e4-8462b9e8ea96\", \"event\": \"OBJ_CREATED\", \"object_name\": \"JobSchedule\", \"message\": \"task created: bab06208-bd27-4002-bc7b-dd92cf7b9b66\", \"caused_by\": \"cli\"}\nTrue\n</code></pre> <p>At this point, the old storage node is automatically removed from the cluster, and the storage node id is taken over by the new storage node. Any operation on the old storage node, such as an OS reinstall, can be safely executed.</p>"},{"location":"maintenance-operations/node-affinity/","title":"Configure Node Affinity","text":"<p>Simplyblock features node affinity, sometimes also referred to as data locality. This feature ensures that storage volumes are physically co-located on storage or Kubernetes worker nodes running the corresponding workloads. This minimizes network latency and maximizes I/O performance by keeping data close to the application. Ideal for latency-sensitive workloads, node affinity enables smarter, faster, and more efficient storage access in hyper-converged and hybrid environments.</p> <p>Info</p> <p>Node affinity is only available with hyper-converged or hybrid setups.</p> <p>Node affinity does not sacrifice fault tolerance, as parity data will still be distributed to other storage cluster nodes enabling transparent failover in case of a failure, or spill over in the situation where the locally available storage runs out of available capacity.</p>"},{"location":"maintenance-operations/node-affinity/#enabling-node-affinity","title":"Enabling Node Affinity","text":"<p>To use node affinity, the storage cluster needs to be created with node affinity activated. When node affinity is enabled for a logical volume, it will influence how the data distribution algorithm will handle read and write requests.</p> <p>To enable node affinity at creation time of the cluster, the <code>--enable-node-affinity</code> parameter needs to be added:</p> Enabling node affinity when the cluster is created<pre><code>sbctl cluster create \\\n    --ifname=&lt;IF_NAME&gt; \\\n    --ha-type=ha \\\n    --enable-node-affinity # &lt;- this is important\n</code></pre> <p>To see all available parameters for cluster creation, see Cluster Create.</p> <p>When the cluster was created with node affinity enabled, logical volumes can be created with node affinity, which will always try to locate data co-located with the requested storage node. </p>"},{"location":"maintenance-operations/node-affinity/#create-a-node-affine-logical-volume","title":"Create a Node Affine Logical Volume","text":"<p>When creating a logical volume, it is possible to provide a host id (storage node UUID) to request the storage cluster to co-locate the volume with this storage node. This configuration will have no influence on storage clusters without node affinity enabled.</p> <p>To create a co-located logical volume, the parameter <code>--host-id</code> needs to be added to the creation command:</p> Create a node affine logical volume<pre><code>sbctl volume add &lt;NAME&gt; &lt;SIZE&gt; &lt;POOL&gt; \\\n    --host-id=&lt;HOST_ID&gt; \\\n    ... # other parameters\n</code></pre> <p>To see all available parameters for a logical volume creation, see Logical Volume Creation.</p> <p>The storage node UUID (or host id) can be found using the <code>sbctl storage-node list</code> command.</p> List all storage nodes in a storage cluster<pre><code>sbctl storage-node list --cluster-id=&lt;CLUSTER_ID&gt;\n</code></pre>"},{"location":"maintenance-operations/reconnect-nvme-device/","title":"Reconnecting Logical Volume","text":"<p>After outages of storage nodes, primary and secondary NVMe over Fabrics connections may need to be re-established. With integrations such as simplyblock's Kubernetes CSI driver and the Proxmox integration, this is automatically handled.</p> <p>With plain Linux clients, the connections have to be reconnected manually. This is especially important when a storage node is unavailable for more than 60 seconds (by default).</p>"},{"location":"maintenance-operations/reconnect-nvme-device/#reconnect-a-missing-nvme-controller","title":"Reconnect a Missing NVMe Controller","text":"<p>To reconnect the NVMe controllers for the logical volume, the normal nvme connect commands are executed again. This will immediately reconnect missing controllers and connection paths.</p> Retrieve connection strings<pre><code>{cliname} volume connect &lt;VOLUME_ID&gt;\n</code></pre> Example output for connection string retrieval<pre><code>[demo@demo ~]# {cliname} volume connect 82e587c5-4a94-42a1-86e5-a5b8a6a75fc4\nsudo nvme connect --reconnect-delay=2 --ctrl-loss-tmo=60 --nr-io-queues=6 --keep-alive-tmo=5 --transport=tcp --traddr=192.168.10.112 --trsvcid=9100 --nqn=nqn.2023-02.io.simplyblock:0f2c4cb0-a71c-4830-bcff-11112f0ee51a:lvol:82e587c5-4a94-42a1-86e5-a5b8a6a75fc4\nsudo nvme connect --reconnect-delay=2 --ctrl-loss-tmo=60 --nr-io-queues=6 --keep-alive-tmo=5 --transport=tcp --traddr=192.168.10.113 --trsvcid=9100 --nqn=nqn.2023-02.io.simplyblock:0f2c4cb0-a71c-4830-bcff-11112f0ee51a:lvol:82e587c5-4a94-42a1-86e5-a5b8a6a75fc4\n</code></pre>"},{"location":"maintenance-operations/reconnect-nvme-device/#increase-loss-timeout","title":"Increase Loss Timeout","text":"<p>Alternatively, depending on the environment, it is possible to increase the timeout after which Linux assumes the NVMe controller to be lost and stops with reconnection attempts.</p> <p>To increase the timeout, the parameter --ctrl-loss-tmo can be increased. The value is the number of seconds until the Linux kernel stops the reconnection attempt and removes the controller from the list of valid multipath routes.</p>"},{"location":"maintenance-operations/replacing-storage-node/","title":"Replacing a Storage Node","text":"<p>A simplyblock storage cluster is designed to be always up. Hence, operations such as extending a cluster or replacing a storage node are online operations and don't require a system downtime. However, there are a few things to keep in mind when replacing a storage node.</p> <p>Danger</p> <p>If a storage node should be migrated, Migrating a Storage Node must be followed. Removing a storage node from a simplyblock cluster without migrating it will make the logical volumes owned by this storage node inaccessible!</p>"},{"location":"maintenance-operations/replacing-storage-node/#starting-the-new-storage-node","title":"Starting the new Storage Node","text":"<p>It is always recommended to start the new storage node before removing the old one, even if the remaining cluster has enough storage available to absorb the additional (temporary) storage requirement.</p> <p>Every operation that changes the cluster topology comes with a set of migration tasks, moving data across the cluster to ensure equal usage distribution.</p> <p>If a storage node failed and cannot be recovered, adding a new storage node is perfectly fine, though.</p> <p>To start a new storage node, follow the storage node installation according to your chosen setup:</p> <ul> <li>Kubernetes</li> <li>Bare Metal or Virtualized Linux</li> <li>AWS EC2</li> </ul>"},{"location":"maintenance-operations/replacing-storage-node/#remove-the-old-storage-node","title":"Remove the old Storage Node","text":"<p>Danger</p> <p>All volumes on this storage node, which haven't been migrated before the removal, will become inaccessible!</p> <p>To remove the old storage node, use the <code>sbctl</code> command line tool. </p> Remove a storage node<pre><code>sbctl storage-node remove &lt;NODE_ID&gt;\n</code></pre> <p>Wait until the operation has successfully finished. Afterward, the storage node is removed from the cluster.</p> <p>This can be checked again with the <code>sbctl</code> command line tool.</p> List storage nodes<pre><code>sbctl storage-node list --cluster-id=&lt;CLUSTER_ID&gt;\n</code></pre>"},{"location":"maintenance-operations/monitoring/","title":"Monitoring","text":"<p>Monitoring the health, performance, and resource utilization of a Simplyblock cluster is crucial for ensuring optimal operation, early issue detection, and efficient capacity planning. The <code>sbctl</code> command line interface provides a comprehensive set of tools to retrieve real-time and historical metrics related to Logical Volumes (LVs), storage nodes, I/O performance, and system status. By leveraging <code>sbctl</code>, administrators can quickly diagnose bottlenecks, monitor resource consumption, and maintain overall system stability.</p>"},{"location":"maintenance-operations/monitoring/accessing-grafana/","title":"Accessing Grafana","text":"<p>Simplyblock's control plane includes a Prometheus, Grafana, and Graylog installation.</p> <p>Grafana retrieves metric data from Prometheus, including capacity, I/O statistics, and the cluster event log. Additionally, Grafana is used for alerting via Slack or email.</p> <p>The standard retention period for metrics is 7 days. However, this can be changed when creating a cluster.</p>"},{"location":"maintenance-operations/monitoring/accessing-grafana/#how-to-access-grafana","title":"How to access Grafana","text":"<p>Grafana can be accessed through all management node API. It is recommended to set up a load balancer with session stickyness in front of the Grafana installation(s).</p> Grafana URLs<pre><code>http://&lt;MGMT_NODE_IP&gt;/grafana\n</code></pre> <p>To retrieve the endpoint address from the cluster itself, use the following command:</p> Retrieving the Grafana endpoint<pre><code>sbctl cluster get &lt;CLUSTER_ID&gt; | grep grafana_endpoint\n</code></pre>"},{"location":"maintenance-operations/monitoring/accessing-grafana/#credentials","title":"Credentials","text":"<p>The Grafana installation uses the cluster secret as its password for the user admin. To retrieve the cluster secret, the following commands should be used:</p> Get the cluster uuid<pre><code>sbctl cluster list\n</code></pre> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> <p>Credentials Username:  Password: </p>"},{"location":"maintenance-operations/monitoring/accessing-grafana/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>All dashboards are stored in per-cluster folders. Each cluster contains the following dashboard entries:</p> <ul> <li>Cluster</li> <li>Storage node</li> <li>Device</li> <li>Logical Volume</li> <li>Storage Pool</li> <li>Storage Plane node(s) system monitoring</li> <li>Control Plane node(s) system monitoring</li> </ul> <p>Dashboard widgets are designed to be self-explanatory.</p> <p>By default, each dashboard contains data for all objects (e.g., all devices) in a cluster. It is, however, possible to filter them by particular objects (e.g., devices, storage nodes, or logical volumes) and to change the timescale and window.</p> <p>Dashboards include physical and logical capacity utilization dynamics, IOPS, I/O throughput, and latency dynamics (all separate for read, write, and unmap). While all data from the event log is currently stored in Prometheus, they weren't used at the time of writing.</p>"},{"location":"maintenance-operations/monitoring/accessing-graylog/","title":"Accessing Graylog","text":"<p>Simplyblock's control plane includes a Prometheus, Grafana, and Graylog installation.</p> <p>Graylog retrieves logs for all control plane and storage node services.</p> <p>The standard retention period for metrics is 7 days. However, this can be changed when creating a cluster.</p>"},{"location":"maintenance-operations/monitoring/accessing-graylog/#how-to-access-graylog","title":"How to access Graylog","text":"<p>Graylog can be accessed through all management node API. It is recommended to set up a load balancer with session stickyness in front of the Graylog installation(s).</p> Graylog URLs<pre><code>http://&lt;MGMT_NODE_IP&gt;/graylog\n</code></pre>"},{"location":"maintenance-operations/monitoring/accessing-graylog/#credentials","title":"Credentials","text":"<p>The Graylog installation uses the cluster secret as its password for the user admin. To retrieve the cluster secret, the following command should be used:</p> Get the cluster secret<pre><code>sbctl cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> <p>Credentials Username: admin Password: </p>"},{"location":"maintenance-operations/monitoring/alerts/","title":"Alerting","text":"<p>Simplyblock uses Grafana to configure and manage alerting rules.</p> <p>By default, Grafana is configured to send alerts to Slack channels. However, Grafana also allows alerting via email notifications, but this requires the use of an authorized SMTP server to send a message.</p> <p>An SMTP server is currently not part of the management stack and must be deployed separately. Alerts can be triggered based on on-time or interval-based thresholds of statistical data collected (IO statistics, capacity information) or based on events from the cluster event log.</p>"},{"location":"maintenance-operations/monitoring/alerts/#pre-defined-alerts","title":"Pre-Defined Alerts","text":"<p>The following pre-defined alerts are available:</p> Alert Trigger device-unavailable Storage device became unavailable. device-read-only Storage device changed to status: read-only. cluster-status-degraded Storage node changed to status: degraded. cluster-status-suspended Storage node changed to status: suspended. storage-node-unreachable Storage node became unreachable. storage-node-offline Storage node became unavailable. storage-node-healthcheck-failure Storage node with negative healthcheck. logical-volume-offline Logical volume became unavailable. critical-capacity-reached Critical absolute capacity utilization in a cluster was reached. The threshold value can be configured at cluster creation time using <code>--cap-crit</code>. critical-provisioning-capacity-reached Critical absolute provisioned capacity utilization in a cluster was reached. The threshold value can be configured at cluster creation time using <code>--prov-cap-crit</code>. root-fs-low-disk-space Root filesystem free disk space is below 20%. <p>It is possible to configure the Slack webhook for alerting during cluster creation or to modify it at a later point in time.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/","title":"Cluster Health","text":"<p>A simplyblock cluster consists of interconnected management nodes (control plane) and storage nodes (storage plane) working together to deliver a resilient, distributed storage platform. Monitoring the overall health, availability, and performance of the cluster is essential for ensuring data integrity, fault tolerance, and optimal operation under varying workloads. Simplyblock provides detailed metrics and status indicators at both the node and cluster levels to help administrators proactively detect issues and maintain system stability.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/#accessing-cluster-status","title":"Accessing Cluster Status","text":"<p>To access a cluster's status, the <code>sbctl</code> command line tool can be used:</p> Accessing the status of a cluster<pre><code>sbctl cluster status &lt;CLUSTER_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/#accessing-cluster-statistics","title":"Accessing Cluster Statistics","text":"<p>To access a cluster's performance and I/O statistics, the <code>sbctl</code> command line tool can be used:</p> Accessing the statistics of a cluster<pre><code>sbctl cluster show &lt;CLUSTER_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p> <p>The information is also available through Grafana in the cluster's dashboard.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/#accessing-cluster-io-statistics","title":"Accessing Cluster I/O Statistics","text":"<p>To access a cluster's performance and I/O statistics, the <code>sbctl</code> command line tool can be used:</p> Accessing the I/O statistics of a cluster<pre><code>sbctl cluster get-io-stats &lt;CLUSTER_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p> <p>The information is also available through Grafana in the cluster's dashboard.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/#accessing-cluster-capacity-information","title":"Accessing Cluster Capacity Information","text":"<p>To access a cluster's capacity information, the <code>sbctl</code> command line tool can be used:</p> Accessing the capcity information of a cluster<pre><code>sbctl cluster get-capacity &lt;CLUSTER_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p>"},{"location":"maintenance-operations/monitoring/cluster-health/#accessing-cluster-health-information","title":"Accessing Cluster Health Information","text":"<p>To access a cluster's health status, the <code>sbctl</code> command line tool can be used:</p> Accessing the health status of a cluster<pre><code>sbctl cluster check &lt;CLUSTER_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p>"},{"location":"maintenance-operations/monitoring/io-stats/","title":"Accessing I/O Stats ({{ cliname }})","text":"<p>Simplyblock's <code>sbctl</code> tool provides the option to retrieve some extensive I/O statistics. Those contain a number of relevant metrics of historic and current I/O activities per device, storage node, logical volume, and cluster.</p> <p>These metrics include:</p> <ul> <li>Read and write throughput (in MB/s)</li> <li>I/O operations per second (IOPS) for read, write, and unmap</li> <li>Total amount of bytes read and written</li> <li>Total number of I/O operations since the start of a node</li> <li>Latency ticks</li> <li>Average read, write, and unmap latency</li> </ul>"},{"location":"maintenance-operations/monitoring/io-stats/#accessing-cluster-statistics","title":"Accessing Cluster Statistics","text":"<p>To access cluster-wide statistics, use the following command:</p> Accessing cluster-wide I/O statistics<pre><code>sbctl cluster get-io-stats &lt;CLUSTER_ID&gt;\n</code></pre> <p>More information about the command is available in the CLI reference section.</p>"},{"location":"maintenance-operations/monitoring/io-stats/#accessing-storage-node-statistics","title":"Accessing Storage Node Statistics","text":"<p>To access the I/O statistics of a storage node (which includes all physical NVMe devices), use the following command:</p> Accessing storage node I/O statistics<pre><code>sbctl storage-node get-io-stats &lt;NODE_ID&gt;\n</code></pre> <p>More information about the command is available in the CLI reference section.</p> <p>To access the I/O statistics of a specific device in a storage node, use the following command:</p> Accessing storage node device I/O statistics<pre><code>sbctl storage-node get-io-stats-device &lt;DEVICE_ID&gt;\n</code></pre> <p>More information about the command is available in the CLI reference section.</p>"},{"location":"maintenance-operations/monitoring/io-stats/#accessing-storage-pool-statistics","title":"Accessing Storage Pool Statistics","text":"<p>To access logical volume-specific statistics, use the following command:</p> Accessing storage pool I/O statistics<pre><code>sbctl storage-pool get-io-stats &lt;POOL_ID&gt;\n</code></pre> <p>More information about the command is available in the CLI reference section.</p>"},{"location":"maintenance-operations/monitoring/io-stats/#accessing-logical-volume-statistics","title":"Accessing Logical Volume Statistics","text":"<p>To access logical volume-specific statistics, use the following command:</p> Accessing logical volume I/O statistics<pre><code>sbctl volume get-io-stats &lt;VOLUME_ID&gt;\n</code></pre> <p>More information about the command is available in the CLI reference section.</p>"},{"location":"maintenance-operations/monitoring/lvol-conditions/","title":"Logical Volume Conditions","text":"<p>Logical volumes are the core storage abstraction in simplyblock, representing high-performance, distributed NVMe block devices backed by the cluster. Maintaining visibility into the health, status, and performance of these volumes is critical for ensuring workload reliability, troubleshooting issues, and planning resource utilization. Simplyblock continuously monitors volume-level metrics and exposes them through both CLI and observability tools, giving operators detailed insight into system behavior.</p>"},{"location":"maintenance-operations/monitoring/lvol-conditions/#accessing-logical-volume-statistics","title":"Accessing Logical Volume Statistics","text":"<p>To access a logical volume's performance and I/O statistics, the <code>sbctl</code> command line tool can be used:</p> Accessing the statistics of a logical volume<pre><code>sbctl volume get-io-stats &lt;VOLUME_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p> <p>The information is also available through Grafana in the logical volume's dashboard.</p>"},{"location":"maintenance-operations/monitoring/lvol-conditions/#accessing-logical-volume-health-information","title":"Accessing Logical Volume Health Information","text":"<p>To access a logical volume's health status, the <code>sbctl</code> command line tool can be used:</p> Accessing the health status of a logical volume<pre><code>sbctl volume check &lt;VOLUME_ID&gt;\n</code></pre> <p>All details of the command are available in the CLI reference.</p>"},{"location":"maintenance-operations/monitoring/third-party/","title":"Integration with Third-Party Tools","text":"<p>Simplyblock provides robust native monitoring capabilities but also supports integration with a wide range of third-party observability platforms to enable centralized monitoring, alerting, and analytics across your infrastructure.</p> <p>While simplyblock provides Prometheus, Grafana, and Graylog, existing monitoring and observability tools, such as Datadog, AppDynamics, or Dynatrace, might be the preferred way to maintain visibility into cluster performance, health, and resource usage.</p> <p>Almost all third-party monitoring and observability tools support access to Prometheus or OpenMetrics.</p>"},{"location":"maintenance-operations/monitoring/third-party/appdynamics/","title":"AppDynamics","text":"<p>Simplyblock uses Prometheus to collect storage and cluster metrics. Hence, it can easily be integrated with external monitoring and observability solutions.</p> <p>AppDynamics provides a specific extension to access Prometheus instances. The extension is available from their GitHub repository\u00a0\u29c9.</p>"},{"location":"maintenance-operations/monitoring/third-party/datadog/","title":"DataDog","text":"<p>Simplyblock uses Prometheus to collect storage and cluster metrics. Hence, it can easily be integrated with external monitoring and observability solutions.</p> <p>DataDog provides an extensive documentation on how to access Prometheus instances in the documentation\u00a0\u29c9.</p> <p>On Kubernetes-based deployments, the Prometheus (OpenMetrics) data can be collected by the DataDog agent directly. The necessary configuration is available from their documentation\u00a0\u29c9.</p>"},{"location":"maintenance-operations/monitoring/third-party/dynatrace/","title":"DynaTrace","text":"<p>Simplyblock uses Prometheus to collect storage and cluster metrics. Hence, it can easily be integrated with external monitoring and observability solutions.</p> <p>DynaTrace provides an extensive documentation on how to access Prometheus instances in the documentation\u00a0\u29c9.</p>"},{"location":"maintenance-operations/scaling/","title":"Scaling","text":"<p>Simplyblock is designed with a scale-out architecture that enables seamless growth of both storage capacity and performance by simply adding more nodes to the cluster. Built for modern, cloud-native environments, simplyblock supports linear scalability across compute, network, and storage layers\u2014without downtime or disruption to active workloads. Whether you're scaling to accommodate petabytes of data, high IOPS requirements, or enhanced throughput, simplyblock delivers predictable performance and resilience at scale.</p>"},{"location":"maintenance-operations/scaling/expanding-storage-cluster/","title":"Expanding a Storage Cluster","text":"<p>Simplyblock is designed as an always-on storage solution. Hence, storage cluster expansion is an online operation without a need for maintenance downtime.</p> <p>However, every operation that changes the cluster topology comes with a set of migration tasks, moving data across the cluster to ensure equal usage distribution. While these migration tasks are low priority and their overhead is designed to be minimal, it is still recommended to expand the cluster at times when the storage cluster isn't under full utilization.</p> <p>To start a new storage node, follow the storage node installation according to your chosen set-up:</p> <ul> <li>Kubernetes</li> <li>Bare Metal or Virtualized Linux</li> <li>AWS EC2</li> </ul>"},{"location":"maintenance-operations/scaling/expanding-storage-pool/","title":"Expanding a Storage Pool","text":"<p>Simplyblock is designed as on always-on a storage system. Therefore, expanding a storage pool is an online operation and does not require a maintenance window or system downtime.</p> <p>When expanding a storage pool, its capacity will be extended, offering an extended quota of the overall storage cluster. </p>"},{"location":"maintenance-operations/scaling/expanding-storage-pool/#storage-pool-expansion","title":"Storage Pool Expansion","text":"<p>To expand a storage pool, the <code>sbctl</code> command line interface:</p> Expanding the storage pool<pre><code>sbctl storage-pool set &lt;POOL_ID&gt; --pool-max=&lt;NEW_SIZE&gt;\n</code></pre> <p>The value of NEW_SIZE must be given as <code>20G</code>, <code>20T</code>, etc.</p> <p>All valid parameters can be found in the  Storage Pool CLI Reference. </p>"},{"location":"maintenance-operations/security/","title":"Security","text":"<p>Security is a core pillar of the simplyblock platform, designed to protect data across every layer of the storage stack. From encryption at rest to multi-tenant isolation and secure communications, simplyblock provides robust, enterprise-grade features that help meet stringent compliance and data protection requirements. Security is enforced by design, ensuring your workloads and sensitive data remain protected against internal and external threats.</p>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/","title":"Encrypting with Kubernetes Secrets","text":"<p>Simplyblock supports encryption of logical volumes (LVs) to protect data at rest, ensuring that sensitive information remains secure across the distributed storage cluster. Encryption is applied during volume creation as part of the storage class specification.</p> <p>Encrypting Logical Volumes ensures that simplyblock storage meets data protection and compliance requirements, safeguarding sensitive workloads without compromising performance.</p> <p>Warning</p> <p>Encryption must be specified at the time of volume creation. Existing logical volumes cannot be retroactively encrypted.</p>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/#encrypting-volumes-with-simplyblock","title":"Encrypting Volumes with Simplyblock","text":"<p>Simplyblock supports the encryption of logical volumes. Internally, simplyblock utilizes the industry-proven crypto bdev\u00a0\u29c9 provided by SPDK to implement its encryption functionality.</p> <p>The encryption uses an AES_XTS variable-length block cipher. This cipher requires two keys of 16 to 32 bytes each. The keys need to have the same length, meaning that if one key is 32 bytes long, the other one has to be 32 bytes, too.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two keys of 32 bytes.</p>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/#generate-random-keys","title":"Generate Random Keys","text":"<p>Simplyblock does not provide an integrated way to generate encryption keys, but recommends using the OpenSSL tool chain. For Kubernetes, the encryption key needs to be provided as base64. Hence, it's encoded right away.</p> <p>To generate the two keys, the following command is run twice. The result must be stored for later.</p> Create an Encryption Key<pre><code>openssl rand -hex 32 | base64 -w0\n</code></pre>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/#create-the-kubernetes-secret","title":"Create the Kubernetes Secret","text":"<p>Next up, a Kubernetes Secret is created, providing the two just-created encryption keys.</p> Create a Kubernetes Secret Resource<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-encryption-keys\ndata:\n  crypto_key1: YzIzYzllY2I4MWJmYmY1ZDM5ZDA0NThjNWZlNzQwNjY2Y2RjZDViNWE4NTZkOTA5YmRmODFjM2UxM2FkZGU4Ngo=\n  crypto_key2: ZmFhMGFlMzZkNmIyODdhMjYxMzZhYWI3ZTcwZDEwZjBmYWJlMzYzMDRjNTBjYTY5Nzk2ZGRlZGJiMDMwMGJmNwo=\n</code></pre> <p>The Kubernetes Secret can be used for one or more logical volumes. Using different encryption keys, multiple tenants can be secured with an additional isolation layer against each other.</p>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/#storageclass-configuration","title":"StorageClass Configuration","text":"<p>A new Kubernetes StorageClass needs to be created, or an existing one needs to be configured. To use encryption on a persistent volume claim level, the storage class has to be set for encryption.</p> Example StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\" # This is important!\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/#create-a-persistentvolumeclaim","title":"Create a PersistentVolumeClaim","text":"<p>When requesting a logical volume through a Kubernetes PersistentVolumeClaim, the storage class and the secret resources have to be connected to the PVC. When picked up, simplyblock will automatically collect the keys and create the logical volumes as a fully encrypted logical volume.</p> Create an encrypting PersistentVolumeClaim<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  annotations:\n    simplybk/secret-name: my-encryption-keys # Encryption keys\n  name: my-encrypted-volume-claim\nspec:\n  storageClassName: my-encrypted-volumes # StorageClass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 200Gi\n</code></pre>"},{"location":"maintenance-operations/security/multi-tenancy/","title":"Multi-Tenancy","text":"<p>Simplyblock is designed to support secure and efficient multitenancy, enabling multiple independent tenants to share the same physical infrastructure without compromising data isolation, performance guarantees, or security. This capability is essential in cloud environments, managed services, and enterprise deployments where infrastructure is consolidated across internal departments or external customers.</p>"},{"location":"maintenance-operations/security/multi-tenancy/#storage-isolation","title":"Storage Isolation","text":"<p>Simplyblock provides multiple layers of isolation between multiple tenants, depending on requirements and how tenants are defined.</p>"},{"location":"maintenance-operations/security/multi-tenancy/#storage-pool-isolation","title":"Storage Pool Isolation","text":"<p>If tenants are expected to have multiple volumes, defining the overall available storage quota a tenant can access and assign to volumes might be required. Hence, simplyblock enables the creation of a storage pool with a maximum capacity per tenant. All volumes for this tenant should be created in their respective storage pool and automatically count towards the storage quota.</p>"},{"location":"maintenance-operations/security/multi-tenancy/#logical-volume-isolation","title":"Logical Volume Isolation","text":"<p>If a tenant is expected to have only one volume or strong isolation between volumes is required, each logical volume can be seen as fully isolated at the storage layer. Access to volumes is tightly controlled, and each LV is only exposed to the workloads explicitly granted access.</p>"},{"location":"maintenance-operations/security/multi-tenancy/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<p>To prevent noisy neighbor effects and ensure fair resource allocation, simplyblock supports per-volume Quality of Service (QoS) configurations. Administrators can define IOPS and bandwidth limits for each logical volume, providing predictable performance and protecting tenants from resource contention.</p> <p>Quality of service is available for Kubernetes-based installation quality of service and plain Linux installation quality of service.</p>"},{"location":"maintenance-operations/security/multi-tenancy/#encryption-and-data-security","title":"Encryption and Data Security","text":"<p>All data is protected with encryption at rest, using strong AES-based cryptographic algorithms. Encryption is applied at the volume level, ensuring that tenant data remains secure and inaccessible to other users, even at the physical storage layer. Encryption keys are logically separated between tenants to support strong cryptographic isolation.</p> <p>Encryption is available for Kubernetes-based installation encryption and plain Linux installation encryption.</p>"},{"location":"reference/","title":"Reference","text":"<p>Simplyblock provides multiple interfaces for managing and interacting with its distributed storage system, including the <code>sbctl</code> command-line interface (CLI) and Management API. The <code>sbctl</code> CLI offers a powerful, scriptable way to perform essential operations such as provisioning, expanding, snapshotting, and cloning logical volumes, making it ideal for administrators who prefer direct command-line access.</p> <p>The simplyblock Management API enables integration with external automation and orchestration tools, allowing seamless management of storage resources at scale. Additionally, this section includes a reference list of supported Linux kernels and distributions, ensuring compatibility across various environments.</p>"},{"location":"reference/cpumask-calculator/","title":"CPU Mask Calculator","text":"CPU Mask Calculator Number of virtual cores<sup>*</sup>: Calculated CPU Mask: 0x0000 <p>* Virtual cores include physical and hyper-treading cores.</p>"},{"location":"reference/nvme-low-level-format/","title":"NVMe Low-Level Format","text":"<p>NVMe devices store data in configurable-sized blocks. Simplyblock expects NVMe devices to provide 4 KB block internal block size. Hence, to prevent data loss in case of a sudden power outage, NVMe devices must be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> CLI can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends asking for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> CLI is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executed similarly to the example below.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre> <p>Warning</p> <p>This operation needs to be repeated for each NVMe device that will be handled by simplyblock.</p>"},{"location":"reference/supported-linux-distributions/","title":"Supported Linux Distributions","text":"<p>Simplyblock requires a Linux Kernel 5.19 or later with NVMe over Fabrics and NVMe over TCP enabled. However, <code>sbctl</code>, the simplyblock commandline interface, requires some additional tools and expects certain conventions for configuration files and locations. Therefore, simplyblock officially only supports Red Hat-based Linux distributions as of now.</p> <p>While others may work, manual intervention may be required, and simplyblock cannot support those.</p>"},{"location":"reference/supported-linux-distributions/#control-plane","title":"Control Plane","text":"<p>The following Linux distributions are considered tested and supported to run a control plane:</p> Distribution Version Architecture Support Level Red Hat Enterprise Linux 9 and later x64 Fully supported Rocky Linux 9 and later x64 Fully supported AlmaLinux 9 and later x64 Fully supported"},{"location":"reference/supported-linux-distributions/#disaggregated-storage-plane","title":"Disaggregated Storage Plane","text":"<p>The following Linux distributions are considered tested and supported to run a disaggregated storage plane:</p> Distribution Version Architecture Support Level Red Hat Enterprise Linux 9 and later x64, arm64 Fully supported Rocky Linux 9 and later x64, arm64 Fully supported AlmaLinux 9 and later x64, arm64 Fully supported"},{"location":"reference/supported-linux-distributions/#hyper-converged-storage-plane","title":"Hyper-Converged Storage Plane","text":"<p>The following Linux distributions are considered tested and supported to run a hyper-converged storage plane:</p> Distribution Version Architecture Support Level Red Hat Enterprise Linux 8.1 and later x64, arm64 Fully supported CentOS 8 and later x64, arm64 Fully supported Rocky Linux 9 and later x64, arm64 Fully supported AlmaLinux 9 and later x64, arm64 Fully supported Ubuntu 18.04 x64, arm64 Fully supported Ubuntu 20.04 x64, arm64 Fully supported Ubuntu 22.04 x64, arm64 Fully supported Debian 12 or later x64, arm64 Fully supported Amazon Linux 2 (AL2) - x64, arm64 Fully supported Amazon Linux 2023 - x64, arm64 Fully supported"},{"location":"reference/supported-linux-distributions/#storage-clients","title":"Storage Clients","text":"<p>The following Linux distributions are considered tested and supported as NVMe-oF storage clients:</p> Distribution Version Architecture Support Level Red Hat Enterprise Linux 8.1 and later x64, arm64 Fully supported CentOS 8 and later x64, arm64 Fully supported Rocky Linux 9 and later x64, arm64 Fully supported AlmaLinux 9 and later x64, arm64 Fully supported Ubuntu 18.04 x64, arm64 Fully supported Ubuntu 20.04 x64, arm64 Fully supported Ubuntu 22.04 x64, arm64 Fully supported Debian 12 or later x64, arm64 Fully supported Amazon Linux 2 (AL2) - x64, arm64 Partially supported<sup>1</sup> Amazon Linux 2023 - x64, arm64 Partially supported<sup>1</sup> <p><sup>1</sup> Amazon Linux 2 and Amazon Linux 2023 have a bug with NVMe over Fabrics Multipathing. That means that NVMe over Fabrics on any Amazon Linux operates in a degraded state with the risk of connection outages. Alternatively, multipathing must be configured using the Linux Device Manager (dm) via DM-MPIO. </p>"},{"location":"reference/supported-linux-kernels/","title":"Supported Linux Kernels","text":"<p>Simplyblock is built upon NVMe over Fabrics. Hence, it requires a Linux kernel with NVMe and NVMe-oF support.</p> <p>As a general rule, every Linux kernel 5.19 or later is expected to work, as long as the kernel modules for NVMe (nvme), NVMe over Fabrics (nvme-of), and NVMe over TCP (nvme-tcp) are available. In most cases, the latter two kernel modules need to be loaded manually or persisted. Please see the Bare Metal or Virtualized (Linux) installation section on how to do this.</p> <p>The following kernels are known to be compatible and tested. Additional kernel versions may work, but are untested.</p> OS Linux Kernel Prerequisite Red Hat Enterprise Linux 4.18.0-xxx Kernel on x86_64 modprobe nvme-tcp Amazon Linux 2 Kernel 5.10 AMI 2.0.20230822.0 modprobe nvme-tcp Amazon Linux 2023 2023.1.20230825.0 x86_64 HVM kernel-6.1 modprobe nvme-tcp <p>Warning</p> <p>Amazon Linux 2 and Amazon Linux 2023 have a bug with NVMe over Fabrics Multipathing. That means that NVMe over Fabrics on any Amazon Linux operates in a degraded state with the risk of connection outages. As an alternative, multipathing must be configured using the Linux Device Manager (dm) via DM-MPIO. Use the following DM-MPIO configuration:</p> <pre><code>cat /etc/multipath.conf \ndefaults {\n    polling_interval 1\n    user_friendly_names yes\n    find_multipaths yes\n    enable_foreign nvme\n    checker_timeout 3\n    failback immediate\n    max_polling_interval 3\n    detect_checker yes\n}\n\ndevices {\n    device {\n        vendor \"NVMe\"\n        product \".*\"\n        path_grouping_policy group_by_prio\n        path_selector \"service-time 0\"\n        failback \"immediate\"\n        no_path_retry \"queue\"\n        hardware_handler \"1 ana\"\n    }\n}\n\nblacklist {\n}\n</code></pre>"},{"location":"reference/upgrade-matrix/","title":"Upgrade Matrix","text":"<p>Simplyblock supports in-place upgrades of existing clusters. However, not all versions can be upgraded straight to the latest versions. Hence, some upgrades may include multiple steps.</p> <p>Possible upgrade paths are described in the following table. If the currently installed version is not listed on the requested version, an upgrade to a further supported version must be executed first.</p> Requested Version Installed Version 25.5.x 25.5.x, 25.3-PRE"},{"location":"reference/api/","title":"API / Developer SDK","text":"<p>Simplyblock offers a comprehensive API to manage and automate cluster operations. This includes all cluster-wide operations, logical volume-specific operations, health information, and </p> <ul> <li>Retrieve information about the cluster and its health status</li> <li>Automatically manage a logical volume lifecycle</li> <li>Integrate simplyblock into deployment processes and workflow automations</li> <li>Create custom alerts and warnings</li> </ul>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>Any request to the simplyblock API requires authorization information to be provided. Unauthorized requests return an HTTP status 401 (Unauthorized).</p> <p>To provide authorization information, the simplyblock API uses the Authorization HTTP header with a combination of the cluster UUID and the cluster secret.</p> <p>HTTP Authorization header:</p> <pre><code>Authorization: &lt;CLUSTER_UUID&gt; &lt;CLUSTER_SECRET&gt;\n</code></pre> <p>The cluster id is provided during the initial cluster installation. The cluster secret can be obtained using the simplyblock commandline interface tool <code>sbctl</code>.</p> <pre><code>sbctl cluster get-secret CLUSTER_UUID\n</code></pre>"},{"location":"reference/api/#put-and-post-requests","title":"PUT and POST Requests","text":"<p>For requests that send a JSON payload to the backend endpoint, it is important to set the Content-Type header accordingly. Requests that require this header to be set are of type HTTP PUT or HTTP POST.</p> <p>The expected content type is <code>application/json</code>:</p> <pre><code>Content-Type: application/json\n</code></pre>"},{"location":"reference/api/#api-documentation","title":"API Documentation","text":"<p>The full API documentation is hosted on Postman. You can find the full API collection on the Postman API project\u00a0\u29c9.</p>"},{"location":"reference/api/reference/","title":"API Reference","text":""},{"location":"reference/cli/","title":"CLI / Command-line interface","text":"<p>Simplyblock provides a feature-rich CLI (command line interface) client to manage all aspects of the storage cluster.</p>"},{"location":"reference/cli/caching-node/","title":"Caching node commands","text":"<pre><code>sbctl caching-node --help\n</code></pre> <p>Aliases:  cn </p> <p>Caching node commands</p>"},{"location":"reference/cli/caching-node/#deploys-a-caching-node-on-this-machine-local-run","title":"Deploys a caching node on this machine (local run)","text":"<p>Deploys a caching node on this machine (local run)</p> <pre><code>sbctl caching-node deploy\n    --ifname=&lt;IFNAME&gt;\n</code></pre> Parameter Description Data Type Required Default --ifname Management interface name, e.g. eth0 string False -"},{"location":"reference/cli/caching-node/#adds-a-new-caching-node-to-the-cluster","title":"Adds a new caching node to the cluster","text":"<p>Adds a new caching node to the cluster</p> <pre><code>sbctl caching-node add-node\n    &lt;CLUSTER_ID&gt;\n    &lt;NODE_IP&gt;\n    &lt;IFNAME&gt;\n    --vcpu-count=&lt;VCPU_COUNT&gt;\n    --namespace=&lt;NAMESPACE&gt;\n    --multipathing=&lt;MULTIPATHING&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True NODE_IP Node IP address string True IFNAME Management interface name string True Parameter Description Data Type Required Default --vcpu-count Number of vCPUs used for SPDK. Remaining CPUs will be used for Linux system, TCP/IP processing, and other workloads. The default on non-Kubernetes hosts is 80%. integer False - --namespace k8s namespace to deploy on string False - --multipathing Enable multipathing for logical volume connection, default: onAvailable Options:- on- off string False True"},{"location":"reference/cli/caching-node/#lists-all-caching-nodes","title":"Lists all caching nodes","text":"<p>Lists all caching nodes</p> <pre><code>sbctl caching-node list\n</code></pre>"},{"location":"reference/cli/caching-node/#lists-all-connected-logical-volumes","title":"Lists all connected logical volumes","text":"<p>Lists all connected logical volumes</p> <pre><code>sbctl caching-node list-lvols\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Caching node id string True"},{"location":"reference/cli/caching-node/#removes-a-caching-node-from-the-cluster","title":"Removes a caching node from the cluster","text":"<p>Removes a caching node from the cluster</p> <pre><code>sbctl caching-node remove\n    &lt;NODE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required NODE_ID Caching node id string True Parameter Description Data Type Required Default --force Force remove marker False -"},{"location":"reference/cli/caching-node/#connects-a-logical-volume-to-the-caching-node","title":"Connects a logical volume to the caching node","text":"<p>Connects a logical volume to the caching node</p> <pre><code>sbctl caching-node connect\n    &lt;NODE_ID&gt;\n    &lt;LVOL_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Caching node id string True LVOL_ID Logical volume id string True"},{"location":"reference/cli/caching-node/#disconnects-a-logical-volume-from-the-caching-node","title":"Disconnects a logical volume from the caching node","text":"<p>Disconnects a logical volume from the caching node</p> <pre><code>sbctl caching-node disconnect\n    &lt;NODE_ID&gt;\n    &lt;LVOL_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Caching node id string True LVOL_ID Logical volume id string True"},{"location":"reference/cli/caching-node/#recreate-a-caching-nodes-bdevs","title":"Recreate a caching node's bdevs","text":"<p>Recreate a caching node's bdevs</p> <pre><code>sbctl caching-node recreate\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Caching node id string True"},{"location":"reference/cli/caching-node/#gets-a-logical-volumes-statistics","title":"Gets a logical volume's statistics","text":"<p>Gets a logical volume's statistics</p> <pre><code>sbctl caching-node get-lvol-stats\n    &lt;LVOL_ID&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required LVOL_ID Logical volume id string True Parameter Description Data Type Required Default --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False -"},{"location":"reference/cli/cluster/","title":"Cluster commands","text":"<pre><code>sbctl cluster --help\n</code></pre> <p>Cluster commands</p>"},{"location":"reference/cli/cluster/#deploys-a-storage-nodes","title":"Deploys a storage nodes","text":"<p>Deploys a storage nodes</p> <pre><code>sbctl cluster deploy\n    --storage-nodes=&lt;STORAGE_NODES&gt;\n    --test\n    --ha-type=&lt;HA_TYPE&gt;\n    --data-chunks-per-stripe=&lt;DATA_CHUNKS_PER_STRIPE&gt;\n    --parity-chunks-per-stripe=&lt;PARITY_CHUNKS_PER_STRIPE&gt;\n    --ifname=&lt;IFNAME&gt;\n    --cap-warn=&lt;CAP_WARN&gt;\n    --cap-crit=&lt;CAP_CRIT&gt;\n    --prov-cap-warn=&lt;PROV_CAP_WARN&gt;\n    --prov-cap-crit=&lt;PROV_CAP_CRIT&gt;\n    --log-del-interval=&lt;LOG_DEL_INTERVAL&gt;\n    --metrics-retention-period=&lt;METRICS_RETENTION_PERIOD&gt;\n    --contact-point=&lt;CONTACT_POINT&gt;\n    --chunk-size-in-bytes=&lt;CHUNK_SIZE_IN_BYTES&gt;\n    --enable-node-affinity\n    --qpair-count=&lt;QPAIR_COUNT&gt;\n    --strict-node-anti-affinity\n    --journal-partition=&lt;JOURNAL_PARTITION&gt;\n    --data-nics=&lt;DATA_NICS&gt;\n    --max-lvol=&lt;MAX_LVOL&gt;\n    --max-size=&lt;MAX_SIZE&gt;\n    --namespace=&lt;NAMESPACE&gt;\n    --id-device-by-nqn\n    --host-id=&lt;HOST_ID&gt;\n</code></pre> Parameter Description Data Type Required Default --storage-nodes comma separated ip addresses string False - --test Test Cluster marker False - --ha-type Logical volume HA type (single, ha), default is cluster HA typeAvailable Options:- single- ha string False ha --data-chunks-per-stripe Erasure coding schema parameter k (distributed raid), default: 1 integer False 1 --parity-chunks-per-stripe Erasure coding schema parameter n (distributed raid), default: 1 integer False 1 --ifname Management interface name, e.g. eth0 string False - --cap-warn Capacity warning level in percent, default: 89 integer False 89 --cap-crit Capacity critical level in percent, default: 99 integer False 99 --prov-cap-warn Capacity warning level in percent, default: 250 integer False 250 --prov-cap-crit Capacity critical level in percent, default: 500 integer False 500 --log-del-interval Logging retention period, default: 3d string False 3d --metrics-retention-period Retention period for I/O statistics (Prometheus), default: 7d string False 7d --contact-point Email or slack webhook url to be used for alerting string False --chunk-size-in-bytes (Dev) distrb bdev chunk block size, default: 4096 integer False 4096 --enable-node-affinity Enable node affinity for storage nodes marker False - --qpair-count Increase for clusters with few but very large logical volumes or decrease for clusters with a large number of very small logical volumes. unknown False 3 --strict-node-anti-affinity Enable strict node anti affinity for storage nodes. Never more than one chunk is placed on a node. This requires a minimum of data-chunks-in-stripe + parity-chunks-in-stripe + 1 nodes in the cluster.\" marker False - --journal-partition 1: auto-partition nvme devices for journal. 0: use a separate nvme device for journal. The smallest NVMe device available on the host will be chosen as a journal. It should provide about 3% of the entire node\u2019s NVMe capacity. If set to false, partitions on other devices will be auto-created to store the journal. string False True --data-nics Storage network interface name(s). Can be more than one. Comma-separated list: e.g. eth0,eth1 string False - --max-lvol Max logical volume per storage node integer False - --max-size Maximum amount of GB to be provisioned via all storage nodes string False --namespace k8s namespace to deploy on string False - --id-device-by-nqn Use device nqn to identify it instead of serial number marker False False --host-id Primary storage node id or hostname string False -"},{"location":"reference/cli/cluster/#creates-a-new-cluster","title":"Creates a new cluster","text":"<p>Created a new control plane cluster with the current node as the primary control plane node.</p> <pre><code>sbctl cluster create\n    --cap-warn=&lt;CAP_WARN&gt;\n    --cap-crit=&lt;CAP_CRIT&gt;\n    --prov-cap-warn=&lt;PROV_CAP_WARN&gt;\n    --prov-cap-crit=&lt;PROV_CAP_CRIT&gt;\n    --ifname=&lt;IFNAME&gt;\n    --log-del-interval=&lt;LOG_DEL_INTERVAL&gt;\n    --metrics-retention-period=&lt;METRICS_RETENTION_PERIOD&gt;\n    --contact-point=&lt;CONTACT_POINT&gt;\n    --grafana-endpoint=&lt;GRAFANA_ENDPOINT&gt;\n    --data-chunks-per-stripe=&lt;DATA_CHUNKS_PER_STRIPE&gt;\n    --parity-chunks-per-stripe=&lt;PARITY_CHUNKS_PER_STRIPE&gt;\n    --ha-type=&lt;HA_TYPE&gt;\n    --enable-node-affinity\n    --qpair-count=&lt;QPAIR_COUNT&gt;\n    --strict-node-anti-affinity\n</code></pre> Parameter Description Data Type Required Default --cap-warn Capacity warning level in percent, default: 89 integer False 89 --cap-crit Capacity critical level in percent, default: 99 integer False 99 --prov-cap-warn Capacity warning level in percent, default: 250 integer False 250 --prov-cap-crit Capacity critical level in percent, default: 500 integer False 500 --ifname Management interface name, e.g. eth0 string False - --log-del-interval Logging retention policy, default: 3d string False 3d --metrics-retention-period Retention period for I/O statistics (Prometheus), default: 7d string False 7d --contact-point Email or slack webhook url to be used for alerting string False --grafana-endpoint Endpoint url for Grafana string False --data-chunks-per-stripe Erasure coding schema parameter k (distributed raid), default: 1 integer False 1 --parity-chunks-per-stripe Erasure coding schema parameter n (distributed raid), default: 1 integer False 1 --ha-type Logical volume HA type (single, ha), default is cluster ha typeAvailable Options:- single- ha string False ha --enable-node-affinity Enable node affinity for storage nodes marker False - --qpair-count Increase for clusters with few but very large logical volumes or decrease for clusters with a large number of very small logical volumes. unknown False 0 --strict-node-anti-affinity Enable strict node anti affinity for storage nodes. Never more than one chunk is placed on a node. This requires a minimum of data-chunks-in-stripe + parity-chunks-in-stripe + 1 nodes in the cluster. marker False -"},{"location":"reference/cli/cluster/#adds-a-new-cluster","title":"Adds a new cluster","text":"<p>Adds a new cluster</p> <pre><code>sbctl cluster add\n    --cap-warn=&lt;CAP_WARN&gt;\n    --cap-crit=&lt;CAP_CRIT&gt;\n    --prov-cap-warn=&lt;PROV_CAP_WARN&gt;\n    --prov-cap-crit=&lt;PROV_CAP_CRIT&gt;\n    --data-chunks-per-stripe=&lt;DATA_CHUNKS_PER_STRIPE&gt;\n    --parity-chunks-per-stripe=&lt;PARITY_CHUNKS_PER_STRIPE&gt;\n    --ha-type=&lt;HA_TYPE&gt;\n    --enable-node-affinity\n    --qpair-count=&lt;QPAIR_COUNT&gt;\n    --strict-node-anti-affinity\n</code></pre> Parameter Description Data Type Required Default --cap-warn Capacity warning level in percent, default: 89 integer False 89 --cap-crit Capacity critical level in percent, default: 99 integer False 99 --prov-cap-warn Capacity warning level in percent, default: 250 integer False 250 --prov-cap-crit Capacity critical level in percent, default: 500 integer False 500 --data-chunks-per-stripe Erasure coding schema parameter k (distributed raid), default: 1 integer False 1 --parity-chunks-per-stripe Erasure coding schema parameter n (distributed raid), default: 1 integer False 1 --ha-type Logical volume HA type (single, ha), default is cluster single typeAvailable Options:- single- ha string False ha --enable-node-affinity Enables node affinity for storage nodes marker False - --qpair-count Increase for clusters with few but very large logical volumes or decrease for clusters with a large number of very small logical volumes. unknown False 0 --strict-node-anti-affinity Enable strict node anti affinity for storage nodes. Never more than one chunk is placed on a node. This requires a minimum of data-chunks-in-stripe + parity-chunks-in-stripe + 1 nodes in the cluster.\" marker False -"},{"location":"reference/cli/cluster/#activates-a-cluster","title":"Activates a cluster.","text":"<p>Once a cluster has sufficient nodes added, it needs to be activated. Can also be used to re-activate a suspended cluster.</p> <pre><code>sbctl cluster activate\n    &lt;CLUSTER_ID&gt;\n    --force\n    --force-lvstore-create\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --force Force recreate distr and lv stores marker False - --force-lvstore-create Force recreate lv stores marker False -"},{"location":"reference/cli/cluster/#shows-the-cluster-list","title":"Shows the cluster list","text":"<p>Shows the cluster list</p> <pre><code>sbctl cluster list\n    --json\n</code></pre> Parameter Description Data Type Required Default --json Print json output marker False -"},{"location":"reference/cli/cluster/#shows-a-clusters-status","title":"Shows a cluster's status","text":"<p>Shows a cluster's status</p> <pre><code>sbctl cluster status\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#create-lvstore-on-newly-added-nodes-to-the-cluster","title":"Create lvstore on newly added nodes to the cluster","text":"<p>Create lvstore on newly added nodes to the cluster</p> <pre><code>sbctl cluster complete-expand\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#shows-a-clusters-statistics","title":"Shows a cluster's statistics","text":"<p>Shows a cluster's statistics</p> <pre><code>sbctl cluster show\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#gets-a-clusters-information","title":"Gets a cluster's information","text":"<p>Gets a cluster's information</p> <pre><code>sbctl cluster get\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#gets-a-clusters-capacity","title":"Gets a cluster's capacity","text":"<p>Gets a cluster's capacity</p> <pre><code>sbctl cluster get-capacity\n    &lt;CLUSTER_ID&gt;\n    --json\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --json Print json output marker False - --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False -"},{"location":"reference/cli/cluster/#gets-a-clusters-io-statistics","title":"Gets a cluster's I/O statistics","text":"<p>Gets a cluster's I/O statistics</p> <pre><code>sbctl cluster get-io-stats\n    &lt;CLUSTER_ID&gt;\n    --records=&lt;RECORDS&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --records Number of records, default: 20 integer False 20 --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False -"},{"location":"reference/cli/cluster/#returns-a-clusters-status-logs","title":"Returns a cluster's status logs","text":"<p>Returns a cluster's status logs</p> <pre><code>sbctl cluster get-logs\n    &lt;CLUSTER_ID&gt;\n    --json\n    --limit=&lt;LIMIT&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --json Return JSON formatted logs marker False - --limit show last number of logs, default 50 integer False 50"},{"location":"reference/cli/cluster/#gets-a-clusters-secret","title":"Gets a cluster's secret","text":"<p>Gets a cluster's secret</p> <pre><code>sbctl cluster get-secret\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#updates-a-clusters-secret","title":"Updates a cluster's secret","text":"<p>Updates a cluster's secret</p> <pre><code>sbctl cluster update-secret\n    &lt;CLUSTER_ID&gt;\n    &lt;SECRET&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True SECRET new 20 characters password string True"},{"location":"reference/cli/cluster/#checks-a-clusters-health","title":"Checks a cluster's health","text":"<p>Checks a cluster's health</p> <pre><code>sbctl cluster check\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/cluster/#updates-a-cluster-to-new-version","title":"Updates a cluster to new version","text":"<p>Updates a the control plane to a new version. To update the storage nodes, they have to be shutdown and restarted. This can be done in a rolling manner. Attention: verify that an upgrade path is available and has been tested!\"</p> <pre><code>sbctl cluster update\n    &lt;CLUSTER_ID&gt;\n    --cp-only=&lt;CP_ONLY&gt;\n    --restart=&lt;RESTART&gt;\n    --spdk-image=&lt;SPDK_IMAGE&gt;\n    --mgmt-image=&lt;MGMT_IMAGE&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --cp-only Update the control plane only boolean False False --restart Restart the management services boolean False False --spdk-image Restart the storage nodes using the provided image string False - --mgmt-image Restart the management services using the provided image string False -"},{"location":"reference/cli/cluster/#lists-tasks-of-a-cluster","title":"Lists tasks of a cluster","text":"<p>Lists tasks of a cluster</p> <pre><code>sbctl cluster list-tasks\n    &lt;CLUSTER_ID&gt;\n    --limit=&lt;LIMIT&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --limit show last number of tasks, default 50 integer False 50"},{"location":"reference/cli/cluster/#cancels-task-by-task-id","title":"Cancels task by task id","text":"<p>Cancels task by task id</p> <pre><code>sbctl cluster cancel-task\n    &lt;TASK_ID&gt;\n</code></pre> Argument Description Data Type Required TASK_ID Task id string True"},{"location":"reference/cli/cluster/#deletes-a-cluster","title":"Deletes a cluster","text":"<p>This is only possible, if no storage nodes and pools are attached to the cluster</p> <pre><code>sbctl cluster delete\n    &lt;CLUSTER_ID&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True"},{"location":"reference/cli/control-plane/","title":"Control plane commands","text":"<pre><code>sbctl control-plane --help\n</code></pre> <p>Aliases:  cp  mgmt </p> <p>Control plane commands</p>"},{"location":"reference/cli/control-plane/#adds-a-control-plane-to-the-cluster-local-run","title":"Adds a control plane to the cluster (local run)","text":"<p>Adds a control plane to the cluster (local run)</p> <pre><code>sbctl control-plane add\n    &lt;CLUSTER_IP&gt;\n    &lt;CLUSTER_ID&gt;\n    &lt;CLUSTER_SECRET&gt;\n    &lt;IFNAME&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_IP Cluster IP address string True CLUSTER_ID Cluster id string True CLUSTER_SECRET Cluster secret string True IFNAME Management interface name string True"},{"location":"reference/cli/control-plane/#lists-all-control-plane-nodes","title":"Lists all control plane nodes","text":"<p>Lists all control plane nodes</p> <pre><code>sbctl control-plane list\n    --json\n</code></pre> Parameter Description Data Type Required Default --json Print outputs in json format marker False -"},{"location":"reference/cli/control-plane/#removes-a-control-plane-node","title":"Removes a control plane node","text":"<p>Removes a control plane node</p> <pre><code>sbctl control-plane remove\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Control plane node id string True"},{"location":"reference/cli/snapshot/","title":"Snapshot commands","text":"<pre><code>sbctl snapshot --help\n</code></pre> <p>Snapshot commands</p>"},{"location":"reference/cli/snapshot/#creates-a-new-snapshot","title":"Creates a new snapshot","text":"<p>Creates a new snapshot</p> <pre><code>sbctl snapshot add\n    &lt;VOLUME_ID&gt;\n    &lt;NAME&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True NAME New snapshot name string True"},{"location":"reference/cli/snapshot/#lists-all-snapshots","title":"Lists all snapshots","text":"<p>Lists all snapshots</p> <pre><code>sbctl snapshot list\n    --all\n</code></pre> Parameter Description Data Type Required Default --all List soft deleted snapshots marker False -"},{"location":"reference/cli/snapshot/#deletes-a-snapshot","title":"Deletes a snapshot","text":"<p>Deletes a snapshot</p> <pre><code>sbctl snapshot delete\n    &lt;SNAPSHOT_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required SNAPSHOT_ID Snapshot id string True Parameter Description Data Type Required Default --force Force remove marker False -"},{"location":"reference/cli/snapshot/#provisions-a-new-logical-volume-from-an-existing-snapshot","title":"Provisions a new logical volume from an existing snapshot","text":"<p>Provisions a new logical volume from an existing snapshot</p> <pre><code>sbctl snapshot clone\n    &lt;SNAPSHOT_ID&gt;\n    &lt;LVOL_NAME&gt;\n    --resize=&lt;RESIZE&gt;\n</code></pre> Argument Description Data Type Required SNAPSHOT_ID Snapshot id string True LVOL_NAME Logical volume name string True Parameter Description Data Type Required Default --resize New logical volume size: 10M, 10G, 10(bytes). Can only increase. unknown False 0"},{"location":"reference/cli/storage-node/","title":"Storage node commands","text":"<pre><code>sbctl storage-node --help\n</code></pre> <p>Aliases:  sn </p> <p>Storage node commands</p>"},{"location":"reference/cli/storage-node/#prepares-a-host-to-be-used-as-a-storage-node","title":"Prepares a host to be used as a storage node","text":"<p>Runs locally on to-be storage node hosts. Installs storage node dependencies and prepares it to be used as a storage node. Only required, in standalone deployment outside of Kubernetes.</p> <pre><code>sbctl storage-node deploy\n    --ifname=&lt;IFNAME&gt;\n    --isolate-cores\n</code></pre> Parameter Description Data Type Required Default --ifname The network interface to be used for communication between the control plane and the storage node. string False - --isolate-cores Isolate cores in kernel args for provided cpu mask marker False False"},{"location":"reference/cli/storage-node/#prepare-a-configuration-file-to-be-used-when-adding-the-storage-node","title":"Prepare a configuration file to be used when adding the storage node","text":"<p>Runs locally on to-be storage node hosts. Reads system information (CPUs topology, NVME devices) and prepares yaml config to be used when adding the storage node.</p> <pre><code>sbctl storage-node configure\n    --max-lvol=&lt;MAX_LVOL&gt;\n    --max-size=&lt;MAX_SIZE&gt;\n    --nodes-per-socket=&lt;NODES_PER_SOCKET&gt;\n    --sockets-to-use=&lt;SOCKETS_TO_USE&gt;\n    --pci-allowed=&lt;PCI_ALLOWED&gt;\n    --pci-blocked=&lt;PCI_BLOCKED&gt;\n</code></pre> Parameter Description Data Type Required Default --max-lvol Max logical volume per storage node integer True - --max-size Maximum amount of GB to be utilized on this storage node. This cannot be larger than the total effective cluster capacity. A safe value is <code>1/n * 2.0</code> of effective cluster capacity. Meaning, if you have three storage nodes, each with 100 TiB of raw capacity and a cluster with erasure coding scheme 1+1 (two replicas), the effective cluster capacity is 100 TiB * 3 / 2 = 150 TiB. Setting this parameter to 150 TiB / 3 * 2 = 100TiB would be a safe choice. string True - --nodes-per-socket number of each node to be added per each socket. integer False 1 --sockets-to-use System socket to use when adding storage nodes. Comma-separated list: e.g. 0,1 string False 0 --pci-allowed Storage PCI addresses to use for storage devices(Normal address and full address are accepted). Comma-separated list: e.g. 0000:00:01.0,00:02.0 string False --pci-blocked Storage PCI addresses to not use for storage devices(Normal address and full address are accepted). Comma-separated list: e.g. 0000:00:01.0,00:02.0 string False"},{"location":"reference/cli/storage-node/#upgrade-the-automated-configuration-file-with-new-changes-of-cpu-mask-or-storage-devices","title":"Upgrade the automated configuration file with new changes of cpu mask or storage devices","text":"<p>Regenerate the core distribution and auto calculation according to changes in cpu_mask and ssd_pcis only</p> <pre><code>sbctl storage-node configure-upgrade\n</code></pre>"},{"location":"reference/cli/storage-node/#cleans-a-previous-simplyblock-deploy-local-run","title":"Cleans a previous simplyblock deploy (local run)","text":"<p>Run locally on storage nodes and control plane hosts. Remove a previous deployment to support a fresh scratch-deployment of cluster software.</p> <pre><code>sbctl storage-node deploy-cleaner\n</code></pre>"},{"location":"reference/cli/storage-node/#adds-a-storage-node-by-its-ip-address","title":"Adds a storage node by its IP address","text":"<p>Adds a storage node by its IP address</p> <pre><code>sbctl storage-node add-node\n    &lt;CLUSTER_ID&gt;\n    &lt;NODE_ADDR&gt;\n    &lt;IFNAME&gt;\n    --journal-partition=&lt;JOURNAL_PARTITION&gt;\n    --data-nics=&lt;DATA_NICS&gt;\n    --ha-jm-count=&lt;HA_JM_COUNT&gt;\n    --namespace=&lt;NAMESPACE&gt;\n</code></pre> Argument Description Data Type Required CLUSTER_ID Cluster id string True NODE_ADDR Address of storage node api to add, like :5000 string True IFNAME Management interface name string True Parameter Description Data Type Required Default --journal-partition 1: auto-create small partitions for journal on nvme devices. 0: use a separate (the smallest) nvme device of the node for journal. The journal needs a maximum of 3 percent of total available raw disk space. integer False 1 --data-nics Storage network interface name(s). Can be more than one. Comma-separated list: e.g. eth0,eth1 string False - --ha-jm-count HA JM count integer False 3 --namespace Kubernetes namespace to deploy on string False -"},{"location":"reference/cli/storage-node/#deletes-a-storage-node-object-from-the-state-database","title":"Deletes a storage node object from the state database.","text":"<p>Deletes a storage node object from the state database. It must only be used on clusters without any logical volumes. Warning: This is dangerous and could lead to unstable cluster if used on active cluster.</p> <pre><code>sbctl storage-node delete\n    &lt;NODE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --force Force delete storage node from DB...Hopefully you know what you do marker False -"},{"location":"reference/cli/storage-node/#removes-a-storage-node-from-the-cluster","title":"Removes a storage node from the cluster","text":"<p>The storage node cannot be used or added any more. Any data residing on this storage node will be migrated to the remaining storage nodes. The user must ensure that there is sufficient free space in remaining cluster to allow for successful node removal.</p> <p>Danger</p> <p>If there isn't enough storage available, the cluster may run full and switch to read-only mode.</p> <pre><code>sbctl storage-node remove\n    &lt;NODE_ID&gt;\n    --force-remove\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --force-remove Force remove all logical volumes and snapshots marker False -"},{"location":"reference/cli/storage-node/#lists-all-storage-nodes","title":"Lists all storage nodes","text":"<p>Lists all storage nodes</p> <pre><code>sbctl storage-node list\n    --cluster-id=&lt;CLUSTER_ID&gt;\n    --json\n</code></pre> Parameter Description Data Type Required Default --cluster-id Cluster id string False - --json Print outputs in json format marker False -"},{"location":"reference/cli/storage-node/#gets-a-storage-nodes-information","title":"Gets a storage node's information","text":"<p>Gets a storage node's information</p> <pre><code>sbctl storage-node get\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-node/#restarts-a-storage-node","title":"Restarts a storage node","text":"<p>A storage node is required to be offline to be restarted. All functions and device drivers will be reset as a result of the restart. New physical devices can only be added with a storage node restart. During restart, the node will not accept any I/O.</p> <pre><code>sbctl storage-node restart\n    &lt;NODE_ID&gt;\n    --max-lvol=&lt;MAX_LVOL&gt;\n    --node-addr=&lt;NODE_ADDR&gt;\n    --force\n    --ssd-pcie=&lt;SSD_PCIE&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --max-lvol Max logical volume per storage node integer False 0 --node-addr, --node-ip Allows to restart an existing storage node on new host or hardware. Devices attached to storage nodes have to be attached to new hosts. Otherwise, they have to be marked as failed and removed from cluster. Triggers a pro-active migration of data from those devices onto other storage nodes. The provided value must be presented in the form of IP:PORT. Be default the port number is 5000. string False - --force Force restart marker False - --ssd-pcie New Nvme PCIe address to add to the storage node. Can be more than one. string False"},{"location":"reference/cli/storage-node/#initiates-a-storage-node-shutdown","title":"Initiates a storage node shutdown","text":"<p>Once the command is issued, the node will stop accepting IO,but IO, which was previously received, will still be processed. In a high-availability setup, this will not impact operations.</p> <pre><code>sbctl storage-node shutdown\n    &lt;NODE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --force Force node shutdown marker False -"},{"location":"reference/cli/storage-node/#suspends-a-storage-node","title":"Suspends a storage node","text":"<p>The node will stop accepting new IO, but will finish processing any IO, which has been received already.</p> <pre><code>sbctl storage-node suspend\n    &lt;NODE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --force Force node suspend marker False -"},{"location":"reference/cli/storage-node/#resumes-a-storage-node","title":"Resumes a storage node","text":"<p>Resumes a storage node</p> <pre><code>sbctl storage-node resume\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-node/#gets-storage-node-io-statistics","title":"Gets storage node IO statistics","text":"<p>Gets storage node IO statistics</p> <pre><code>sbctl storage-node get-io-stats\n    &lt;NODE_ID&gt;\n    --history=&lt;HISTORY&gt;\n    --records=&lt;RECORDS&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --history list history records -one for every 15 minutes- for XX days and YY hours -up to 10 days in total-, format: XXdYYh string False - --records Number of records, default: 20 integer False 20"},{"location":"reference/cli/storage-node/#gets-a-storage-nodes-capacity-statistics","title":"Gets a storage node's capacity statistics","text":"<p>Gets a storage node's capacity statistics</p> <pre><code>sbctl storage-node get-capacity\n    &lt;NODE_ID&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --history list history records -one for every 15 minutes- for XX days and YY hours -up to 10 days in total-, format: XXdYYh string False -"},{"location":"reference/cli/storage-node/#lists-storage-devices","title":"Lists storage devices","text":"<p>Lists storage devices</p> <pre><code>sbctl storage-node list-devices\n    &lt;NODE_ID&gt;\n    --json\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True Parameter Description Data Type Required Default --json Print outputs in json format marker False -"},{"location":"reference/cli/storage-node/#gets-storage-device-by-its-id","title":"Gets storage device by its id","text":"<p>Gets storage device by its id</p> <pre><code>sbctl storage-node get-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True"},{"location":"reference/cli/storage-node/#resets-a-storage-device","title":"Resets a storage device","text":"<p>Hardware device reset. Resetting the device can return the device from an unavailable into online state, if successful.</p> <pre><code>sbctl storage-node reset-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True"},{"location":"reference/cli/storage-node/#restarts-a-storage-device","title":"Restarts a storage device","text":"<p>A previously logically or physically removed or unavailable device, which has been re-inserted, may be returned into online state. If the device is not physically present, accessible or healthy, it will flip back into unavailable state again.</p> <pre><code>sbctl storage-node restart-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True"},{"location":"reference/cli/storage-node/#adds-a-new-storage-device","title":"Adds a new storage device","text":"<p>Adds a device, including a previously detected device (currently in \"new\" state) into cluster and launches an auto-rebalancing background process in which some cluster capacity is re-distributed to this newly added device.</p> <pre><code>sbctl storage-node add-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True"},{"location":"reference/cli/storage-node/#logically-removes-a-storage-device","title":"Logically removes a storage device","text":"<p>Logical removes a storage device. The device will become unavailable, irrespectively if it was physically removed from the server. This function can be used if auto-detection of removal did not work or if the device must be maintained while remaining inserted into the server.</p> <pre><code>sbctl storage-node remove-device\n    &lt;DEVICE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True Parameter Description Data Type Required Default --force Force device remove marker False -"},{"location":"reference/cli/storage-node/#sets-storage-device-to-failed-state","title":"Sets storage device to failed state","text":"<p>Sets a storage device to state failed. This command can be used, if an administrator believes that the device must be replaced. Attention: a failed state is final, meaning, all data on the device will be automatically recovered to other devices in the cluster.</p> <pre><code>sbctl storage-node set-failed-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device ID string True"},{"location":"reference/cli/storage-node/#gets-a-devices-capacity","title":"Gets a device's capacity","text":"<p>Gets a device's capacity</p> <pre><code>sbctl storage-node get-capacity-device\n    &lt;DEVICE_ID&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True Parameter Description Data Type Required Default --history list history records -one for every 15 minutes- for XX days and YY hours -up to 10 days in total-, format: XXdYYh string False -"},{"location":"reference/cli/storage-node/#gets-a-devices-io-statistics","title":"Gets a device's IO statistics","text":"<p>Gets a device's IO statistics</p> <pre><code>sbctl storage-node get-io-stats-device\n    &lt;DEVICE_ID&gt;\n    --history=&lt;HISTORY&gt;\n    --records=&lt;RECORDS&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True Parameter Description Data Type Required Default --history list history records -one for every 15 minutes- for XX days and YY hours -up to 10 days in total-, format: XXdYYh string False - --records Number of records, default: 20 integer False 20"},{"location":"reference/cli/storage-node/#gets-the-data-interfaces-list-of-a-storage-node","title":"Gets the data interfaces list of a storage node","text":"<p>Gets the data interfaces list of a storage node</p> <pre><code>sbctl storage-node port-list\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-node/#gets-the-data-interfaces-io-stats","title":"Gets the data interfaces' IO stats","text":"<p>Gets the data interfaces' IO stats</p> <pre><code>sbctl storage-node port-io-stats\n    &lt;PORT_ID&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required PORT_ID Data port id string True Parameter Description Data Type Required Default --history list history records -one for every 15 minutes- for XX days and YY hours -up to 10 days in total, format: XXdYYh string False -"},{"location":"reference/cli/storage-node/#checks-the-health-status-of-a-storage-node","title":"Checks the health status of a storage node","text":"<p>Verifies if all of the NVMe-oF connections to and from the storage node, including those to and from other storage devices in the cluster and the meta-data journal, are available and healthy and all internal objects of the node, such as data placement and erasure coding services, are in a healthy state.</p> <pre><code>sbctl storage-node check\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-node/#checks-the-health-status-of-a-device","title":"Checks the health status of a device","text":"<p>Checks the health status of a device</p> <pre><code>sbctl storage-node check-device\n    &lt;DEVICE_ID&gt;\n</code></pre> Argument Description Data Type Required DEVICE_ID Device id string True"},{"location":"reference/cli/storage-node/#gets-the-nodes-information","title":"Gets the node's information","text":"<p>Gets the node's information</p> <pre><code>sbctl storage-node info\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-node/#restarts-a-journaling-device","title":"Restarts a journaling device","text":"<p>Restarts a journaling device</p> <pre><code>sbctl storage-node restart-jm-device\n    &lt;JM_DEVICE_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required JM_DEVICE_ID Journaling device id string True Parameter Description Data Type Required Default --force Force device remove marker False -"},{"location":"reference/cli/storage-node/#forces-to-make-the-provided-node-id-primary","title":"Forces to make the provided node id primary","text":"<p>Makes the storage node the primary node. This is required after certain storage cluster operations, such as a storage node migration.</p> <pre><code>sbctl storage-node make-primary\n    &lt;NODE_ID&gt;\n</code></pre> Argument Description Data Type Required NODE_ID Storage node id string True"},{"location":"reference/cli/storage-pool/","title":"Storage pool commands","text":"<pre><code>sbctl storage-pool --help\n</code></pre> <p>Aliases:  pool </p> <p>Storage pool commands</p>"},{"location":"reference/cli/storage-pool/#adds-a-new-storage-pool","title":"Adds a new storage pool","text":"<p>Adds a new storage pool</p> <pre><code>sbctl storage-pool add\n    &lt;NAME&gt;\n    &lt;CLUSTER_ID&gt;\n    --pool-max=&lt;POOL_MAX&gt;\n    --lvol-max=&lt;LVOL_MAX&gt;\n    --max-rw-iops=&lt;MAX_RW_IOPS&gt;\n    --max-rw-mbytes=&lt;MAX_RW_MBYTES&gt;\n    --max-r-mbytes=&lt;MAX_R_MBYTES&gt;\n    --max-w-mbytes=&lt;MAX_W_MBYTES&gt;\n</code></pre> Argument Description Data Type Required NAME New pool name string True CLUSTER_ID Cluster id string True Parameter Description Data Type Required Default --pool-max Pool maximum size: 20M, 20G, 0. Default: 0 unknown False 0 --lvol-max Logical volume maximum size: 20M, 20G, 0. Default: 0 unknown False 0 --max-rw-iops Maximum Read Write IO Per Second integer False - --max-rw-mbytes Maximum Read Write Megabytes Per Second integer False - --max-r-mbytes Maximum Read Megabytes Per Second integer False - --max-w-mbytes Maximum Write Megabytes Per Second integer False -"},{"location":"reference/cli/storage-pool/#sets-a-storage-pools-attributes","title":"Sets a storage pool's attributes","text":"<p>Sets a storage pool's attributes</p> <pre><code>sbctl storage-pool set\n    &lt;POOL_ID&gt;\n    --pool-max=&lt;POOL_MAX&gt;\n    --lvol-max=&lt;LVOL_MAX&gt;\n    --max-rw-iops=&lt;MAX_RW_IOPS&gt;\n    --max-rw-mbytes=&lt;MAX_RW_MBYTES&gt;\n    --max-r-mbytes=&lt;MAX_R_MBYTES&gt;\n    --max-w-mbytes=&lt;MAX_W_MBYTES&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True Parameter Description Data Type Required Default --pool-max Pool maximum size: 20M, 20G unknown False - --lvol-max Logical volume maximum size: 20M, 20G unknown False - --max-rw-iops Maximum Read Write IO Per Second integer False - --max-rw-mbytes Maximum Read Write Megabytes Per Second integer False - --max-r-mbytes Maximum Read Megabytes Per Second integer False - --max-w-mbytes Maximum Write Megabytes Per Second integer False -"},{"location":"reference/cli/storage-pool/#lists-all-storage-pools","title":"Lists all storage pools","text":"<p>Lists all storage pools</p> <pre><code>sbctl storage-pool list\n    --json\n    --cluster-id=&lt;CLUSTER_ID&gt;\n</code></pre> Parameter Description Data Type Required Default --json Print outputs in json format marker False - --cluster-id Cluster id string False -"},{"location":"reference/cli/storage-pool/#gets-a-storage-pools-details","title":"Gets a storage pool's details","text":"<p>Gets a storage pool's details</p> <pre><code>sbctl storage-pool get\n    &lt;POOL_ID&gt;\n    --json\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True Parameter Description Data Type Required Default --json Print outputs in json format marker False -"},{"location":"reference/cli/storage-pool/#deletes-a-storage-pool","title":"Deletes a storage pool","text":"<p>It is only possible to delete a pool if it is empty (no provisioned logical volumes contained).</p> <pre><code>sbctl storage-pool delete\n    &lt;POOL_ID&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True"},{"location":"reference/cli/storage-pool/#set-a-storage-pools-status-to-active","title":"Set a storage pool's status to Active","text":"<p>Set a storage pool's status to Active</p> <pre><code>sbctl storage-pool enable\n    &lt;POOL_ID&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True"},{"location":"reference/cli/storage-pool/#sets-a-storage-pools-status-to-inactive","title":"Sets a storage pool's status to Inactive.","text":"<p>Sets a storage pool's status to Inactive.</p> <pre><code>sbctl storage-pool disable\n    &lt;POOL_ID&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True"},{"location":"reference/cli/storage-pool/#gets-a-storage-pools-capacity","title":"Gets a storage pool's capacity","text":"<p>Gets a storage pool's capacity</p> <pre><code>sbctl storage-pool get-capacity\n    &lt;POOL_ID&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True"},{"location":"reference/cli/storage-pool/#gets-a-storage-pools-io-statistics","title":"Gets a storage pool's I/O statistics","text":"<p>Gets a storage pool's I/O statistics</p> <pre><code>sbctl storage-pool get-io-stats\n    &lt;POOL_ID&gt;\n    --history=&lt;HISTORY&gt;\n    --records=&lt;RECORDS&gt;\n</code></pre> Argument Description Data Type Required POOL_ID Pool id string True Parameter Description Data Type Required Default --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False - --records Number of records, default: 20 integer False 20"},{"location":"reference/cli/volume/","title":"Logical volume commands","text":"<pre><code>sbctl volume --help\n</code></pre> <p>Aliases:  lvol </p> <p>Logical volume commands</p>"},{"location":"reference/cli/volume/#adds-a-new-logical-volume","title":"Adds a new logical volume","text":"<p>Adds a new logical volume</p> <pre><code>sbctl volume add\n    &lt;NAME&gt;\n    &lt;SIZE&gt;\n    &lt;POOL&gt;\n    --snapshot\n    --max-size=&lt;MAX_SIZE&gt;\n    --host-id=&lt;HOST_ID&gt;\n    --encrypt\n    --crypto-key1=&lt;CRYPTO_KEY1&gt;\n    --crypto-key2=&lt;CRYPTO_KEY2&gt;\n    --max-rw-iops=&lt;MAX_RW_IOPS&gt;\n    --max-rw-mbytes=&lt;MAX_RW_MBYTES&gt;\n    --max-r-mbytes=&lt;MAX_R_MBYTES&gt;\n    --max-w-mbytes=&lt;MAX_W_MBYTES&gt;\n    --ha-type=&lt;HA_TYPE&gt;\n    --lvol-priority-class=&lt;LVOL_PRIORITY_CLASS&gt;\n    --namespace=&lt;NAMESPACE&gt;\n    --pvc-name=&lt;PVC_NAME&gt;\n</code></pre> Argument Description Data Type Required NAME New logical volume name string True SIZE Logical volume size: 10M, 10G, 10(bytes) unknown True POOL Pool id or name string True Parameter Description Data Type Required Default --snapshot, -s Make logical volume with snapshot capability, default: false marker False False --max-size Logical volume max size unknown False 1000T --host-id Primary storage node id or Hostname string False - --encrypt Use inline data encryption and decryption on the logical volume marker False - --crypto-key1 Hex value of key1 to be used for logical volume encryption string False - --crypto-key2 Hex value of key2 to be used for logical volume encryption string False - --max-rw-iops Maximum Read Write IO Per Second integer False - --max-rw-mbytes Maximum Read Write Megabytes Per Second integer False - --max-r-mbytes Maximum Read Megabytes Per Second integer False - --max-w-mbytes Maximum Write Megabytes Per Second integer False - --ha-type Logical volume HA type (single, ha), default is cluster HA typeAvailable Options:- single- default- ha string False default --lvol-priority-class Logical volume priority class integer False 0 --namespace Set logical volume namespace for k8s clients string False - --pvc-name, --pvc_name Set the logical volume persistent volume claim name for Kubernetes clients. <p>Warning</p> <p>The old parameter name <code>--pvc_name</code> is deprecated and shouldn't be used anymore. It will eventually be removed. Please exchange the use of <code>--pvc_name</code> with <code>--pvc-name</code>. | string | False | - |</p>"},{"location":"reference/cli/volume/#changes-qos-settings-for-an-active-logical-volume","title":"Changes QoS settings for an active logical volume","text":"<p>Changes QoS settings for an active logical volume</p> <pre><code>sbctl volume qos-set\n    &lt;VOLUME_ID&gt;\n    --max-rw-iops=&lt;MAX_RW_IOPS&gt;\n    --max-rw-mbytes=&lt;MAX_RW_MBYTES&gt;\n    --max-r-mbytes=&lt;MAX_R_MBYTES&gt;\n    --max-w-mbytes=&lt;MAX_W_MBYTES&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True Parameter Description Data Type Required Default --max-rw-iops Maximum Read Write IO Per Second integer False - --max-rw-mbytes Maximum Read Write Megabytes Per Second integer False - --max-r-mbytes Maximum Read Megabytes Per Second integer False - --max-w-mbytes Maximum Write Megabytes Per Second integer False -"},{"location":"reference/cli/volume/#lists-logical-volumes","title":"Lists logical volumes","text":"<p>Lists logical volumes</p> <pre><code>sbctl volume list\n    --cluster-id=&lt;CLUSTER_ID&gt;\n    --pool=&lt;POOL&gt;\n    --json\n    --all\n</code></pre> Parameter Description Data Type Required Default --cluster-id List logical volumes in particular cluster string False - --pool List logical volumes in particular pool id or name string False - --json Print outputs in json format marker False - --all List soft deleted logical volumes marker False -"},{"location":"reference/cli/volume/#gets-the-logical-volume-details","title":"Gets the logical volume details","text":"<p>Gets the logical volume details</p> <pre><code>sbctl volume get\n    &lt;VOLUME_ID&gt;\n    --json\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id or name string True Parameter Description Data Type Required Default --json Print outputs in json format marker False -"},{"location":"reference/cli/volume/#deletes-a-logical-volume","title":"Deletes a logical volume","text":"<p>Deletes a logical volume. Attention: All data will be lost! This is an irreversible operation! Actual storage capacity will be freed as an asynchronous background task. It may take a while until the actual storage is released.</p> <pre><code>sbctl volume delete\n    &lt;VOLUME_ID&gt;\n    --force\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volumes id or ids string True Parameter Description Data Type Required Default --force Force delete logical volume from the cluster marker False -"},{"location":"reference/cli/volume/#gets-the-logical-volumes-nvmetcp-connection-strings","title":"Gets the logical volume's NVMe/TCP connection string(s)","text":"<p>Multiple connections to the cluster are always available for multi-pathing and high-availability.</p> <pre><code>sbctl volume connect\n    &lt;VOLUME_ID&gt;\n    --ctrl-loss-tmo=&lt;CTRL_LOSS_TMO&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True Parameter Description Data Type Required Default --ctrl-loss-tmo Control loss timeout for this volume integer False -"},{"location":"reference/cli/volume/#resizes-a-logical-volume","title":"Resizes a logical volume","text":"<p>Resizes a logical volume. Only increasing a volume is possible. The new capacity must fit into the storage pool's free capacity.</p> <pre><code>sbctl volume resize\n    &lt;VOLUME_ID&gt;\n    &lt;SIZE&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True SIZE New logical volume size size: 10M, 10G, 10(bytes) unknown True"},{"location":"reference/cli/volume/#creates-a-snapshot-from-a-logical-volume","title":"Creates a snapshot from a logical volume","text":"<p>Creates a snapshot from a logical volume</p> <pre><code>sbctl volume create-snapshot\n    &lt;VOLUME_ID&gt;\n    &lt;NAME&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True NAME Snapshot name string True"},{"location":"reference/cli/volume/#provisions-a-logical-volumes-from-an-existing-snapshot","title":"Provisions a logical volumes from an existing snapshot","text":"<p>Provisions a logical volumes from an existing snapshot</p> <pre><code>sbctl volume clone\n    &lt;SNAPSHOT_ID&gt;\n    &lt;CLONE_NAME&gt;\n    --resize=&lt;RESIZE&gt;\n</code></pre> Argument Description Data Type Required SNAPSHOT_ID Snapshot id string True CLONE_NAME Clone name string True Parameter Description Data Type Required Default --resize New logical volume size: 10M, 10G, 10(bytes). Can only increase. unknown False 0"},{"location":"reference/cli/volume/#gets-a-logical-volumes-capacity","title":"Gets a logical volume's capacity","text":"<p>Gets a logical volume's capacity</p> <pre><code>sbctl volume get-capacity\n    &lt;VOLUME_ID&gt;\n    --history=&lt;HISTORY&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True Parameter Description Data Type Required Default --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False -"},{"location":"reference/cli/volume/#gets-a-logical-volumes-io-statistics","title":"Gets a logical volume's I/O statistics","text":"<p>Gets a logical volume's I/O statistics</p> <pre><code>sbctl volume get-io-stats\n    &lt;VOLUME_ID&gt;\n    --history=&lt;HISTORY&gt;\n    --records=&lt;RECORDS&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True Parameter Description Data Type Required Default --history (XXdYYh), list history records (one for every 15 minutes) for XX days and YY hours (up to 10 days in total). string False - --records Number of records, default: 20 integer False 20"},{"location":"reference/cli/volume/#checks-a-logical-volumes-health","title":"Checks a logical volume's health","text":"<p>Checks a logical volume's health</p> <pre><code>sbctl volume check\n    &lt;VOLUME_ID&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True"},{"location":"reference/cli/volume/#inflate-a-logical-volume","title":"Inflate a logical volume","text":"<p>All unallocated clusters are allocated and copied from the parent or zero filled if not allocated in the parent. Then all dependencies on the parent are removed.</p> <pre><code>sbctl volume inflate\n    &lt;VOLUME_ID&gt;\n</code></pre> Argument Description Data Type Required VOLUME_ID Logical volume id string True"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<p>Simplyblock is designed as a system for minimal manual intervention. However, once in a while, there may be issues that require some special treatment.</p> <p>This section provides practical solutions for common issues you might encounter when deploying or operating simplyblock. Whether you're dealing with deployment hiccups, performance anomalies, connectivity problems, or configuration errors, you'll find step-by-step guidance to help you diagnose and resolve them quickly. Use this guide to keep your simplyblock environment running smoothly and with confidence.</p>"},{"location":"reference/troubleshooting/control-plane/","title":"Control Plane","text":""},{"location":"reference/troubleshooting/control-plane/#foundationdb-error","title":"FoundationDB Error","text":"<p>Symptom: FoundationDB error. All services that rely upon the FoundationDB key-value storage are offline or refuse to start.</p> <ol> <li>Ensure that IPv6 is disabled. To disable IPv6 follow these steps.</li> <li>Ensure sufficient disk space on the root partition on all control plane nodes. Free disk space can be checked with <code>df -h</code>.</li> <li>If not enough free disk space is available, start by checking the Graylog, MongoDB, and Elasticsearch containers. If those consume most of the disk space, old indices (2-3) can be deleted.</li> <li>Increase the root partition size.</li> <li>If you cannot increase the root partition size, remove any data or service not relevant to the simplyblock control plane and run a <code>docker system prune</code>.</li> <li>Restart the Docker daemon: <code>systemctl restart docker</code></li> <li>Reboot the node</li> </ol>"},{"location":"reference/troubleshooting/control-plane/#graylog-service-is-unresponsive","title":"Graylog Service Is Unresponsive","text":"<p>Symptom: The Graylog service cannot be reached anymore or is unresponsive.</p> <ol> <li>Ensure enough free available memory</li> <li>If short on available memory, stop services non-relevant to the simplyblock control plane.</li> <li>If that doesn't help, reboot the host.</li> </ol>"},{"location":"reference/troubleshooting/control-plane/#graylog-storage-is-full","title":"Graylog Storage is Full","text":"<p>Symptom: The Graylog service cannot start or is unresponsive, and the storage disk is full.</p> <ol> <li>Identify the cause of the disk running full. Run the following commands to find the largest files on the Graylog disk.    Find the largest files<pre><code>df -h\ndu -sh /var/lib/docker\ndu -sh /var/lib/docker/containers\ndu -sh /var/lib/docker/volumes\n</code></pre></li> <li>Delete the old Graylog indices via the Graylog UI.<ul> <li>Go to System -&gt; Indices</li> <li>Select your index set</li> <li>Adjust the Max Number of Indices to a lower number</li> </ul> </li> <li>Reduce Docker disk usage by removing unused Docker volumes and images, as well as old containers.    Remove old Docker entities<pre><code>docker volume prune -f\ndocker image prune -f\ndocker rm $(sudo docker ps -aq --filter \"status=exited\")\n</code></pre></li> <li>Cleanup OpenSearch, Graylog, and MongoDB volume paths and restart the services.    Cleaning up adjacent services<pre><code># Scale services down\ndocker service update monitoring_graylog --replicas=0\ndocker service update monitoring_opensearch --replicas=0\ndocker service update monitoring_mongodb --replicas=0\n\n# Remove old data\nrm -rf /var/lib/docker/volumes/monitoring_graylog_data\nrm -rf /var/lib/docker/volumes/monitoring_os_data\nrm -rf /var/lib/docker/volumes/monitoring_mongodb_data\n\n# Restart services\ndocker service update monitoring_mongodb --replicas=1\ndocker service update monitoring_opensearch --replicas=1\ndocker service update monitoring_graylog --replicas=1\n</code></pre></li> </ol>"},{"location":"reference/troubleshooting/simplyblock-csi/","title":"Kubernetes CSI","text":""},{"location":"reference/troubleshooting/simplyblock-csi/#high-level-csi-driver-architecture","title":"High-Level CSI Driver Architecture","text":"<p>** Controller Plugin:** Runs as a Deployment and manages volume provisioning and deletion.</p> <p>Node Plugin: Runs as a DaemonSet and handles volume attachment, mounting, and unmounting.</p> <p>Sidecars: Handle tasks like external provisioning (<code>csi-provisioner</code>), attaching (<code>csi-attacher</code>), and monitoring (<code>csi-node-driver-registrar</code>).</p>"},{"location":"reference/troubleshooting/simplyblock-csi/#finding-csi-driver-logs-for-a-specific-pvc","title":"Finding CSI Driver Logs for a Specific PVC","text":"<ol> <li>Identify the Node Where the PVC is Mounted    Get the pod name using the persistent volume claim<pre><code>kubectl get pods -A -o \\\njsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.volumes[*].persistentVolumeClaim.claimName}{\"\\n\"}{end}' | \\\ngrep &lt;PVC_NAME&gt;\n</code></pre> Find the node the pod is bound to<pre><code>kubectl get pods -A -o \\\njsonpath='{range .items[*]}{.spec.nodeName}{\"\\t\"}{.spec.volumes[*].persistentVolumeClaim.claimName}{\"\\n\"}{end}' | \\\ngrep &lt;PVC_NAME&gt;\n</code></pre></li> <li>Find the CSI driver pod on that node    Find the CSI driver pod<pre><code>kubectl get pods -n &lt;CSI_NAMESPACE&gt; -o wide | grep &lt;NODE_NAME&gt;\n</code></pre></li> <li>Get Logs from the node plugin    Get the CSI driver pod logs<pre><code>kubectl logs -n &lt;CSI_NAMESPACE&gt; &lt;CSI_NODE_POD&gt; -c &lt;DRIVER_CONTAINER&gt;\n</code></pre></li> </ol>"},{"location":"reference/troubleshooting/simplyblock-csi/#troubleshooting-nvme-related-errors","title":"Troubleshooting NVMe-Related Errors","text":"<p>If the error is NVMe-related (e.g., volume attachment failure, device not found), follow these steps.</p> <ol> <li> <p>Ensure that <code>nvme-cli</code> is installed</p> RHEL / Alma / RockyDebian / Ubuntu <pre><code>sudo dnf install -y nvme-cli\n</code></pre> <pre><code>sudo apt install -y nvme-cli\n</code></pre> </li> <li> <p>Verify if the nvme-tcp kernel module is loaded    </p>Check NVMe/TCP kernel module is loaded<pre><code>lsmod | grep nvme_tcp\n</code></pre> <p>If not available, the driver can be loaded temporarily using the following command:</p> Load NVMe/TCP kernel module<pre><code>sudo modprobe nvme-tcp\n</code></pre> <p>However, to ensure it is automatically loaded at system startup, it should be persisted as following:</p> Red Hat / Alma / RockyDebian / Ubuntu <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules-load.d/nvme-tcp.conf\n</code></pre> <pre><code>echo \"nvme-tcp\" | sudo tee -a /etc/modules\n</code></pre> </li> <li> <p>Check NVMe Connection Status    </p>Check NVMe-oF connection<pre><code>sudo nvme list-subsys\n</code></pre> <p>If the expected NVMe subsystem is missing, reconnect manually:</p> Manually reconnect the NVMe-oF device<pre><code>sudo nvme connect -t tcp \\\n    -n &lt;NVME_SUBSYS_NAME&gt; \\\n    -a &lt;TARGET_IP&gt; \\\n    -s &lt;TARGET_PORT&gt; \\\n    -l &lt;CTRL_LOSS_TIMEOUT&gt; \\\n    -c &lt;RECONNECT_DELAY&gt; \\\n    -i &lt;NR_IO_QUEUES&gt;\n</code></pre> </li> <li> <p>If the issue persists, gather kernel logs and provide them to the simplyblock support team:    </p>Collect logs for support<pre><code>sudo dmesg | grep -i nvme\n</code></pre> </li> </ol>"},{"location":"reference/troubleshooting/storage-plane/","title":"Storage Plane","text":""},{"location":"reference/troubleshooting/storage-plane/#fresh-cluster-cannot-be-activated","title":"Fresh Cluster Cannot Be Activated","text":"<p>Symptom: After a fresh deployment, the cluster cannot be activated. The activation process hangs or fails, and the storage nodes show <code>n/0</code> disks available in the disks column (<code>sbctl storage-node list</code>).</p> <ol> <li>Shutdown all storage nodes: <code>sbctl storage-node shutdown --force</code></li> <li>Force remove all storage nodes: <code>sbctl storage-node remove --force</code></li> <li>Delete all storage nodes: <code>sbctl storage-node delete</code></li> <li>Re-add all storage nodes. The disks should become active.</li> <li>Try to activate the cluster.</li> </ol>"},{"location":"reference/troubleshooting/storage-plane/#storage-node-health-check-shows-healthfalse","title":"Storage Node Health Check Shows Health=False","text":"<p>Symptom: The storage node health check returns health=false (<code>sbctl storage-node list</code>).</p> <ol> <li>First run <code>sbctl storage-node check</code>.</li> <li>If the command keeps showing an unhealthy storage node, suspend, shutdown, and restart the storage node.</li> </ol> <p>Danger</p> <p>Never shutdown or restart a storage node while the cluster is in rebalancing state. This can lead to potential I/O operation. This is independent of the cluster's high-availability status. Check the cluster status with any of the following commands:</p> <pre><code>sbctl cluster list\nsbctl cluster get &lt;cluster-id&gt;\nsbctl cluster show &lt;cluster-id&gt;\n</code></pre>"},{"location":"release-notes/","title":"Release Notes","text":"<p>Simplyblock regularly provides new releases with new features, performance enhancements, bugfixes, and more. </p> <p>This section provides detailed information about each Simplyblock release, including new features, enhancements, bug fixes, and known issues. Stay informed about the latest developments to ensure optimal performance and take full advantage of simplyblock's capabilities.</p>"},{"location":"release-notes/25-3-pre/","title":"25.3-PRE","text":"<p>Simplyblock is happy to release the pre-release version of the upcoming Simplyblock 25.3.</p> <p>Warning</p> <p>This is a pre-release and may contain issues. It is not recommended for production use.</p>"},{"location":"release-notes/25-3-pre/#new-features","title":"New Features","text":"<p>Simplyblock strives to provide a strong product. Following is a list of the enhancements and features that made it into this release.</p> <ul> <li>High availability has been significantly hardened for production. Main improvements concern the support for safe and interruption free fail-over and fail-back in different types of outage scenarios. Those include: partial network outage, full network outage, host failure, container failure, reboots, graceful and ungraceful shutdowns of nodes. Tested for single und dial node outages.</li> <li>Multiple journal compression bugs have been identified and fixed.</li> <li>Multiple journal fail-over bugs have been identified and fixed.</li> <li>Logical volume creation, deletion, snapshotting, and resizing can now be performed via a secondary storage node (when the primary storage node is offline).</li> <li>The system has been hardened against high load scenarios, determined by the amount of parallel NVMe-oF volumes per node, the amount of storage and parallel I/O. Tested up to 400 concurrent and fully active logical volumes per node and u to 20 concurrent I/O processes per logical volume.</li> <li>Erasure coding schemes 2+1, 2+2, 4+2, 4+4 have been made power-fail-safe with high availability enabled.</li> <li>System has been extensively tested outside of AWS with KVM-based virtualization and on bare-metal deployments.</li> <li>Significant rework of the command line tool <code>sbcli</code> to simplify commands and parameters and make it more consistent. For more information see Important Changes.</li> <li>Support for Linux Core Isolation to improve performance and system stability.</li> <li>Added support for Proxmox via the Simplyblock Proxmox Integration.</li> </ul>"},{"location":"release-notes/25-3-pre/#important-changes","title":"Important Changes","text":"<p>Simplyblock made significant changes to the command line tool <code>sbcli</code> to simplify working with it. Many parameters and commands were meant for internal testing and confusing to users. Hence, simplyblock decided to make those private.</p> <p>Parameters and commands that were made private should not influence users. If the change to private for a parameter or command should affect your deployment, please reach out.</p> <p>Most changes are backwards-compatible, however, some are not. Following is a list of all changes.</p> <ul> <li>Command: <code>storage-node</code><ul> <li>Renamed command <code>sn</code> to <code>storage-node</code> (<code>sn</code> still works as an alias)</li> <li>Changed subcommand <code>device-testing-mode</code> to private</li> <li>Changed subcommand <code>info-spdk</code> to private</li> <li>Changed subcommand <code>remove-jm-device</code> to private</li> <li>Changed subcommand <code>send-cluster-map</code> to private</li> <li>Changed subcommand <code>get-cluster-map</code> to private</li> <li>Changed subcommand <code>dump-lvstore</code> to private</li> <li>Changed subcommand <code>set</code> to private</li> <li>Subcommand: <code>deploy</code><ul> <li>Added parameter <code>--cpu-mask</code></li> <li>Added parameter <code>--isolate-cores</code></li> </ul> </li> <li>Subcommand: <code>add-node</code><ul> <li>Renamed parameter <code>--partitions</code> to <code>--journal-patition</code></li> <li>Renamed parameter <code>--storage-nics</code> to <code>--data-nics</code></li> <li>Renamed parameter <code>--number-of-vcpus</code> to <code>--vcpu-count</code></li> <li>Added parameter <code>--max-snap</code> (private)</li> <li>Changed parameter <code>--jm-percent</code> to private</li> <li>Changed parameter <code>--number-of-devices</code> to private</li> <li>Changed parameter <code>--size-of-device</code> to private</li> <li>Changed parameter <code>--cpu-mask</code> to private</li> <li>Changed parameter <code>--spdk-image</code> to private</li> <li>Changed parameter <code>--spdk-debug</code> to private</li> <li>Changed parameter <code>--iobuf_small_bufsize</code> to private</li> <li>Changed parameter <code>--iobuf_large_bufsize</code> to private</li> <li>Changed parameter <code>--enable-test-device</code> to private</li> <li>Changed parameter <code>--disable-ha-jm</code> to private</li> <li>Changed parameter <code>--id-device-by-nqn</code> to private</li> <li>Changed parameter <code>--max-snap</code> to private </li> </ul> </li> <li>Subcommand: <code>restart</code><ul> <li>Renamed parameter <code>--node-ip</code> to <code>--node-addr</code> (<code>--node-ip</code> still works but is deprecated and should be exchanged)</li> <li>Changed parameter <code>--max-snap</code> to private</li> <li>Changed parameter <code>--max-size</code> to private</li> <li>Changed parameter <code>--spdk-image</code> to private</li> <li>Changed parameter <code>--spdk-debug</code> to private</li> <li>Changed parameter <code>--iobuf_small_bufsize</code> to private</li> <li>Changed parameter <code>--iobuf_large_bufsize</code> to private</li> </ul> </li> <li>Subcommand: <code>list-devices</code><ul> <li>Removed parameter <code>--sort</code> / <code>-s</code></li> </ul> </li> </ul> </li> <li>Command: <code>cluster</code></li> <li>Changed subcommand <code>graceful-shutdown</code> to private</li> <li>Changed subcommand <code>graceful-startup</code> to private<ul> <li>Subcommand: <code>deploy</code><ul> <li>Renamed parameter <code>--separate-journal-device</code> to <code>--journal-partition</code></li> <li>Renamed parameter <code>--storage-nics</code> to <code>--data-nics</code></li> <li>Renamed parameter <code>--number-of-vcpus</code> to <code>--vcpu-count</code></li> <li>Changed parameter <code>--ha-jm-count</code> to private</li> <li>Changed parameter <code>--enable-qos</code> to private</li> <li>Changed parameter <code>--blk-size</code> to private</li> <li>Changed parameter <code>--page_size</code> to private</li> <li>Changed parameter <code>--CLI_PASS</code> to private</li> <li>Changed parameter <code>--grafana-endpoint</code> to private</li> <li>Changed parameter <code>--distr-bs</code> to private</li> <li>Changed parameter <code>--max-queue-size</code> to private</li> <li>Changed parameter <code>--inflight-io-threshold</code> to private</li> <li>Changed parameter <code>--jm-percent</code> to private</li> <li>Changed parameter <code>--max-snap</code> to private</li> <li>Changed parameter <code>--number-of-distribs</code> to private</li> <li>Changed parameter <code>--size-of-device</code> to private</li> <li>Changed parameter <code>--cpu-mask</code> to private</li> <li>Changed parameter <code>--spdk-image</code> to private</li> <li>Changed parameter <code>--spdk-debug</code> to private</li> <li>Changed parameter <code>--iobuf_small_bufsize</code> to private</li> <li>Changed parameter <code>--iobuf_large_bufsize</code> to private</li> <li>Changed parameter <code>--enable-test-device</code> to private</li> <li>Changed parameter <code>--disable-ha-jm</code> to private</li> <li>Changed parameter <code>--lvol-name</code> to private</li> <li>Changed parameter <code>--lvol-size</code> to private</li> <li>Changed parameter <code>--pool-name</code> to private</li> <li>Changed parameter <code>--pool-max</code> to private</li> <li>Changed parameter <code>--snapshot</code> / <code>-s</code> to private</li> <li>Changed parameter <code>--max-volume-size</code> to private</li> <li>Changed parameter <code>--encrypt</code> to private</li> <li>Changed parameter <code>--crypto-key1</code> to private</li> <li>Changed parameter <code>--crypto-key2</code> to private</li> <li>Changed parameter <code>--max-rw-iops</code> to private</li> <li>Changed parameter <code>--max-rw-mbytes</code> to private</li> <li>Changed parameter <code>--max-r-mbytes</code> to private</li> <li>Changed parameter <code>--max-w-mbytes</code> to private</li> <li>Changed parameter <code>--distr-vuid</code> to private</li> <li>Changed parameter <code>--lvol-ha-type</code> to private</li> <li>Changed parameter <code>--lvol-priority-class</code> to private</li> <li>Changed parameter <code>--fstype</code> to private</li> </ul> </li> <li>Subcommand: <code>create</code><ul> <li>Changed parameter <code>--page_size</code> to private</li> <li>Changed parameter <code>--CLI_PASS</code> to private</li> <li>Changed parameter <code>--distr-bs</code> to private</li> <li>Changed parameter <code>--distr-chunk-bs</code> to private</li> <li>Changed parameter <code>--ha-type</code> to private</li> <li>Changed parameter <code>--max-queue-size</code> to private</li> <li>Changed parameter <code>--inflight-io-threshold</code> to private</li> <li>Changed parameter <code>--enable-qos</code> to private</li> </ul> </li> <li>Subcommand: <code>add</code><ul> <li>Changed parameter <code>--page_size</code> to private</li> <li>Changed parameter <code>--distr-bs</code> to private</li> <li>Changed parameter <code>--distr-chunk-bs</code> to private</li> <li>Changed parameter <code>--max-queue-size</code> to private</li> <li>Changed parameter <code>--inflight-io-threshold</code> to private</li> <li>Changed parameter <code>--enable-qos</code> to private</li> </ul> </li> </ul> </li> <li>Command: <code>storage-pool</code>         - Removed subcommand <code>get-secret</code>         - Removed subcommand <code>update-secret</code><ul> <li>Subcommand: <code>add</code><ul> <li>Changed parameter <code>--has-secret</code> to private -Command: <code>caching-node</code></li> </ul> </li> <li>Subcommand: <code>add-node</code><ul> <li>Renamed parameter <code>--number-of-vcpus</code> to <code>--vcpu-count</code></li> <li>Changed parameter <code>--cpu-mask</code> to private</li> <li>Changed parameter <code>--memory</code> to private</li> <li>Changed parameter <code>--spdk-image</code> to private</li> </ul> </li> <li>Command: <code>volume</code><ul> <li>Changed subcommand <code>list-mem</code> to private</li> <li>Changed subcommand <code>move</code> to private</li> </ul> </li> <li>Subcommand: <code>add</code><ul> <li>Renamed parameter <code>--pvc_name</code> to <code>--pvc-name</code> (<code>--pvc_name</code> still works but is deprecated and should be exchanged)</li> <li>Changed parameter <code>--distr-vuid</code> to private</li> <li>Changed parameter <code>--uid</code> to private</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/25-3-pre/#known-issues","title":"Known Issues","text":"<p>Simplyblock always seeks to provide a stable and strong release. However, smaller known issues happen. Following is a list of known issues of the current simplyblock release.</p> <p>Info</p> <p>This is a pre-release and many of those known issues are expected to be resolved with the final release.</p> <ul> <li>The control plane reaches a limit at around 2,200 logical volumes.</li> <li>If a storage node goes offline while a logical volume is being deleted, the storage cluster may keep some garbage.</li> <li>In rare cases, resizing a logical volume under high I/O load may cause a storage node restart.</li> <li>If a storage cluster reaches its capacity limit and runs full, file systems on logical volumes may return I/O errors.</li> <li>A fail-back after a fail-over may increase to &gt;10s (with freezing I/O) with a larger number of logical volumes per storage node (&gt;100 logical volumes).</li> <li>A fail-over time may increase to &gt;5s (with freezing I/O) on large logical volumes (&gt;5 TB).</li> <li>During a node outage, I/O performance may drop significantly with certain I/O patterns due to a performance issue in the journal compression.</li> <li>Journal compression may cause significant I/O performance drops (10-20s) in periodic intervals under certain I/O load patterns, especially when the logical volume capacity reaches its limits for the first time.</li> <li>A peak read IOPS performance regression has been observed.</li> <li>In rare cases, a primary-secondary storage node combination may get into a flip-flop situation with multiple fail-over/fail-back iterations due to network or configuration issues of particular logical volumes or clients.</li> <li>A secondary node may get stuck when trying to restart under high load (&gt;100 logical volumes).</li> <li>Node affinity rules are not considered after a storage node migration to a new host.</li> <li>Return code of sbcli commands is always 0.</li> </ul>"},{"location":"release-notes/25-6-ga/","title":"25.6","text":"<p>Simplyblock is happy to release the general availability release of Simplyblock 25.6.</p>"},{"location":"release-notes/25-6-ga/#new-features","title":"New Features","text":"<p>Simplyblock strives to provide a strong product. Following is a list of the enhancements and features that made it into this release.</p> <ul> <li>General: Renamed <code>sbcli</code> to <code>sbctl</code>. The old <code>sbcli</code> command is deprecated but still available as a fallback for scripts.</li> <li>Storage Plane: Increased maximum of available logical volumes per storage node to 1,000.</li> <li>Storage Plane: Added the option to start multiple storage nodes in parallel on the same host. This is useful for machines with multiple NUMA nodes and many CPU cores to increase scalability.</li> <li>Storage Plane: Added NVMe multipathing independent high-availability between storage nodes to harden resilience against network issues and improve failover.</li> <li>Storage Plane: Removed separate secondary storage nodes for failover. From now on, all storage nodes act as primary and secondary storage nodes.</li> <li>Storage Plane: Added I/O redirection in case of failover to secondary to improve cluster stability and failover times.</li> <li>Storage Plane: Added support for CPU Core Isolation to improve performance consistency. Core masks and core isolation are auto-applied on disaggregated setups.</li> <li>Storage Plane: Added <code>sbctl storage-node configure</code> command to automate the configuration of new storage nodes. See Configure a Storage Node for more information.</li> <li>Storage Plane: Added optimized algorithms for the 4+1 and 4+2 erasure coding configurations.</li> <li>Storage Plane: Reimplemented the Quality of Service (QoS) subsystem with significant less overhead than the old one.</li> <li>Storage Plane: Added support for namespaced logical volumes (experimental).</li> <li>Storage Plane: Reimplemented the initialization of a new page to significantly improve the performance of first write to page issues.</li> <li>Storage Plane: Added support for optional labels when using strict anti-affinity.</li> <li>Storage Plane: Added support for node affinity in case of a device failure to try to recover onto another device on the host.</li> <li>Proxmox: Added support for native Proxmox node migration.</li> <li>AWS: Added Bottlerocket support.</li> <li>AWS: Added multipathing support for Amazon Linux 2, Amazon Linux 2023, Bottlerocket.</li> <li>GCP: Added support for Google Compute Engine.</li> </ul>"},{"location":"release-notes/25-6-ga/#fixes","title":"Fixes","text":"<ul> <li>Storage Plane: Fixed a critical issue on cluster expansion during rebalancing.</li> <li>Storage Plane: Optimized internal journal compression of meta-data to use fewer CPU resources.</li> <li>Storage Plane: Significantly improved the fail-back time in failover situations.</li> <li>Storage Plane: Fixed a CRC checksum error that occurred in rare situations after a node outage.</li> <li>Storage Plane: Fixed a conflict resolution issue with could lead to data corruption in failover scenarios.</li> <li>Storage Plane: Fixed a segmentation fault after resizing multiple logical volumes in a fast sequence.</li> <li>Storage Plane: Fixed data placement issues which could lead to unexpected I/O interruptions after a sequence of outages.</li> <li>Storage Plane: Reduced huge pages consumption by about 1.5x. Huge pages are automatically recalculated on node restart.</li> <li>Storage Plane: Fixed an RPC issue on clusters with eight or more storage nodes.</li> <li>Storage Plane: Fixed a race condition on storage node restarts or cluster reactivations.</li> <li>Storage Plane: Hardened a health-check issue which affected multiple services.</li> <li>Storage Plane: Improved shared buffers calculation on large storage nodes.</li> <li>Storage Plane: Fixed an issue in the metadata journal which could lead to a temporary conflict and I/O interruption on the fail-back of large logical volumes.</li> <li>Storage Plane: Fixed an issue where a storage node would stay unhealthy after a cluster upgrade.</li> <li>Storage Plane: Fixed an issue where the internal journal devices would fail to automatically reconnect after a cluster outage.</li> <li>Storage Plane: Fixed an issue where a restart of one storage node could lead to a crash on another storage node.</li> <li>Storage Plane: Fixed reattaching Amazon EBS volumes when migrating a storage node to a new host.</li> <li>Control Plane: Improved error handling on the internal controller code base.</li> <li>Control Plane: Fixed a range of false-positive detections that lead to unexpected storage node restarts or, in rare cases, cluster suspension.</li> <li>Control Plane: Cleaned up the API from unnecessary calls and fixed smaller response content issues.</li> <li>Control Plane: Improved handling of Greylog in case of primary management node failover.</li> <li>Control Plane: Fixed multiple issues regarding spill-over and outages in case of a management node disk running full.</li> <li>Control Plane: Fixed an issue where the generated logical volume connection string is missing the logical volume id in the NQN.</li> <li>Control Plane: Fixed multiple build issues with ARM64 CPUs.</li> <li>Control Plane: Fixed multiple issues when deleting logical volumes and snapshots which could lead to dangling garbage and inconsistencies.</li> <li>Control Plane: Fixed a primary storage node restart bug.</li> <li>Control Plane: Improved NVMe device detection by switching from serial number to PCIe address.</li> <li>Control Plane: Fixed an issue, related to logical volume operations, where FoundationDB's memory consumption would continue to increase over time.</li> <li>Control Plane: Fixed an issue where migration tasks would stale with status \"mig error: 8, retrying\".</li> <li>Control Plane: Improved observability by polling thread information from SPDK and store it in Prometheus.</li> <li>Control Plane: Improved the performance of parallel logical volume creations.</li> <li>Control Plane: Fixed data unit of read_speed in the Grafana cluster dashboard.</li> <li>Control Plane: Fixed an issue where the PromAgent image wouldn't be upgraded on a cluster upgrade.</li> <li>Kubernetes: Fixed an issue where the CSI driver would hang if it tries to delete a snapshot in error state.  </li> <li>Kubernetes: Fixed hanging NVMe/TCP connections in the CSI driver on storage node restarts or failovers.</li> <li>Kubernetes: Fixed an issue with failing volume snapshots.</li> <li>Kubernetes: Improved the version pinning of required services.</li> <li>Kubernetes: Fixed an issue where NVMe/TCP connections in multipathing setups would be disconnected in the wrong order.</li> <li>Proxmox: Improved automatic reconnecting of volumes after storage node restarts and failovers.</li> <li>"},{"location":"release-notes/25-6-ga/#important-changes","title":"Important Changes","text":"<ul> <li>Architecture: Separate secondary nodes have been removed as a concept. Instead, in a high-availability cluster, every deployed primary storage node also acts as a secondary storage node to another primary. </li> <li>Storage Plane: NVMe devices are now identified by their serial number to enable PCIe renumbering in case of changes to the system configuration.</li> <li>Firewall rules adjustment: An existing port range TCP/8080-8890 was changed to TCP/8080-8180. The firewall configuration and AWS Security Groups need to be adjusted accordingly.</li> <li>Firewall rules adjustment: An existing port range TCP/9090-9900 was changed to TCP/9100-9200. The firewall configuration and AWS Security Groups need to be adjusted accordingly.</li> <li>Firewall rules adjustment: A new port range TCP/9030-9059 was added. The firewall configuration and AWS Security Groups need to be adjusted accordingly.</li> <li>Firewall rules adjustment: A new port range TCP/9060-9099 was added. The firewall configuration and AWS Security Groups need to be adjusted accordingly.</li> <li>Firewall rules adjustment: An existing port TCP/4420 has been removed. The firewall configuration and AWS Security Groups need to be adjusted accordingly.</li> <li>Image registry: The image registry moved from Amazon ECR to DockerHub.</li> </ul>"},{"location":"release-notes/25-6-ga/#known-issues","title":"Known Issues","text":"<p>Simplyblock always seeks to provide a stable and strong release. However, smaller known issues happen. Following up is a list of known issues for the current simplyblock release.</p> <ul> <li>GCP: On GCP, multiple Local SSDs are connected as NVMe Namespace devices. There is a bug that prevents more than one Local SSD from being added to a storage node. For the time being, use one Local SSD per storage node. The storage node must be sized accordingly.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>The tutorial section provides a collection of tutorials, introduction videos, and step-by-step guides to help users get started with simplyblock.</p>"},{"location":"tutorials/kubernetes-deployment/","title":"Deployment on Kubernetes","text":""},{"location":"tutorials/simplyblock-intro/","title":"Introduction to Simplyblock","text":""},{"location":"tutorials/sla-intro/","title":"Introduction to SLA","text":""},{"location":"usage/","title":"Usage","text":"<p>Simplyblock provides powerful tools for managing Logical Volumes (LVs) through its command-line interface (sbctl) and Kubernetes CSI driver, enabling seamless storage operations in cloud-native and enterprise environments. Whether working in a standalone environment or integrating with Kubernetes, these tools allow users to provision, unprovision, expand, snapshot, and clone logical volumes efficiently.</p> <p>The <code>sbctl</code> utility offers direct control over storage management via the command line, making it ideal for administrators who need fine-grained operational control. Meanwhile, the Kubernetes CSI driver enables automated volume provisioning and lifecycle management within containerized workloads. This section provides detailed guidance on using both methods to perform essential storage operations, ensuring scalable and resilient data management within simplyblock\u2019s distributed storage architecture.</p>"},{"location":"usage/baremetal/","title":"Command Line Interface","text":"<p><code>sbctl</code> is the official command line interface (CLI) for managing simplyblock management and storage clusters. It provides administrators with a powerful and flexible tool to perform essential storage operations directly from the terminal, enabling full control over logical volumes (LVs) and the simplyblock environment. With <code>sbctl</code>, common tasks such as provisioning, cloning, resizing, and encrypting logical volumes are efficiently executed with clear, consistent commands.</p>"},{"location":"usage/baremetal/cloning/","title":"Cloning a Logical Volume","text":"<p>Cloning a logical Volume (LV) in simplyblock creates a writable, independent copy-on-write clone of an existing volume. This is useful for scenarios such as testing, staging, backups, and development environments, all while preserving the original data. Clones can be created quickly and efficiently using the <code>sbctl</code> command line interface.</p>"},{"location":"usage/baremetal/cloning/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with an existing logical volume.</li> <li>An existing snapshot of a logical volume.</li> <li><code>sbctl</code> installed and configured with access to the simplyblock management API.</li> </ul>"},{"location":"usage/baremetal/cloning/#cloning-a-logical-volume","title":"Cloning a Logical Volume","text":"<p>To create a clone of an existing Logical Volume:</p> <pre><code>sbctl snapshot clone \\\n  &lt;SNAPSHOT_UUID&gt; \\\n  &lt;NEW_VOLUME_NAME&gt;\n</code></pre>"},{"location":"usage/baremetal/cloning/#verification","title":"Verification","text":"<p>After cloning, the new Logical Volume can be listed:</p> <pre><code>sbctl volume list\n</code></pre> <p>Details of the cloned volume can be retrieved using:</p> <pre><code>sbctl volume get &lt;VOLUME_UUID&gt;\n</code></pre>"},{"location":"usage/baremetal/encrypting/","title":"Encrypting a Logical Volume","text":"<p>Simplyblock supports encryption of logical volumes (LVs) to protect data at rest, ensuring that sensitive information remains secure across the distributed storage cluster. Encryption is applied during volume creation using the <code>sbctl</code> command line interface, and encrypted volumes are handled transparently during regular operation.</p> <p>Encrypting Logical Volumes ensures that simplyblock storage meets data protection and compliance requirements, safeguarding sensitive workloads without compromising performance.</p> <p>Warning</p> <p>Encryption must be specified at the time of volume creation. Existing logical volumes cannot be retroactively encrypted.</p>"},{"location":"usage/baremetal/encrypting/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with encryption support enabled.</li> <li><code>sbctl</code> installed and configured with access to the Simplyblock management API.</li> </ul>"},{"location":"usage/baremetal/encrypting/#encrypted-volumes-in-simplyblock","title":"Encrypted Volumes in Simplyblock","text":"<p>Simplyblock supports the encryption of logical volumes. Internally, simplyblock utilizes the industry-proven crypto bdev\u00a0\u29c9 provided by SPDK to implement its encryption functionality.</p> <p>The encryption uses an AES_XTS variable-length block cipher. This cipher requires two keys of 16 to 32 bytes each. The keys need to have the same length, meaning that if one key is 32 bytes long, the other one has to be 32 bytes, too.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two keys of 32 bytes.</p>"},{"location":"usage/baremetal/encrypting/#generate-random-keys","title":"Generate Random Keys","text":"<p>Simplyblock does not provide an integrated way to generate encryption keys, but recommends using the OpenSSL tool chain.</p> <p>To generate the two keys, the following command is run twice. The result must be stored for later.</p> Create an Encryption Key<pre><code>openssl rand -hex 32\n</code></pre>"},{"location":"usage/baremetal/encrypting/#creating-an-encrypted-logical-volume","title":"Creating an Encrypted Logical Volume","text":"<p>To provision a new Logical Volume with encryption enabled:</p> <pre><code>sbctl volume add \\\n  --encrypt \\\n  --crypto-key1 &lt;HEX_KEY_1&gt; \\\n  --crypto-key2 &lt;HEX_KEY_2&gt; \\\n  &lt;VOLUME_NAME&gt; \\\n  &lt;VOLUME_SIZE&gt; \\\n  &lt;POOL_NAME&gt;\n</code></pre> <p>To see all available parameters when creating a logical volume, see Provisioning.</p>"},{"location":"usage/baremetal/encrypting/#parameters","title":"Parameters","text":"Parameter Description Default --encrypt Enables inline encryption on the logical volume. false --crypto-key1 CRYPTO_KEY1 The hex value of the first encryption key. --crypto-key2 CRYPTO_KEY2 The hex value of the second encryption key."},{"location":"usage/baremetal/encrypting/#verification","title":"Verification","text":"<p>Check encryption status with:</p> <pre><code>sbctl volume get &lt;VOLUME_UUID&gt;\n</code></pre> <p>Look for the encryption field to confirm that encryption is active.</p>"},{"location":"usage/baremetal/expanding/","title":"Expanding a Logical Volume","text":""},{"location":"usage/baremetal/expanding/#resizing-expanding-a-logical-volume-with-sbcli","title":"Resizing (Expanding) a Logical Volume with sbcli","text":""},{"location":"usage/baremetal/expanding/#overview","title":"Overview","text":"<p>Resizing a logical Volume (LV) in simplyblock allows additional capacity to be allocated without downtime, ensuring workloads have sufficient storage as demand grows. The <code>sbctl</code> command line interface is used to expand the size of an existing Logical Volume in a simple and efficient manner.</p>"},{"location":"usage/baremetal/expanding/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with a valid logical volume.</li> <li><code>sbctl</code> installed and configured with access to the simplyblock management API.</li> </ul>"},{"location":"usage/baremetal/expanding/#expanding-a-logical-volume","title":"Expanding a Logical Volume","text":"<p>To increase the size of an existing logical volume:</p> <pre><code>sbctl volume resize \\\n  &lt;VOLUME_UUID&gt; \\\n  &lt;NEW_SIZE&gt;\n</code></pre>"},{"location":"usage/baremetal/expanding/#verification","title":"Verification","text":"<p>After resizing, confirm the new volume size:</p> <pre><code>sbctl volume get &lt;VOLUME_UUID&gt;\n</code></pre>"},{"location":"usage/baremetal/expanding/#resize-the-filesystem-if-required","title":"Resize the Filesystem (If Required)","text":"<p>Certain filesystems, such as ext4, may require growing the filesystem after the underlying volume has been expanded.</p>"},{"location":"usage/baremetal/expanding/#for-ext4-filesystems","title":"For <code>ext4</code> filesystems:","text":"<pre><code>resize2fs /dev/nvmeXnY\n</code></pre>"},{"location":"usage/baremetal/expanding/#for-xfs-filesystems","title":"For <code>xfs</code> filesystems:","text":"<pre><code>xfs_growfs /mount/point\n</code></pre>"},{"location":"usage/baremetal/expanding/#shrinking-a-volume","title":"Shrinking a Volume","text":"<p>Theoretically, it is possible to shrink a volume. It can, however, create issues with certain filesystems. When a volume needs to be shrunk, it is recommended to create a snapshot and restore it onto a new volume.</p>"},{"location":"usage/baremetal/provisioning/","title":"Provisioning a Logical Volume","text":"<p>A logical volume (LV) in simplyblock can be provisioned using the <code>sbctl</code> command line interface.  This allows administrators to create virtual NVMe block devices backed by simplyblock\u2019s distributed storage, enabling  high-performance and fault-tolerant storage for workloads.</p>"},{"location":"usage/baremetal/provisioning/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with healthy management and storage nodes.</li> <li><code>sbctl</code> installed and configured with access to the simplyblock management API.</li> </ul>"},{"location":"usage/baremetal/provisioning/#provisioning-a-new-logical-volume","title":"Provisioning a New Logical Volume","text":"<p>To create a new logical volume:</p> <pre><code>sbctl volume add \\\n  --max-rw-iops &lt;IOPS&gt; \\\n  --max-r-mbytes &lt;THROUGHPUT&gt; \\\n  --max-w-mbytes &lt;THROUGHPUT&gt; \\\n  &lt;VOLUME_NAME&gt; \\\n  &lt;VOLUME_SIZE&gt; \\\n  &lt;POOL_NAME&gt;\n</code></pre>"},{"location":"usage/baremetal/provisioning/#available-parameters","title":"Available Parameters","text":"Parameter Description Default --snapshot, -s Enables snapshot capability on the logical volume. false --max-size Maximum size of the logical volume. 0 --ha-type High availability mode of the logical volume. ha --encrypt Enables inline encryption on the logical volume. false --crypto-key1 CRYPTO_KEY1 The hex value of the first encryption key. --crypto-key2 CRYPTO_KEY2 The hex value of the second encryption key. --max-rw-iops MAX_RW_IOPS Maximum IO operations per second. 0 --max-rw-mbytes MAX_RW_MBYTES Maximum read/write throughput. 0 --max-r-mbytes MAX_R_MBYTES Maximum read throughout. 0 --max-w-mbytes MAX_W_MBYTES Maximum write throughput. 0"},{"location":"usage/baremetal/provisioning/#verification","title":"Verification","text":"<p>After creation, the Logical Volume can be listed and verified:</p> <pre><code>sbctl volume list\n</code></pre> <p>Details of the volume can be retrieved using:</p> <pre><code>sbctl volume get &lt;VOLUME_UUID&gt;\n</code></pre>"},{"location":"usage/baremetal/quality-of-service/","title":"Defining Quality of Service","text":"<p>Simplyblock allows Quality of Service (QoS) limits to be applied to logical volumes (LVs) to control performance by defining maximum IOPS and throughput. QoS settings can be configured during volume creation or adjusted on an active Logical Volume using the <code>sbctl</code> command line interface.</p> <p>Configuring QoS allows simplyblock logical volumes to deliver predictable performance by limiting resource consumption and ensuring balanced workload distribution across the storage cluster.</p>"},{"location":"usage/baremetal/quality-of-service/#setting-qos-during-volume-creation","title":"Setting QoS During Volume Creation","text":"<p>QoS can be applied when creating a new logical volume:</p> <pre><code>sbctl volume create \\\n  --max-rw-iops MAX_RW_IOPS 3500 \\\n  --max-rw-mbytes MAX_RW_MBYTES 125 \\\n  &lt;VOLUME_NAME&gt; \\\n  &lt;VOLUME_SIZE&gt; \\\n  &lt;POOL_NAME&gt;\n</code></pre>"},{"location":"usage/baremetal/quality-of-service/#parameters","title":"Parameters","text":"Parameter Description Default --max-rw-iops MAX_RW_IOPS Maximum IO operations per second. 0 --max-rw-mbytes MAX_RW_MBYTES Maximum read/write throughput. 0 --max-r-mbytes MAX_R_MBYTES Maximum read throughout. 0 --max-w-mbytes MAX_W_MBYTES Maximum write throughput. 0 <p>To see all available parameters when creating a logical volume, see Provisioning.</p>"},{"location":"usage/baremetal/quality-of-service/#changing-qos-on-an-active-logical-volume","title":"Changing QoS on an Active Logical Volume","text":"<p>QoS settings can also be updated on an existing logical volume:</p> <pre><code>sbctl volume qos-set \\\n  --max-rw-iops MAX_RW_IOPS 5000 \\\n  --max-rw-mbytes MAX_RW_MBYTES 250 \\\n  &lt;VOLUME_UUID&gt;\n</code></pre>"},{"location":"usage/baremetal/quality-of-service/#verification","title":"Verification","text":"<p>To check the current QoS settings:</p> <pre><code>sbctl volume get &lt;VOLUME_UUID&gt;\n</code></pre> <p>Review the output for the active QoS configuration.</p>"},{"location":"usage/baremetal/removing/","title":"Removing a Logical Volume","text":"<p>Removing a logical Volume (LV) in simplyblock permanently deletes the volume and its associated data from the cluster. This operation is performed using the <code>sbctl</code> command line interface. Care should be taken to verify that the volume is no longer in use and that backups are in place if needed.</p> <p>While deleting a logical volume is a straightforward operation, it must be executed carefully to avoid accidental data loss. Always ensure that the volume is no longer needed before removal.</p> <p>Danger</p> <p>This action is irreversible. Once a logical volume is deleted, all data stored on it is permanently lost, except a snapshot or snapshot chain exists.</p>"},{"location":"usage/baremetal/removing/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with <code>sbctl</code> configured.</li> <li>Ensure the Logical Volume is not mounted or in active use.</li> <li>Verify that data stored on the volume is no longer required or has been backed up.</li> </ul>"},{"location":"usage/baremetal/removing/#deleting-a-logical-volume","title":"Deleting a Logical Volume","text":"<p>To remove a Logical Volume:</p> <pre><code>sbctl volume delete &lt;VOLUME_UUID&gt; [--force]\n</code></pre>"},{"location":"usage/baremetal/removing/#parameters","title":"Parameters","text":"<ul> <li><code>--force</code>: Optional parameter to force the deletion of the logical volume</li> </ul>"},{"location":"usage/baremetal/removing/#verification","title":"Verification","text":"<p>To confirm that the volume has been successfully deleted:</p> <pre><code>sbctl volume list\n</code></pre> <p>Verify that the volume no longer appears in the list of active logical volumes.</p> <p>Warning</p> <p>If snapshots or snapshot chains of the logical volume exist, the internal storage is not reclaimed until all of the snapshots are deleted as well.</p>"},{"location":"usage/baremetal/snapshotting/","title":"Snapshotting a Logical Volume","text":"<p>Snapshots in simplyblock provide point-in-time copies of logical Volumes (LVs), allowing for backup, recovery, or cloning operations without impacting the active workload. Snapshots can be created using the <code>sbctl</code> command line interface to protect critical data or enable development and testing environments based on production data.</p>"},{"location":"usage/baremetal/snapshotting/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running simplyblock cluster with an existing logical volume.</li> <li><code>sbctl</code> installed and configured with access to the simplyblock management API.</li> </ul>"},{"location":"usage/baremetal/snapshotting/#creating-a-snapshot","title":"Creating a Snapshot","text":"<p>To create a snapshot of an existing Logical Volume:</p> <pre><code>sbctl snapshot add \\\n  &lt;VOLUME_UUID&gt; \\\n  &lt;SNAPSHOT_NAME&gt;\n</code></pre>"},{"location":"usage/baremetal/snapshotting/#verification","title":"Verification","text":"<p>After creation, the snapshot can be listed:</p> <pre><code>sbctl snapshot list\n</code></pre>"},{"location":"usage/simplyblock-csi/","title":"Kubernetes CSI","text":"<p>Simplyblock integrates seamlessly with Kubernetes through its Container Storage Interface (CSI) driver, enabling dynamic provisioning and management of high-performance Logical Volumes (LVs) directly from Kubernetes workloads. This documentation section provides detailed guidance on how to handle the full lifecycle of simplyblock-backed volumes in Kubernetes, including provisioning, removal, expansion, snapshotting, cloning, and applying performance controls.</p> <p>By leveraging the Simplyblock CSI driver, administrators and developers can automate storage operations with standard Kubernetes objects, ensuring efficient and reliable storage for stateful workloads running on simplyblock clusters.</p>"},{"location":"usage/simplyblock-csi/cloning/","title":"Cloning","text":"<p>Kubernetes PersistentVolumes, backed by simplyblock, can be instantly cloned. A clone refers back to the same data, as simplyblock is a full copy-on-write storage engine. That enables instant database forks that act independently after cloning.</p>"},{"location":"usage/simplyblock-csi/cloning/#create-a-volume-clone","title":"Create a Volume Clone","text":"<p>To clone an existing PersistentVolume, it has to be named as the clone basis (dataSource) in the PersistentVolumeClaim Kubernetes resource.</p> PersistentVolumeClaim to clone an existing volume<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-persistent-volume-clone\nspec:\n  storageClassName: simplyblock-storage-class\n  dataSource:\n    name: original-persistent-volume-name # &lt;- Name of the original volume\n    kind: PersistentVolumeClaim\n    apiGroup: \"\"\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the cloned PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: cloned-database\n  labels:\n    app: cloned-database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-cloned-volume\n  volumes:\n  - name: my-cloned-volume\n    persistentVolumeClaim:\n      claimName: my-persistent-volume-clone\n</code></pre>"},{"location":"usage/simplyblock-csi/encrypting/","title":"Encrypting","text":"<p>Simplyblock supports encryption of logical volumes (LVs) to protect data at rest, ensuring that sensitive information remains secure across the distributed storage cluster. Encryption is applied during volume creation as part of the storage class specification.</p> <p>Encrypting Logical Volumes ensures that simplyblock storage meets data protection and compliance requirements, safeguarding sensitive workloads without compromising performance.</p> <p>Warning</p> <p>Encryption must be specified at the time of volume creation. Existing logical volumes cannot be retroactively encrypted.</p>"},{"location":"usage/simplyblock-csi/encrypting/#encrypting-volumes-with-simplyblock","title":"Encrypting Volumes with Simplyblock","text":"<p>Simplyblock supports the encryption of logical volumes. Internally, simplyblock utilizes the industry-proven crypto bdev\u00a0\u29c9 provided by SPDK to implement its encryption functionality.</p> <p>The encryption uses an AES_XTS variable-length block cipher. This cipher requires two keys of 16 to 32 bytes each. The keys need to have the same length, meaning that if one key is 32 bytes long, the other one has to be 32 bytes, too.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two keys of 32 bytes.</p>"},{"location":"usage/simplyblock-csi/encrypting/#generate-random-keys","title":"Generate Random Keys","text":"<p>Simplyblock does not provide an integrated way to generate encryption keys, but recommends using the OpenSSL tool chain. For Kubernetes, the encryption key needs to be provided as base64. Hence, it's encoded right away.</p> <p>To generate the two keys, the following command is run twice. The result must be stored for later.</p> Create an Encryption Key<pre><code>openssl rand -hex 32 | base64 -w0\n</code></pre>"},{"location":"usage/simplyblock-csi/encrypting/#create-the-kubernetes-secret","title":"Create the Kubernetes Secret","text":"<p>Next up, a Kubernetes Secret is created, providing the two just-created encryption keys.</p> Create a Kubernetes Secret Resource<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-encryption-keys\ndata:\n  crypto_key1: YzIzYzllY2I4MWJmYmY1ZDM5ZDA0NThjNWZlNzQwNjY2Y2RjZDViNWE4NTZkOTA5YmRmODFjM2UxM2FkZGU4Ngo=\n  crypto_key2: ZmFhMGFlMzZkNmIyODdhMjYxMzZhYWI3ZTcwZDEwZjBmYWJlMzYzMDRjNTBjYTY5Nzk2ZGRlZGJiMDMwMGJmNwo=\n</code></pre> <p>The Kubernetes Secret can be used for one or more logical volumes. Using different encryption keys, multiple tenants can be secured with an additional isolation layer against each other.</p>"},{"location":"usage/simplyblock-csi/encrypting/#storageclass-configuration","title":"StorageClass Configuration","text":"<p>A new Kubernetes StorageClass needs to be created, or an existing one needs to be configured. To use encryption on a persistent volume claim level, the storage class has to be set for encryption.</p> Example StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\" # This is important!\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"usage/simplyblock-csi/encrypting/#create-a-persistentvolumeclaim","title":"Create a PersistentVolumeClaim","text":"<p>When requesting a logical volume through a Kubernetes PersistentVolumeClaim, the storage class and the secret resources have to be connected to the PVC. When picked up, simplyblock will automatically collect the keys and create the logical volumes as a fully encrypted logical volume.</p> Create an encrypting PersistentVolumeClaim<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  annotations:\n    simplybk/secret-name: my-encryption-keys # Encryption keys\n  name: my-encrypted-volume-claim\nspec:\n  storageClassName: my-encrypted-volumes # StorageClass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 200Gi\n</code></pre>"},{"location":"usage/simplyblock-csi/expanding/","title":"Expanding","text":"<p>Expanding a Persistent Volume (PV) in Kubernetes allows for increasing the size of a volume without downtime, ensuring applications continue running with sufficient storage. Simplyblock supports online expansion of Logical Volumes (LVs) through its CSI driver, making it possible to resize volumes dynamically as storage requirements grow.</p> <p>Info</p> <p>To enable volume expansion, Kubernetes 1.16 or later is required.</p>"},{"location":"usage/simplyblock-csi/expanding/#enable-volume-expansion","title":"Enable Volume Expansion","text":"<p>To enable volume expansion, the StorageClass has to be configured accordingly. To enable volume expansion, the property <code>allowVolumeExpansion</code> has to be set to true.</p> Allowing volume expansion in StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\"\n  csi.storage.k8s.io/fstype: ext4\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true # &lt;- Enable volume expansion\n</code></pre>"},{"location":"usage/simplyblock-csi/expanding/#expand-a-persistentvolume","title":"Expand a PersistentVolume","text":"<p>To expand an existing volume, update the field <code>spec.resources.requests.storage</code> in the existing resource descriptor.</p> Updating the volume size<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-example-pvc\nspec:\n  resources:\n    requests:\n      storage: 500Gi # &lt;- Was 100Gi before\n</code></pre> <p>Then apply the change.</p> Apply resource update<pre><code>kubectl apply -f pvc.yaml\n</code></pre>"},{"location":"usage/simplyblock-csi/expanding/#resize-the-filesystem-if-required","title":"Resize the Filesystem (If Required)","text":"<p>Certain filesystems, such as ext4, may require growing the filesystem after the underlying volume has been expanded. This can usually be handled automatically by the CSI driver or may require running filesystem-specific commands within the pod.</p>"},{"location":"usage/simplyblock-csi/expanding/#shrinking-a-volume","title":"Shrinking a Volume","text":"<p>Theoretically, it is possible to shrink a volume. It can, however, create issues with certain filesystems. When a volume needs to be shrunk, it is recommended to create a snapshot and restore it onto a new volume.</p>"},{"location":"usage/simplyblock-csi/provisioning/","title":"Provisioning","text":"<p>Provisioning a new PersistentVolume using simplyblock's Kubernetes CSI driver integration requires at least one StorageClass to be set up.</p>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-new-volume","title":"Create a new Volume","text":"<p>To create a new persistent volume backed by simplyblock, requires a persistent volume claim with the correct storage class.</p> Create a new PersistentVolumeClaim<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-simplyblock-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n  storageClassName: simplyblock-storage-class     \n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: database\n  labels:\n    app: database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-volume\n  volumes:\n  - name: my-volume\n    persistentVolumeClaim:\n      claimName: my-simplyblock-volume\n</code></pre>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-volume-from-a-snapshot","title":"Create a Volume from a Snapshot","text":"<p>To create a new persistent volume claim from an existing snapshot, see the section about Restoring a Snapshot.</p>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-cloned-volume","title":"Create a cloned Volume","text":"<p>To create a new persistent volume claim from an existing and live volume, see the section about Cloning.</p>"},{"location":"usage/simplyblock-csi/provisioning/#static-provisioning","title":"Static Provisioning","text":"<p>Warning</p> <p>Simplyblock discourages the static provisioning of Kubernetes Persistent Volumes. Only do it if you know what you are doing. We highly recommend using the dynamic provisioning through the Simplyblock CSI driver.</p>"},{"location":"usage/simplyblock-csi/provisioning/#nvme-over-fabrics-target","title":"NVMe over Fabrics Target","text":"<p>To create the static persistent volume, the following values need to be known: - <code>model</code> - <code>nqn</code> - <code>lvol</code> - <code>targetAddr</code> - <code>targetPort</code> - <code>targetType</code> - Name of the logical volume</p> Staticly provisioned persistent volume: pv-static.yaml<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  annotations:\n    pv.kubernetes.io/provisioned-by: csi.simplyblock.io\n  finalizers:\n  - kubernetes.io/pv-protection\n  name: pv-static\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 256Mi\n  csi:\n    driver: csi.simplyblock.io\n    fsType: ext4\n    volumeAttributes:\n      # MODEL_NUMBER, set by the `nvmf_create_subsystem` method\n      model: aa481c21-26f8-4056-87fa-cd306f69a71e\n      # Subsystem NQN (ASCII), set by the `nvmf_create_subsystem` method\n      nqn: nqn.2020-04.io.spdk.csi:uuid:aa481c21-26f8-4056-87fa-cd306f69a71e\n      # The listen address to an NVMe-oF subsystemset, set by the `nvmf_subsystem_add_listener` method\n      targetAddr: 127.0.0.1\n      targetPort: \"4420\"\n      # transport type, TCP or RDMA\n      targetType: TCP\n    # volumeHandle should be same as lvol store name(uuid)\n    volumeHandle: aa481c21-26f8-4056-87fa-cd306f69a71e\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: spdkcsi-sc\n  volumeMode: Filesystem\n</code></pre> Example output of applying the statically persistent volume<pre><code>demo@demo ~&gt; kubectl create -f pv-static.yaml\npersistentvolume/pv-static created\n</code></pre> <p>Warning</p> <p>Simplyblock's CSI driver does not support logical volume deletion for static persistent volumes. Hence, <code>persistentVolumeReclaimPolicy</code> in persistent volume specification must be set to <code>Retain</code> to avoid persistent volume delete attempt in csi-provisioner.</p>"},{"location":"usage/simplyblock-csi/provisioning/#iscsi-target","title":"iSCSI Target","text":"<p>To create the static persistent volume, the following values need to be known:</p> <ul> <li><code>lvol</code></li> <li><code>targetAddr</code></li> <li><code>targetPort</code></li> <li>Name of the logical volume</li> </ul> Staticly provisioned persistent volume: pv-static.yaml<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  annotations:\n    pv.kubernetes.io/provisioned-by: csi.simplyblock.io\n  name: pv-static\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 256Mi\n  csi:\n    driver: csi.simplyblock.io\n    fsType: ext4\n    volumeAttributes:\n      # number Initiator group tag, the default value is `iqn.2016-06.io.spdk:`+ `volumeHandle`\n      iqn: iqn.2016-06.io.spdk:c0cd9559-cd6e-43b6-98af-45196e41655f\n      # iSCSI transport address, set by the `iscsi_create_portal_group` method\n      targetAddr: 127.0.0.1\n      targetPort: \"3260\"\n      targetType: iscsi\n    # volumeHandle should be same as lvol store name(uuid)\n    volumeHandle: c0cd9559-cd6e-43b6-98af-45196e41655f\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: spdkcsi-sc\n  volumeMode: Filesystem\n</code></pre> Example output of applying the statically persistent volume<pre><code>demo@demo ~&gt; kubectl create -f pv-static.yaml\npersistentvolume/pv-static created\n</code></pre> <p>Warning</p> <p>Simplyblock's CSI driver does not support logical volume deletion for static persistent volumes. Hence, <code>persistentVolumeReclaimPolicy</code> in persistent volume specification must be set to <code>Retain</code> to avoid persistent volume delete attempt in csi-provisioner.</p>"},{"location":"usage/simplyblock-csi/provisioning/#create-static-persistent-volume-claim","title":"Create static Persistent Volume Claim","text":"Staticly provisioned persistent volume claim: pvc-static.yaml<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: pvc-static\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n  # As a functional test, volumeName is same as PV name\n  volumeName: pv-static\n  storageClassName: spdkcsi-sc\n</code></pre> <pre><code>demo@demo ~&gt; kubectl create -f pvc-static.yaml\npersistentvolumeclaim/pvc-static created\n</code></pre>"},{"location":"usage/simplyblock-csi/quality-of-service/","title":"Defining Quality of Service","text":"<p>Simplyblock's Kubernetes CSI driver supports Quality of Service (QoS) to define minimum guaranteed performance characteristics of a logical volume.</p> <p>To define the QoS properties, create a StorageClass with the required parameters.</p> StorageClass with Quality of Service<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: qos-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  qos_rw_iops: 1000\n  qos_rw_mbytes: 125\n  qos_r_mbytes: 125\n  qos_w_mbytes: 125\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> <p>The available parameters are:</p> Parameter Name Value Type Description Optional Default qos_rw_iops int Defines the maximum IOPS reserved for a logical volume of this storage class. A zero (0) means no maximum. true 0 qos_rw_mbytes int Defines the maximum total throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no maximum. true 0 qos_r_mbytes int Defines the maximum read throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no maximum. true 0 qos_w_mbytes int Defines the maximum write throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no maximum. true 0"},{"location":"usage/simplyblock-csi/removing/","title":"Removing","text":"<p>A simplyblock-managed logical volume which is connected to a Kubernetes PersistentVolumeClaim is targeted to Kubernetes' automatic lifecycle management. Therefore, if the PVC is removed, the logical volume is removed as well.</p> <p>If the storage class is defined with a reclaim policy that keeps the volume around after its claim has been deleted, it has to be removed specifically.</p>"},{"location":"usage/simplyblock-csi/removing/#removing-a-persistent-volume","title":"Removing a Persistent Volume","text":"<p>When a Persistent Volume (PV) in Kubernetes has its reclaim policy set to Retain, deleting the associated Persistent Volume Claim (PVC) does not automatically delete the PV or its underlying storage. Instead, the PV enters a Released state, signaling that the PVC has been deleted, but the storage remains intact and requires manual cleanup. This reclaim policy is commonly used when manual review of data or explicit deprovisioning is required.</p>"},{"location":"usage/simplyblock-csi/removing/#steps-to-remove-a-retained-persistent-volume","title":"Steps to remove a retained persistent volume","text":"<p>If the PersistentVolumeClaim still exists, it has to be deleted first:</p> Removing a PersistentVolumeClaim<pre><code>kubectl delete pvc &lt;pvc-name&gt;\n</code></pre> <p>When the PVC is deleted, the PersistentVolume state must be checked. It should be released:</p> Check PersistentVolume status<pre><code>kubectl get pv\n</code></pre> <p>Now the PV can be deleted:</p> Delete a PersistentVolume<pre><code>kubectl get pv &lt;pv-name&gt;\n</code></pre> <p>Warning</p> <p>If snapshots or snapshot chains of the logical volume exist, the internal storage is not reclaimed until all of the snapshots are deleted as well.</p>"},{"location":"usage/simplyblock-csi/snapshotting/","title":"Snapshotting","text":"<p>Kubernetes PersistentVolumes backed by simplyblock can be instantly snapshotted. Snapshots are almost free due to simplyblock's copy-on-write nature.</p> <p>In simplyblock, a snapshot is comparable to the table of contents in a book, meaning that the snapshot refers to the same data as the original volume. If the volume diverges from the snapshot, the mutated data segment is duplicated, changed, and stored as a new data block. Now the volume refers to the new block, while the snapshot refers to the old one.</p> <p>A deeper explanation can be found here:</p> <p></p>"},{"location":"usage/simplyblock-csi/snapshotting/#snapshotting-a-persistentvolume","title":"Snapshotting a PersistentVolume","text":"<p>To snapshot a persistent volume, a new Kubernetes Snapshot resource is created. When applying the resource, the snapshot is taken immediately.</p> Creating a Snapshot resource<pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: my-volume-snapshot\nspec:\n  volumeSnapshotClassName: csi-spdk-snapclass\n  source:\n    persistentVolumeClaimName: my-persistent-volume-claim # &lt;- refers to the PVC to snapshot\n</code></pre>"},{"location":"usage/simplyblock-csi/snapshotting/#restore-a-volume-from-a-snapshot","title":"Restore a Volume from a Snapshot","text":"<p>After a snapshot was created, it can be used as a source (dataSource) of a new persistent volume. In this case, the new persistent volume claim refers to the snapshot, which is automatically restored into the new persistent volume.</p> Restoring a snapshot<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-restored-snapshot-volume\nspec:\n  storageClassName: simplyblock-storage-class\n  dataSource:\n    name: my-volume-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the restored PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: restored-database\n  labels:\n    app: restored-database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-restored-volume\n  volumes:\n  - name: my-restored-volume\n    persistentVolumeClaim:\n      claimName: my-restored-snapshot-volume\n</code></pre>"},{"location":"usage/simplyblock-csi/storage-class/","title":"Storage Class","text":"<p>A Kubernetes StorageClass defines the way dynamic storage provisioning is handled within a cluster. StorageClasses allow administrators to specify different types of storage with varying performance characteristics, redundancy configurations, and provisioning parameters. When a PersistentVolumeClaim (PVC) references a StorageClass, Kubernetes automatically provisions a Persistent Volume (PV) according to the defined specifications.</p>"},{"location":"usage/simplyblock-csi/storage-class/#how-simplyblock-uses-storageclass","title":"How Simplyblock Uses StorageClass","text":"<p>Simplyblock integrates with Kubernetes through its CSI (Container Storage Interface) driver and leverages StorageClasses to manage the dynamic provisioning of Logical Volumes (LVs). The simplyblock StorageClass defines how LVs are created within the simplyblock cluster, specifying parameters such as:</p> <ul> <li>Provisioning size</li> <li>Quality of Service (QoS)</li> <li>Encryption</li> </ul> <p>When a user deploys a PVC referencing the simplyblock StorageClass, the CSI driver automatically communicates with the simplyblock control plane to provision a logical volume matching the requested specifications. This process abstracts the complexity of volume creation and ensures that workloads running in Kubernetes receive high-performance, resilient block storage directly backed by simplyblock.</p>"},{"location":"usage/simplyblock-csi/storage-class/#example-usage","title":"Example Usage","text":"<p>A typical simplyblock StorageClass contains the name of the storage class, a filesystem type to automatically format the logical volume (or provide a raw block device if missing), the reclaim policy\u00a0\u29c9.</p> Example StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\"\n  csi.storage.k8s.io/fstype: ext4\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"usage/simplyblock-csi/storage-class/#available-parameters","title":"Available Parameters","text":"Parameter Name Value Type Description Optional Default csi.storage.k8s.io/fstype string Defines the filesystem to format the logical volume. If not specific, a raw block device is given to the container. true pool_name string Defines the simplyblock storage pool name to use. false testing1 qos_rw_iops int Defines the minimum IOPS reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_rw_mbytes int Defines the minimum total throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_r_mbytes int Defines the minimum read throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_w_mbytes int Defines the minimum write throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 compression bool Defines if the logical volume of this storage class will be stored compressed or not. true false encryption bool Defines if the logical volume of this storage class will be encrypted or not. true false distr_ndcs int ? true 1 distr_npcs int ? true 1 lvol_priority_class int Defines the priority class of a logical volume of this storage class. true 0 type string Defines the type of the logical volume. If set to <code>cache</code>, the logical volume will use a local caching node. true"},{"location":"usage/simplyblock-csi/trimming/","title":"Trimming a Filesystem","text":"<p>Filesystem trimming is the process of informing the underlying storage system about unused blocks, allowing simplyblock to reclaim and optimize storage space. This is particularly important when using thin-provisioned Logical Volumes (LVs) in Simplyblock, as it helps maintain efficient resource utilization and reduces unnecessary storage consumption over time.</p>"},{"location":"usage/simplyblock-csi/trimming/#when-to-trim","title":"When to Trim","text":"<p>Trimming should be performed:</p> <ul> <li>After large file deletions.</li> <li>As part of regular maintenance to keep storage optimized.</li> <li>Following data migration or cleanup tasks.</li> </ul>"},{"location":"usage/simplyblock-csi/trimming/#perform-a-trim","title":"Perform a Trim","text":"<p>Trimming must be executed within the filesystem inside the mounted volume. The specific command depends on the filesystem type. Below are common examples:</p> Trimming an ext4 filesystem<pre><code>fstrim -v /mount/point\n</code></pre> Trimming an xfs filesystem<pre><code>xfs_fsr -v /mount/point\n</code></pre>"}]}