{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-simplyblock-documentation","title":"Welcome to the Simplyblock Documentation","text":"<p>Welcome to the Simplyblock Documentation, your comprehensive resource for understanding, deploying, and managing simplyblock's cloud-native, high-performance storage platform. This documentation provides detailed information on architecture, installation, configuration, and best practices, ensuring you have the necessary guidance to maximize the efficiency and reliability of your simplyblock deployment.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Learn the basics</p> <p>General information about simplyblock, the documentation, and important terms. Read here first.</p> <p> Important Notes</p> </li> <li> <p> Plan the deployment</p> <p>Before start deploying simplyblock, take a moment to make yourself familiar with the required node sizing and other considerations for a performant and stable cluster operation.</p> <p> Deployment Planning</p> </li> <li> <p> Deploy Simplyblock</p> <p>Deploy simplyblock on Kubernetes, bare metal, or virtualized Linux machines. Choose between hyper-converged, disaggregated, or hybrid deployment models.</p> <p> Simplyblock Deployment</p> </li> <li> <p> Operate Simplyblock</p> <p>After installation of a simplyblock cluster, learn how to operate and maintain it.</p> <p> Simplyblock Usage  Simplyblock Operations</p> </li> </ul> <p>Subscribe to our Newsletter</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Simplyblock is a cloud-native, software-defined storage platform designed for high performance, scalability, and resilience. It provides NVMe over TCP (NVMe/TCP) block storage, enabling efficient data access across distributed environments. Understanding the architecture, key concepts, and common terminology is essential for effectively deploying and managing simplyblock in various infrastructure setups, including Kubernetes clusters, virtualized environments, and bare-metal deployments. This documentation aims to provide a comprehensive overview of simplyblock\u2019s internal architecture, the components that power it, and the best practices for integrating it into your storage infrastructure.</p> <p>This section covers several critical topics, including the architecture of simplyblock, core concepts such as Logical Volumes (LVs), Storage Nodes, and Management Nodes, as well as Quality of Service (QoS) mechanisms and redundancy strategies. Additionally, we define common terminology used throughout the documentation to ensure clarity and consistency. Readers will also find guidelines on document conventions, such as formatting, naming standards, and command syntax, which help maintain uniformity across all technical content.</p> <p>Simplyblock is an evolving platform, and community contributions play a vital role in improving its documentation. Whether you are a developer, storage administrator, or end user, your insights and feedback are valuable. This section provides details on how to contribute to the documentation, report issues, suggest improvements, and submit pull requests. By working together, we can ensure that simplyblock\u2019s documentation remains accurate, up-to-date, and beneficial for all users.</p>"},{"location":"architecture/high-availability-fault-tolerance/","title":"High Availability and Fault Tolerance","text":""},{"location":"architecture/simplyblock-architecture/","title":"Simplyblock Architecture","text":""},{"location":"architecture/simplyblock-architecture/#control-plane","title":"Control Plane","text":"<p>The control plane hosts the Simplyblock Management API and CLI endpoints with identical features. The CLI is equally available on all management nodes. The API and CLI are secured using HTTPS / TLS.</p> <p>The control plane provides the following functionality:</p> <ul> <li>Lifecycle management of clusters:<ul> <li>Deploy storage clusters</li> <li>Manages nodes and devices</li> <li>Resize and re-configure clusters</li> </ul> </li> <li>Lifecycle management of logical volumes and pools<ul> <li>For Kubernetes, the Simplyblock CSI driver integrates with the persistent volume lifecycle management</li> </ul> </li> <li>Cluster operations<ul> <li>I/O Statistics</li> <li>Capacity Statistics</li> <li>Alerts</li> <li>Logging</li> <li>others</li> </ul> </li> </ul> <p>The control plane also provides real-time collection and aggregation of io stats (performance, capacity, utilization), proactive cluster monitoring and health checks, monitoring dashboards, alerting, a log file repository with a management interface, data migration and automated node and device restart services.</p> <p>For monitoring dashboards and alerting, the simplyblock control plane provides Grafana and Prometheus. Both systems are configured to provide a set of standard alerts which can be delivered via Slack or email. Additionally, customers are free to define their own custom alerts.</p> <p>For log management, simplyblock uses Graylog. For a comprehensive insight, Graylog is configured to collect container logs from control plane and storage plane services, the RPC communication between the control plane and storage cluster and the data services logs (SPDK).</p>"},{"location":"architecture/simplyblock-architecture/#control-plane-state-storage","title":"Control Plane State Storage","text":"<p>The control plane is implemented as a stack of containers running on one or more management nodes. For production environments, simplyblock requires at least 3 management nodes for high availability. The management nodes run as  a set of replicated, stateful services.</p> <p>For internal state storage, the control plane uses (FoundationDB) as its key-value store. FoundationDB, by itself, operates in a replicated high-available cluster across all management nodes.</p>"},{"location":"architecture/simplyblock-architecture/#storage-plane","title":"Storage Plane","text":""},{"location":"architecture/what-is-simplyblock/","title":"What is Simplyblock?","text":"<p>Simplyblock is a high-performance, distributed storage orchestration layer designed for cloud-native environments. It provides NVMe over TCP (NVMe/TCP) block storage to hosts and offers block storage to containers through its Container Storage Interface (CSI) driver.</p>"},{"location":"architecture/what-is-simplyblock/#what-makes-simplyblock-special","title":"What makes Simplyblock Special?","text":"<ul> <li> <p>Environment Agnostic: Simplyblock operates seamlessly across major cloud providers, regional and specialized   providers, bare-metal provisioners, and private clouds, including both virtualized and bare-metal Kubernetes   environments.</p> </li> <li> <p>Containerized Architecture: The solution comprises:</p> <ul> <li>Storage Nodes: Container stacks delivering distributed data services via NVMe over Fabrics (NVMe over TCP),   forming storage clusters.</li> <li>Management Nodes: Container stacks offering control and management services, collectively known as the control   plane.</li> </ul> </li> <li> <p>Platform Support: Simplyblock supports deployment on virtual machines, bare-metal instances, and Kubernetes   containers, compatible with x86 and ARM architectures.</p> </li> <li> <p>Deployment Flexibility: Simplyblock offers the greatest deployment flexibility in the industry. It can be deployed   hyper-converged, disaggregated, and in a hybrid fashion, combining the best of both worlds.</p> </li> </ul>"},{"location":"architecture/what-is-simplyblock/#customer-benefits-across-industries","title":"Customer Benefits Across Industries","text":"<p>Simplyblock offers tailored advantages to various sectors:</p> <ul> <li> <p>Financial Services: Enhances data management by boosting performance, strengthening security, and optimizing cloud   storage costs.</p> </li> <li> <p>Media and Gaming: Improves storage performance, reduces costs, and streamlines data management, facilitating   efficient handling of large media files and gaming data.</p> </li> <li> <p>Technology and SaaS Companies: Provides cost savings and performance enhancements, simplifying storage management   and improving application performance without significant infrastructure changes.</p> </li> <li> <p>Telecommunications: Offers ultra-low latency access to data, enhances security, and simplifies complex storage   infrastructures, aiding in the efficient management of customer records and network telemetry.</p> </li> <li> <p>Blockchain and Cryptocurrency: Delivers cost efficiency, enhanced performance, scalability, and robust data   security, addressing the unique storage demands of blockchain networks.</p> </li> </ul>"},{"location":"architecture/concepts/","title":"Concepts","text":"<p>Understanding the fundamental concepts behind simplyblock is essential for effectively utilizing its distributed storage architecture. Simplyblock provides a cloud-native, software-defined storage solution that enables highly scalable, high-performance storage for containerized and virtualized environments. By leveraging NVMe over TCP (NVMe/TCP) and advanced data management features, simplyblock ensures low-latency access, high availability, and seamless scalability. This documentation section provides detailed explanations of key storage concepts within simplyblock, helping users grasp how its storage components function and interact within a distributed system.</p> <p>The concepts covered in this section include Logical Volumes (LVs), Snapshots, Clones, Hyper-Convergence, Disaggregation, and more. Each concept plays a crucial role in optimizing storage performance, ensuring data durability, and enabling efficient resource allocation. Whether you are deploying simplyblock in a Kubernetes environment, a virtualized infrastructure, or a bare-metal setup, understanding these core principles will help you design, configure, and manage your storage clusters effectively.</p> <p>By familiarizing yourself with these concepts, you will gain insight into how simplyblock abstracts storage resources, provides scalable and resilient data services and integrates with modern cloud-native environments. This knowledge is essential for leveraging simplyblock to meet your organization's storage performance, reliability, and scalability requirements.</p>"},{"location":"architecture/concepts/automatic-rebalancing/","title":"Automatic Rebalancing","text":"<p>Automatic rebalancing is a fundamental feature of distributed data storage systems designed to maintain an even distribution of data across storage nodes. This process ensures optimal performance, prevents resource overutilization, and enhances system resilience by dynamically redistributing data in response to changes in cluster topology or workload patterns.</p> <p>In a distributed storage system, data is typically spread across multiple storage nodes for redundancy, scalability, and performance. Over time, various factors can lead to an imbalance in data distribution, such as:</p> <ul> <li>The addition of new storage nodes, which initially lack any data.</li> <li>The removal or failure of existing nodes, requiring data redistribution to maintain availability.</li> <li>The equal distribution of data across storage nodes.</li> </ul> <p>Automatic rebalancing addresses these issues by dynamically redistributing data across the cluster. This process is driven by an algorithm that continuously monitors data distribution and redistributes data when imbalances are detected. The goal is to achieve uniform data placement while minimizing performance overhead during the rebalancing process.</p>"},{"location":"architecture/concepts/disaggregated/","title":"Disaggregated","text":"<p>Disaggregated storage represents a modern approach to distributed storage architectures, where compute and storage resources are decoupled. This separation allows for greater flexibility, scalability, and efficiency in managing data across large-scale distributed environments.</p> <p>Traditional storage architectures typically integrate compute and storage within the same nodes, leading to resource contention and inefficiencies. Disaggregated storage solutions address these limitations by separating storage resources from compute resources, enabling independent scaling of each component based on workload demands.</p> <p>Key characteristics of disaggregated storage solutions include:</p> <ul> <li>Independent Scalability: Compute and storage can be scaled separately, optimizing resource utilization and   reducing unnecessary hardware expansion.</li> <li>Resource Efficiency: Storage is pooled and accessible across multiple compute nodes, reducing data duplication and   improving overall efficiency.</li> <li>Improved Performance: By reducing bottlenecks associated with tightly coupled storage, applications can achieve   better latency and throughput.</li> <li>Flexibility and Adaptability: Different storage technologies (e.g., NVMe-over-Fabrics, object storage) can be   integrated seamlessly, allowing organizations to adopt the best-fit storage solutions for specific workloads.</li> <li>Simplified Management: Centralized storage management reduces complexity, enabling easier provisioning,   monitoring, and maintenance of storage resources.</li> </ul>"},{"location":"architecture/concepts/erasure-coding/","title":"Erasure Coding","text":"<p>Erasure coding is a data protection mechanism used in distributed storage systems to enhance fault tolerance and optimize storage efficiency. It provides redundancy by dividing data into multiple fragments and encoding it with additional parity fragments, enabling data recovery in the event of node failures.</p> <p>Traditional data redundancy methods, such as replication, require multiple full copies of data, leading to significant storage overhead. Erasure coding improves upon this by using mathematical algorithms to generate parity fragments that allow data reconstruction with fewer overheads.</p> <p>The core principle of erasure coding involves breaking data into k data fragments and computing m parity fragments. These k+m fragments are distributed across multiple storage nodes. The system can recover lost data using any k available fragments, even if up to m fragments are missing or corrupted.</p> <p>Erasure coding has a number of key characteristics:</p> <ul> <li>High Fault Tolerance: Erasure coding can tolerate multiple node failures while still allowing full data recovery.</li> <li>Storage Efficiency: Compared to replication, erasure coding requires less additional storage to achieve similar   levels of redundancy.</li> <li>Computational Overhead: Encoding and decoding operations involve computational complexity, which may impact   performance in latency-sensitive applications.</li> <li>Flexibility: The parameters k and m can be adjusted to balance redundancy, performance, and storage   overhead.</li> </ul>"},{"location":"architecture/concepts/hyper-converged/","title":"Hyper-Converged","text":"<p>Hyper-converged storage is a key component of hyper-converged infrastructure (HCI), where compute, storage, and networking resources are tightly integrated into a unified system. This approach simplifies management, enhances scalability, and optimizes resource utilization in distributed data storage environments.</p> <p>Traditional storage architectures often separate compute and storage into distinct hardware layers, requiring complex management and specialized hardware. Hyper-converged storage consolidates these resources within the same nodes, forming a software-defined storage (SDS) layer that dynamically distributes and manages data across the cluster.</p> <p>Key characteristics of hyper-converged storage include:</p> <ul> <li>Integrated Storage and Compute: Storage resources are virtualized and distributed across the compute nodes,   eliminating the need for dedicated storage arrays.</li> <li>Scalability: New nodes can be added seamlessly, increasing both compute and storage capacity without complex   reconfiguration.</li> <li>Software-Defined Storage (SDS): A software layer abstracts and manages storage resources, enabling automation,   fault tolerance, and efficiency.</li> <li>High Availability and Resilience: Data is replicated across nodes to ensure redundancy and fault tolerance,   minimizing downtime.</li> <li>Simplified Management: A unified management interface enables streamlined provisioning, monitoring, and   maintenance of storage and compute resources.</li> </ul>"},{"location":"architecture/concepts/logical-volumes/","title":"Logical Volumes","text":"<p>Logical Volumes (LVs) in Simplyblock are virtual NVMe devices that provide scalable, high-performance storage within a distributed storage cluster. They enable flexible storage allocation, efficient resource utilization, and seamless data management for cloud-native applications.</p> <p>A Logical Volume (LV) in simplyblock is an abstracted storage entity dynamically allocated from a storage pool managed by the simplyblock system. Unlike traditional block storage, simplyblock\u2019s LVs offer advanced features such as thin provisioning, snapshotting, and replication to enhance resilience and scalability.</p> <p>Key characteristics of Logical Volumes include:</p> <ul> <li>Dynamic Allocation: LVs can be created, resized, and deleted on demand without manual intervention in the   underlying hardware.</li> <li>Thin Provisioning: Storage space is allocated only when needed, optimizing resource utilization.</li> <li>High Performance: Simplyblock\u2019s architecture ensures low-latency access to LVs, making them suitable for demanding   workloads.</li> <li>Fault Tolerance: Data is distributed across multiple nodes to prevent data loss and improve reliability.</li> <li>Integration with Kubernetes: LVs can be used as persistent storage for Kubernetes workloads, enabling seamless   stateful application management.</li> </ul>"},{"location":"architecture/concepts/persistent-volumes/","title":"Persistent Volumes","text":"<p>Persistent Volumes (PVs) in Kubernetes provide a mechanism for managing storage resources independently of individual Pods. Unlike ephemeral storage, which is tied to the lifecycle of a Pod, PVs ensure data persistence across Pod restarts and rescheduling, enabling stateful applications to function reliably in a Kubernetes cluster.</p> <p>In Kubernetes, storage resources are abstracted through the Persistent Volume framework, which decouples storage provisioning from application deployment. A Persistent Volume (PV) represents a piece of storage that has been provisioned in the cluster, while a Persistent Volume Claim (PVC) is a request for storage made by an application.</p> <p>Key characteristics of Persistent Volumes include:</p> <ul> <li>Decoupled Storage Management: PVs exist independently of Pods, allowing storage to persist even when Pods are   deleted or rescheduled.</li> <li>Dynamic and Static Provisioning: Storage can be provisioned manually by administrators (static provisioning) or   automatically by storage classes (dynamic provisioning).</li> <li>Access Modes: PVs support multiple access modes, such as ReadWriteOnce (RWO), ReadOnlyMany (ROX), and   ReadWriteMany (RWX), defining how storage can be accessed by Pods.</li> <li>Reclaim Policies: When a PV is no longer needed, it can be retained, recycled, or deleted based on its configured   reclaim policy.</li> <li>Storage Classes: Kubernetes allows administrators to define different types of storage using StorageClasses,   enabling automated provisioning of PVs based on workload requirements.</li> </ul>"},{"location":"architecture/concepts/simplyblock-cluster/","title":"Simplyblock Cluster","text":"<p>The simplyblock storage platform consists of three different types of cluster nodes and belong to the control plane or storage plane.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#control-plane","title":"Control Plane","text":"<p>The control plane orchestrates, monitors, and controls the overall storage infrastructure. It provides centralized administration, policy enforcement, and automation for managing storage nodes, logical volumes (LVs), and cluster-wide configurations. The control plane operates independently of the storage plane, ensuring that control and metadata operations do not interfere with data processing. It facilitates provisioning, fault management, and system scaling while offering APIs and CLI tools for seamless integration with external management systems. A single control plane can manage multiple clusters.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#storage-plane","title":"Storage Plane","text":"<p>The storage plane is the layer responsible for managing and distributing data across storage nodes within a cluster. It handles data placement, replication, fault tolerance, and access control, ensuring that logical volumes (LVs) provide high-performance, low-latency storage to applications. The storage plane operates independently of the control plane, allowing seamless scalability and dynamic resource allocation without disrupting system operations. By leveraging NVMe-over-TCP and software-defined storage principles, simplyblock\u2019s storage plane ensures efficient data distribution, high availability, and resilience, making it ideal for cloud-native and high-performance computing environments.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#management-node","title":"Management Node","text":"<p>A management node is a node of the control plane cluster. The management node runs the necessary management services including the Cluster API, services such as Grafana, Prometheus, and Graylog, as well as the FoundationDB database cluster.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#storage-node","title":"Storage Node","text":"<p>A storage node is a node of the storage plane cluster. The storage node provides storage capacity to the distributed storage pool of a specific storage cluster. The storage node runs the necessary data management services including the Storage Node Management API, the SPDK service, and handles logical volume primary connections of NVMe-oF multipathing.</p>"},{"location":"architecture/concepts/simplyblock-cluster/#secondary-node","title":"Secondary Node","text":"<p>A secondary node is a node of the storage plane cluster. The secondary node provides automatic fail over and high availability for logical volumes using NVMe-oF multipathing. Simplyblock requires additional secondary nodes for a fully high available cluster to minimize performance impact in case of a storage node failure.</p>"},{"location":"architecture/concepts/snapshots-clones/","title":"Snapshots and Clones","text":"<p>Volume snapshots and volume clones are essential data management features in distributed storage systems that enable data protection, recovery, and replication. While both techniques involve capturing the state of a volume at a specific point in time, they serve distinct purposes and operate using different mechanisms.</p>"},{"location":"architecture/concepts/snapshots-clones/#volume-snapshots","title":"Volume Snapshots","text":"<p>A volume snapshot is a read-only, point-in-time copy of a storage volume. It preserves the state of the volume at the moment the snapshot is taken, allowing users to restore data or create new volumes based on the captured state. Snapshots are typically implemented using copy-on-write (COW) or redirect-on-write (ROW) techniques, minimizing storage overhead and improving efficiency.</p> <p>Key characteristics of volume snapshots include:</p> <ul> <li>Space Efficiency: Snapshots share common data blocks with the original volume, requiring minimal additional   storage.</li> <li>Fast Creation: As snapshots do not duplicate data immediately, they can be created almost instantaneously.</li> <li>Versioning and Recovery: Users can revert a volume to a previous state using snapshots, aiding in disaster   recovery and data protection.</li> <li>Performance Considerations: While snapshots are efficient, excessive snapshot accumulation can impact performance   due to metadata overhead and fragmentation.</li> </ul>"},{"location":"architecture/concepts/snapshots-clones/#volume-clones","title":"Volume Clones","text":"<p>A volume clone is a writable, independent copy of a storage volume, created from either an existing volume or a snapshot. Unlike snapshots, clones are fully functional duplicates that can operate as separate storage entities.</p> <p>Key characteristics of volume clones include:</p> <ul> <li>Writable and Independent: Clones can be modified without affecting the original volume.</li> <li>Use Case for Testing and Development: Clones are commonly used for staging environments, testing, and application   sandboxing.</li> <li>Storage Overhead: Unlike snapshots, clones may require additional storage capacity to accommodate changes made   after cloning.</li> <li>Immediate Availability: A clone provides an instant copy of the original volume, avoiding long data copying   processes.</li> </ul>"},{"location":"architecture/concepts/storage-pooling/","title":"Storage Pooling","text":"<p>Storage pooling is a technique used in distributed data storage systems to aggregate multiple storage devices into a single, unified storage resource. This approach enhances resource utilization, improves scalability, and simplifies management by abstracting physical storage infrastructure into a logical storage pool.</p> <p>Traditional storage architectures often rely on dedicated storage devices assigned to specific applications or workloads, leading to inefficiencies in resource allocation and potential underutilization. Storage pooling addresses these challenges by combining storage resources from multiple nodes into a shared pool, allowing dynamic allocation based on demand.</p> <p>Key characteristics of storage pooling include:</p> <ul> <li>Resource Aggregation: Multiple physical storage devices, such as HDDs, SSDs, or NVMe drives, are combined into a   single logical storage entity.</li> <li>Dynamic Allocation: Storage capacity can be allocated dynamically to workloads based on usage patterns and demand.</li> <li>Improved Efficiency: By eliminating the constraints of static storage assignments, storage pooling optimizes   resource utilization and reduces wasted capacity.</li> <li>Scalability: Additional storage devices or nodes can be seamlessly integrated into the storage pool without   disrupting operations.</li> <li>Simplified Management: Centralized control and monitoring enable streamlined administration of storage resources.</li> </ul>"},{"location":"deployments/","title":"Deployments","text":"<p>Simplyblock is a highly flexible storage solution. It can be installed in a variety of different deployment models inside and outside of Kubernetes.</p> <ul> <li> <p> Kubernetes</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Hyper-Converged Setup  Disaggregated Setup  Hybrid Setup</p> </li> <li> <p> Bare Metal Linux</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Install Storage Cluster  Install Kubernetes CSI Driver</p> </li> <li> <p> AWS EC2</p> <p> Node Sizing  Erasure Coding Configuration  Prerequisites  Install Storage Cluster  Install Kubernetes CSI Driver</p> </li> <li> <p> Air Gapped</p> <p> General Information</p> </li> </ul>"},{"location":"deployments/air-gap/","title":"Air Gap Installation","text":"<p>Simplyblock can be installed in an air-gapped environment, however, the necessary images have to be downloaded to install and run the control plane, the storage nodes, and the Kubernetes CSI driver. In addition, for Kubernetes deployments, you want to download or clone the  simplyblock helm repository which contains the helm charts for Kubernetes-based storage and caching nodes, as well as the Kubernetes CSI driver.</p> <p>For an air-gapped installation, we recommend an air-gapped container repository installation. Tools such as JFrog Artifactory or Sonatype Nexus helps with the setup and management of container images in air-gapped environments.</p> <p>The general installation instructions are similar to non-air-gapped installations, with the need to update the container download locations to point to your local container repository.</p> <p>Learn more about the deployment options:</p> <ul> <li>Deploy into Kubernetes</li> <li>Deploy on Bare Metal (or VM) Linux</li> <li>Deploy on AWS EC2</li> </ul>"},{"location":"deployments/aws-ec2/","title":"AWS EC2 (Amazon Linux 2)","text":"<p>An installation on Amazon EC2 is mostly comparable with a bare metal installation. There are however a few small differences. Hence, we decided to give it a separate documentation section.</p> <p>An installation on Amazon EKS is comparable with a standard Kubernetes installation. The differences between an Amazon EKS installation and a basic Kubernetes</p> <p>Warning</p> <p>Amazon Linux 2 does not support NVMe over Fabrics Multipathing right now!</p> <ul> <li> <p> Prerequisites</p> <p> Node Sizing  Prerequisites</p> </li> <li> <p> Simplyblock Installation</p> <p> Install Simplyblock</p> </li> <li> <p> Kubernetes CSI Driver Installation</p> <p> Install Kubernetes CSI Driver</p> </li> <li> <p> Caching Node Installation</p> <p> Install Caching Nodes</p> </li> </ul>"},{"location":"deployments/aws-ec2/data-migration/","title":"Data Migration","text":""},{"location":"deployments/aws-ec2/install-caching-nodes/","title":"Install Caching Nodes (Kubernetes)","text":""},{"location":"deployments/aws-ec2/install-simplyblock-csi/","title":"Install Simplyblock CSI","text":""},{"location":"deployments/aws-ec2/install-simplyblock/","title":"Install Simplyblock Storage Cluster","text":"<p>Installing simplyblock for production, requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides a test script to automatically check your system's configuration. While it may not catch all edge cases, it can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configuration during the process.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> <li>Secondary nodes are part of a specific storage cluster and enable automatic fail over for NVMe-oF connections. A production cluster requires at least one node.</li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, as well as the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separated subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24 <p>Warning</p> <p>Simplyblock strongly recommends to set up individual networks for the storage plane and control plane traffic.  </p>"},{"location":"deployments/aws-ec2/install-simplyblock/#amazon-elastic-kubernetes-service-eks","title":"Amazon Elastic Kubernetes Service (EKS)","text":"<p>Info</p> <p>If simplyblock is to be installed into Amazon EKS, the Kubernetes documentation section has the necessary step-by-step guide.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbcli cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbcli cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> <pre><code>sbcli cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster, requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbcli</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbcli mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbcli mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets to be available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) bdts ingress storage 4420 TCP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080-8890 TCP lvol-proxy ingress storage, control 9090-9900 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/aws-ec2/install-simplyblock/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes, before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>Once the check is complete, the NVMe devices in each storage node can be prepared. To prevent data loss in case of a sudden power outage, NVMe devices need to be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> cli can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends to ask for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executing similar to the below example.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre> <p>With all NVMe devices prepared, the storage node software can be deployed.</p> Deploy the storage node<pre><code>sudo sbcli sn deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbcli sn deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#secondary-node-installation","title":"Secondary Node Installation","text":"<p>A secondary node is a storage node without additional storage disks to contribute to the distributed storage pool. Apart from that, it is the same as a normal storage node.</p> <p>However, due to the missing storage devices, preparing a secondary node on requires deploying the storage node software.</p> Deploy the secondary node<pre><code>sudo sbcli sn deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a secondary node deployment<pre><code>[demo@demo-4 ~]# sudo sbcli sn deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.4\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.4:5000\n</code></pre> <p>On a successful deployment, the last line will provide the secondary node's control channel address. This should be noted, as it is required in the next step to attach the secondary node to the simplyblock storage cluster.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#attach-the-storage-node-to-the-control-plane","title":"Attach the Storage Node to the Control Plane","text":"<p>When all storage nodes are prepared, they can be added to the storage cluster.</p> <p>Warning</p> <p>The following command are executed from a management node. Attaching a storage node to a control plane is executed from a management node.</p> Attaching a storage node to the storage plane<pre><code>sudo sbcli sn add-node &lt;CLUSTER_ID&gt; &lt;SN_CTR_ADDR&gt; &lt;MGT_IF&gt; \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-prov &lt;MAX_PROVISIONING_CAPACITY&gt; \\\n  --number-of-devices &lt;NUM_STOR_NVME&gt; \\\n  --partitions &lt;NUM_OF_PARTITIONS&gt; \\\n  --data-nics &lt;DATA_IF&gt;\n</code></pre> <p>Info</p> <p>The number of partitions (&lt;NUM_OF_PARTITIONS&gt;) depends on the storage node setup. If a storage node has a separate journaling device (which is strongly recommended), the value should be zero (0) to prevent the storage devices to be partitioned. This improves the performance and prevents device sharing between the journal and actual data storage location.</p> <p>The output will look something like the following example:</p> Example output of adding a storage node to the storage plane<pre><code>[demo@demo ~]# sudo sbcli sn add-node 7bef076c-82b7-46a5-9f30-8c938b30e655 192.168.10.2:5000 eth0 --max-lvol 50 --max-prov 500g --number-of-devices 3 --partitions 0 --data-nics eth1\n2025-02-26 14:55:17,236: INFO: Adding Storage node: 192.168.10.2:5000\n2025-02-26 14:55:17,340: INFO: Instance id: 0b0c825e-3d16-4d91-a237-51e55c6ffefe\n2025-02-26 14:55:17,341: INFO: Instance cloud: None\n2025-02-26 14:55:17,341: INFO: Instance type: None\n2025-02-26 14:55:17,342: INFO: Instance privateIp: 192.168.10.2\n2025-02-26 14:55:17,342: INFO: Instance public_ip: 192.168.10.2\n2025-02-26 14:55:17,347: INFO: Node Memory info\n2025-02-26 14:55:17,347: INFO: Total: 24.3 GB\n2025-02-26 14:55:17,348: INFO: Free: 23.2 GB\n2025-02-26 14:55:17,348: INFO: Minimum required huge pages memory is : 14.8 GB\n2025-02-26 14:55:17,349: INFO: Joining docker swarm...\n2025-02-26 14:55:21,060: INFO: Deploying SPDK\n2025-02-26 14:55:31,969: INFO: adding alceml_2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,010: INFO: creating subsystem nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,022: INFO: adding listener for nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4 on IP 10.10.10.2\n2025-02-26 14:55:32,303: INFO: Connecting to remote devices\n2025-02-26 14:55:32,321: INFO: Connecting to remote JMs\n2025-02-26 14:55:32,342: INFO: Make other nodes connect to the new devices\n2025-02-26 14:55:32,346: INFO: Setting node status to Active\n2025-02-26 14:55:32,357: INFO: {\"cluster_id\": \"3196b77c-e6ee-46c3-8291-736debfe2472\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"StorageNode\", \"message\": \"Storage node status changed from: in_creation to: online\", \"caused_by\": \"monitor\"}\n2025-02-26 14:55:32,361: INFO: Sending event updates, node: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5, status: online\n2025-02-26 14:55:32,368: INFO: Sending to: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5\n2025-02-26 14:55:32,389: INFO: Connecting to remote devices\n2025-02-26 14:55:32,442: WARNING: The cluster status is not active (unready), adding the node without distribs and lvstore\n2025-02-26 14:55:32,443: INFO: Done\n</code></pre> <p>Repeat this process for all prepared storage nodes to add them to the storage plane.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#attach-the-secondary-node-to-the-control-plane","title":"Attach the Secondary Node to the Control Plane","text":"<p>Afterward, the secondary node needs to be added to the cluster.</p> Attaching a secondary node to the storage plane<pre><code>sudo sbcli sn add-node &lt;CLUSTER_ID&gt; &lt;SN_CTR_ADDR&gt; &lt;MGT_IF&gt; \\\n  --data-nics &lt;DATA_IF&gt;\n  --is-secondary-node\n</code></pre> <p>The output will look something like the following example:</p> Example output of a secondary node to the storage plane<pre><code>[demo@demo ~]# sudo sbcli sn add-node 7bef076c-82b7-46a5-9f30-8c938b30e655 192.168.10.5:5000 ens18 --data-nics=ens16 --is-secondary-node\n2025-02-28 13:34:57,877: INFO: Adding Storage node: 192.168.10.115:5000\n2025-02-28 13:34:57,952: INFO: Node found: vm5\n2025-02-28 13:34:57,953: INFO: Instance id: 5d679365-1361-40b0-bac0-3de949057bbc\n2025-02-28 13:34:57,953: INFO: Instance cloud: None\n2025-02-28 13:34:57,954: INFO: Instance type: None\n2025-02-28 13:34:57,954: INFO: Instance privateIp: 192.168.10.5\n2025-02-28 13:34:57,955: INFO: Instance public_ip: 192.168.10.5\n2025-02-28 13:34:57,977: WARNING: Unsupported instance-type None for deployment\n2025-02-28 13:34:57,977: INFO: Node Memory info\n...\n025-02-28 13:35:08,068: INFO: Connecting to remote devices\n2025-02-28 13:35:08,111: INFO: Connecting to node 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:08,111: INFO: bdev found remote_alceml_378cf3b5-1959-4415-87bf-392fa1bbed6c_qosn1\n2025-02-28 13:35:08,112: INFO: bdev found remote_alceml_c4c4011a-8f82-4c9d-8349-b4023a20b87c_qosn1\n2025-02-28 13:35:08,112: INFO: bdev found remote_alceml_d27388b9-bbd8-4e82-8880-d8811aa45383_qosn1\n2025-02-28 13:35:08,113: INFO: Connecting to node b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:08,113: INFO: bdev found remote_alceml_7f5ade89-53c6-440b-9614-ec24db3afbd9_qosn1\n2025-02-28 13:35:08,114: INFO: bdev found remote_alceml_8d160125-f095-43ae-9781-16d841ae9719_qosn1\n2025-02-28 13:35:08,114: INFO: bdev found remote_alceml_b0691372-1a4b-4fa9-a805-c2c1f311541c_qosn1\n2025-02-28 13:35:08,114: INFO: Connecting to node 43560b0a-f966-405f-b27a-2c571a2bb4eb\n2025-02-28 13:35:08,115: INFO: bdev found remote_alceml_29e74188-5efa-47d9-9282-84b4e46b77db_qosn1\n2025-02-28 13:35:08,115: INFO: bdev found remote_alceml_a1efcbbf-328c-4f86-859f-fcfceae1c7a8_qosn1\n2025-02-28 13:35:08,116: INFO: bdev found remote_alceml_cf2d3d24-e244-4a45-a71d-6383db07806f_qosn1\n2025-02-28 13:35:08,274: WARNING: The cluster status is not active (unready), adding the node without distribs and lvstore\n2025-02-28 13:35:08,274: INFO: Done\n</code></pre> <p>On a successful response, it's finally time to activate the storage plane.</p>"},{"location":"deployments/aws-ec2/install-simplyblock/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step after all nodes are added to the storage cluster, the storage plane can be activated.</p> Storage cluster activation<pre><code>sudo sbcli cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbcli cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre> <p>Now that the cluster is ready, it is time to install the Kubernetes CSI Driver or learn how to use the simplyblock storage cluster to manually provision logical volumes.</p>"},{"location":"deployments/aws-ec2/prerequisites/","title":"Prerequisites","text":"<p>When installing simplyblock control planes and storage planes, a number of prerequisites are important to understand.</p> <p>Simplyblock uses Docker Swarm for the control plane cluster. In case of a bare metal installation, it will also use Docker Swarm for the storage plane. Hence, Docker has to be installed.</p> <p>Furthermore, simplyblock requires the installation of the <code>sbcli</code> command line tool. This tool is written in Python. Therefore, Python (3.5 or later) has to be installed. Likewise, pip, the Python package manager, has to be installed with version 20 or later.</p> <p>To install <code>sbcli</code> run:</p> <pre><code>sudo pip install sbcli --upgrade\n</code></pre>"},{"location":"deployments/aws-ec2/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/aws-ec2/prerequisites/#network-configuration","title":"Network Configuration","text":"<p>Simplyblock requires a number of network ports to be available from different networks. The configuration of the required network ports is provided in the installation documentation.</p> <p>Additionally, IPv6 must be disabled on all nodes running simplyblock.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"},{"location":"deployments/aws-ec2/prerequisites/#aws-networks","title":"AWS Networks","text":"<p>Specifically for AWS, simplyblock strong advises to use individual networks for the control plane and storage plane.</p> <p>For access to the Cluster Management API, simplyblock recommends using an AWS load balancer as a front instead of making the API available directly.</p> <p>Warning</p> <p>Amazon Linux 2 does not support NVMe over Fabrics Multipathing right now!</p>"},{"location":"deployments/aws-ec2/prerequisites/#network-ports-for-control-plane","title":"Network Ports for Control Plane","text":"Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP"},{"location":"deployments/aws-ec2/prerequisites/#network-ports-for-storage-plane","title":"Network Ports for Storage Plane","text":"Service Direction Source / Target Network Port(s) Protocol(s) bdts ingress storage 4420 TCP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080-8890 TCP lvol-proxy ingress storage, control 9090-9900 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP"},{"location":"deployments/aws-ec2/prerequisites/#storage-configuration","title":"Storage Configuration","text":"<p>Simplyblock has certain requirements in terms of storage. While the most important facts are provided in the installation section, here are things to consider.</p>"},{"location":"deployments/aws-ec2/prerequisites/#root-volume","title":"Root Volume","text":"<p>The volume mounted as the root directory has to provide at least 35GiB of free capacity. More free space is recommended, especially for control plane nodes which collect logs and the cluster state.</p>"},{"location":"deployments/aws-ec2/prerequisites/#nvme-devices","title":"NVMe Devices","text":"<p>NVMe devices used for simplyblock, should ALWAYS be formatted using the <code>nvme</code> command line tool before adding them to a simplyblock storage node. Failing to do so can negatively impact storage performance and lead to data corruption or even data loss in case of a sudden power outage.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> cli can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends to ask for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executing similar to the below example.</p> Example output of NVMe device formatting<pre><code>[demo@demo ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/baremetal/","title":"Bare Metal Linux","text":"<p>Bare metal installations are installations on physically dedicated hosts, or virtual machines. These physical or virtual machines provide a Red Hat-based Linux installation.</p> <ul> <li> <p> Prerequisites</p> <p> Node Sizing  Prerequisites</p> </li> <li> <p> Simplyblock Installation</p> <p> Install Simplyblock</p> </li> <li> <p> Kubernetes CSI Driver Installation</p> <p> Install Kubernetes CSI Driver</p> </li> <li> <p> Caching Node Installation</p> <p> Install Caching Nodes</p> </li> </ul>"},{"location":"deployments/baremetal/data-migration/","title":"Data Migration","text":""},{"location":"deployments/baremetal/install-caching-nodes/","title":"Install Caching Nodes (Kubernetes)","text":""},{"location":"deployments/baremetal/install-simplyblock-csi/","title":"Install Simplyblock CSI","text":""},{"location":"deployments/baremetal/install-simplyblock/","title":"Install Simplyblock Storage Cluster","text":"<p>Installing simplyblock for production, requires a few components to be installed, as well as a couple of configurations to secure the network, ensure the performance and data protection in the case of hardware or software failures.</p> <p>Simplyblock provides a test script to automatically check your system's configuration. While it may not catch all edge cases, it can help to streamline the configuration check. This script can be run multiple times during the preparation phase to find missing configuration during the process.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#before-we-start","title":"Before We Start","text":"<p>A simplyblock production cluster consists of three different types of nodes:</p> <ol> <li>Management nodes are part of the control plane which managed the cluster(s). A production cluster requires at least three nodes.</li> <li>Storage nodes are part of a specific storage cluster and provide capacity to the distributed storage pool. A production cluster requires at least three nodes.</li> <li>Secondary nodes are part of a specific storage cluster and enable automatic fail over for NVMe-oF connections. A production cluster requires at least one node.</li> </ol> <p>A single control plane can manage one or more clusters. If started afresh, a control plane must be set up before creating a storage cluster. If there is a preexisting control plane, an additional storage cluster can be added to it directly.</p> <p>More information on the control plane, storage plane, as well as the different node types is available under Simplyblock Cluster in the architecture section.</p>"},{"location":"deployments/baremetal/install-simplyblock/#network-preparation","title":"Network Preparation","text":"<p>Simplyblock recommends two individual network interfaces, one for the control plane and one for the storage plane. Hence, in the following installation description, we assume two separated subnets. To install simplyblock in your environment, you may have to adopt these commands to match your configuration.</p> Network interface Network definition Abbreviation Subnet eth0 Control Plane control 192.168.10.0/24 eth1 Storage Plane storage 10.10.10.0/24 <p>Danger</p> <p>Simplyblock requires a fully redundant network interconnect, implemented via a solution such as LACP or Static LAG. Failing to provide that may cause data corruption or data loss in case of network issues. For more information see the Network Considerations section.</p>"},{"location":"deployments/baremetal/install-simplyblock/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/baremetal/install-simplyblock/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbcli cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbcli cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> <pre><code>sbcli cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster, requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbcli</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbcli mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbcli mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/baremetal/install-simplyblock/#storage-plane-installation","title":"Storage Plane Installation","text":"<p>The installation of a storage plane requires a functioning control plane. If no control plane cluster is available yet, it must be installed beforehand. Jump right to the Control Plane Installation.</p> <p>The following examples assume two subnets to be available. These subnets are defined as shown in Network Preparation.</p>"},{"location":"deployments/baremetal/install-simplyblock/#firewall-configuration-sp","title":"Firewall Configuration (SP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a storage node. Attention is required, as this list is for storage nodes only. Management nodes have a different port configuration. See the Firewall Configuration section for the control plane.</p> Service Direction Source / Target Network Port(s) Protocol(s) bdts ingress storage 4420 TCP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080-8890 TCP lvol-proxy ingress storage, control 9090-9900 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for port 22.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4420 -s 10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 5000 -s 192.168.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 8080 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/baremetal/install-simplyblock/#storage-node-installation","title":"Storage Node Installation","text":"<p>Now that the network is configured, the storage node software can be installed.</p> <p>Info</p> <p>All storage nodes can be prepared at this point, as they are added to the cluster in the next step. Therefore, it is recommended to execute this step on all storage nodes, before moving to the next step.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>Once the check is complete, the NVMe devices in each storage node can be prepared. To prevent data loss in case of a sudden power outage, NVMe devices need to be formatted for a specific LBA format.</p> <p>Danger</p> <p>Failing to format NVMe devices with the correct LBA format can lead to data loss or data corruption in the case of a sudden power outage or other loss of power.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo-3 ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> cli can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo-3 ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends to ask for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executing similar to the below example.</p> Example output of NVMe device formatting<pre><code>[demo@demo-3 ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre> <p>With all NVMe devices prepared, the storage node software can be deployed.</p> Deploy the storage node<pre><code>sudo sbcli sn deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a storage node deployment<pre><code>[demo@demo-3 ~]# sudo sbcli sn deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.2\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.2:5000\n</code></pre> <p>On a successful deployment, the last line will provide the storage node's control channel address. This should be noted for all storage nodes, as it is required in the next step to attach the storage node to the simplyblock storage cluster.</p>"},{"location":"deployments/baremetal/install-simplyblock/#secondary-node-installation","title":"Secondary Node Installation","text":"<p>A secondary node is a storage node without additional storage disks to contribute to the distributed storage pool. Apart from that, it is the same as a normal storage node.</p> <p>However, due to the missing storage devices, preparing a secondary node on requires deploying the storage node software.</p> Deploy the secondary node<pre><code>sudo sbcli sn deploy --ifname eth0\n</code></pre> <p>The output will look something like the following example:</p> Example output of a secondary node deployment<pre><code>[demo@demo-4 ~]# sudo sbcli sn deploy --ifname eth0\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.4\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.4:5000\n</code></pre> <p>On a successful deployment, the last line will provide the secondary node's control channel address. This should be noted, as it is required in the next step to attach the secondary node to the simplyblock storage cluster.</p>"},{"location":"deployments/baremetal/install-simplyblock/#attach-the-storage-node-to-the-control-plane","title":"Attach the Storage Node to the Control Plane","text":"<p>When all storage nodes are prepared, they can be added to the storage cluster.</p> <p>Warning</p> <p>The following command are executed from a management node. Attaching a storage node to a control plane is executed from a management node.</p> Attaching a storage node to the storage plane<pre><code>sudo sbcli sn add-node &lt;CLUSTER_ID&gt; &lt;SN_CTR_ADDR&gt; &lt;MGT_IF&gt; \\\n  --max-lvol &lt;MAX_LOGICAL_VOLUMES&gt; \\\n  --max-prov &lt;MAX_PROVISIONING_CAPACITY&gt; \\\n  --number-of-devices &lt;NUM_STOR_NVME&gt; \\\n  --partitions &lt;NUM_OF_PARTITIONS&gt; \\\n  --data-nics &lt;DATA_IF&gt;\n</code></pre> <p>Info</p> <p>The number of partitions (&lt;NUM_OF_PARTITIONS&gt;) depends on the storage node setup. If a storage node has a separate journaling device (which is strongly recommended), the value should be zero (0) to prevent the storage devices to be partitioned. This improves the performance and prevents device sharing between the journal and actual data storage location.</p> <p>The output will look something like the following example:</p> Example output of adding a storage node to the storage plane<pre><code>[demo@demo ~]# sudo sbcli sn add-node 7bef076c-82b7-46a5-9f30-8c938b30e655 192.168.10.2:5000 eth0 --max-lvol 50 --max-prov 500g --number-of-devices 3 --partitions 0 --data-nics eth1\n2025-02-26 14:55:17,236: INFO: Adding Storage node: 192.168.10.2:5000\n2025-02-26 14:55:17,340: INFO: Instance id: 0b0c825e-3d16-4d91-a237-51e55c6ffefe\n2025-02-26 14:55:17,341: INFO: Instance cloud: None\n2025-02-26 14:55:17,341: INFO: Instance type: None\n2025-02-26 14:55:17,342: INFO: Instance privateIp: 192.168.10.2\n2025-02-26 14:55:17,342: INFO: Instance public_ip: 192.168.10.2\n2025-02-26 14:55:17,347: INFO: Node Memory info\n2025-02-26 14:55:17,347: INFO: Total: 24.3 GB\n2025-02-26 14:55:17,348: INFO: Free: 23.2 GB\n2025-02-26 14:55:17,348: INFO: Minimum required huge pages memory is : 14.8 GB\n2025-02-26 14:55:17,349: INFO: Joining docker swarm...\n2025-02-26 14:55:21,060: INFO: Deploying SPDK\n2025-02-26 14:55:31,969: INFO: adding alceml_2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,010: INFO: creating subsystem nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,022: INFO: adding listener for nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4 on IP 10.10.10.2\n2025-02-26 14:55:32,303: INFO: Connecting to remote devices\n2025-02-26 14:55:32,321: INFO: Connecting to remote JMs\n2025-02-26 14:55:32,342: INFO: Make other nodes connect to the new devices\n2025-02-26 14:55:32,346: INFO: Setting node status to Active\n2025-02-26 14:55:32,357: INFO: {\"cluster_id\": \"3196b77c-e6ee-46c3-8291-736debfe2472\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"StorageNode\", \"message\": \"Storage node status changed from: in_creation to: online\", \"caused_by\": \"monitor\"}\n2025-02-26 14:55:32,361: INFO: Sending event updates, node: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5, status: online\n2025-02-26 14:55:32,368: INFO: Sending to: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5\n2025-02-26 14:55:32,389: INFO: Connecting to remote devices\n2025-02-26 14:55:32,442: WARNING: The cluster status is not active (unready), adding the node without distribs and lvstore\n2025-02-26 14:55:32,443: INFO: Done\n</code></pre> <p>Repeat this process for all prepared storage nodes to add them to the storage plane.</p>"},{"location":"deployments/baremetal/install-simplyblock/#attach-the-secondary-node-to-the-control-plane","title":"Attach the Secondary Node to the Control Plane","text":"<p>Afterward, the secondary node needs to be added to the cluster.</p> Attaching a secondary node to the storage plane<pre><code>sudo sbcli sn add-node &lt;CLUSTER_ID&gt; &lt;SN_CTR_ADDR&gt; &lt;MGT_IF&gt; \\\n  --data-nics &lt;DATA_IF&gt;\n  --is-secondary-node\n</code></pre> <p>The output will look something like the following example:</p> Example output of a secondary node to the storage plane<pre><code>[demo@demo ~]# sudo sbcli sn add-node 7bef076c-82b7-46a5-9f30-8c938b30e655 192.168.10.5:5000 ens18 --data-nics=ens16 --is-secondary-node\n2025-02-28 13:34:57,877: INFO: Adding Storage node: 192.168.10.115:5000\n2025-02-28 13:34:57,952: INFO: Node found: vm5\n2025-02-28 13:34:57,953: INFO: Instance id: 5d679365-1361-40b0-bac0-3de949057bbc\n2025-02-28 13:34:57,953: INFO: Instance cloud: None\n2025-02-28 13:34:57,954: INFO: Instance type: None\n2025-02-28 13:34:57,954: INFO: Instance privateIp: 192.168.10.5\n2025-02-28 13:34:57,955: INFO: Instance public_ip: 192.168.10.5\n2025-02-28 13:34:57,977: WARNING: Unsupported instance-type None for deployment\n2025-02-28 13:34:57,977: INFO: Node Memory info\n...\n025-02-28 13:35:08,068: INFO: Connecting to remote devices\n2025-02-28 13:35:08,111: INFO: Connecting to node 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:08,111: INFO: bdev found remote_alceml_378cf3b5-1959-4415-87bf-392fa1bbed6c_qosn1\n2025-02-28 13:35:08,112: INFO: bdev found remote_alceml_c4c4011a-8f82-4c9d-8349-b4023a20b87c_qosn1\n2025-02-28 13:35:08,112: INFO: bdev found remote_alceml_d27388b9-bbd8-4e82-8880-d8811aa45383_qosn1\n2025-02-28 13:35:08,113: INFO: Connecting to node b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:08,113: INFO: bdev found remote_alceml_7f5ade89-53c6-440b-9614-ec24db3afbd9_qosn1\n2025-02-28 13:35:08,114: INFO: bdev found remote_alceml_8d160125-f095-43ae-9781-16d841ae9719_qosn1\n2025-02-28 13:35:08,114: INFO: bdev found remote_alceml_b0691372-1a4b-4fa9-a805-c2c1f311541c_qosn1\n2025-02-28 13:35:08,114: INFO: Connecting to node 43560b0a-f966-405f-b27a-2c571a2bb4eb\n2025-02-28 13:35:08,115: INFO: bdev found remote_alceml_29e74188-5efa-47d9-9282-84b4e46b77db_qosn1\n2025-02-28 13:35:08,115: INFO: bdev found remote_alceml_a1efcbbf-328c-4f86-859f-fcfceae1c7a8_qosn1\n2025-02-28 13:35:08,116: INFO: bdev found remote_alceml_cf2d3d24-e244-4a45-a71d-6383db07806f_qosn1\n2025-02-28 13:35:08,274: WARNING: The cluster status is not active (unready), adding the node without distribs and lvstore\n2025-02-28 13:35:08,274: INFO: Done\n</code></pre> <p>On a successful response, it's finally time to activate the storage plane.</p>"},{"location":"deployments/baremetal/install-simplyblock/#activate-the-storage-cluster","title":"Activate the Storage Cluster","text":"<p>The last step after all nodes are added to the storage cluster, the storage plane can be activated.</p> Storage cluster activation<pre><code>sudo sbcli cluster activate &lt;CLUSTER_ID&gt;\n</code></pre> <p>The command output should look like this and respond with a successful activation of the storage cluster</p> Example output of a storage cluster activation<pre><code>[demo@demo ~]# sbcli cluster activate 7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-28 13:35:26,053: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from unready to in_activation\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:26,322: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to 2f4dafb1-d610-42a7-9a53-13732459523e\n2025-02-28 13:35:31,133: INFO: Connecting remote_jm_43560b0a-f966-405f-b27a-2c571a2bb4eb to b7db725a-96e2-40d1-b41b-738495d97093\n2025-02-28 13:35:55,791: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"Cluster\", \"message\": \"Cluster status changed from in_activation to active\", \"caused_by\": \"cli\"}\n2025-02-28 13:35:55,794: INFO: Cluster activated successfully\n</code></pre> <p>Now that the cluster is ready, it is time to install the Kubernetes CSI Driver or learn how to use the simplyblock storage cluster to manually provision logical volumes.</p>"},{"location":"deployments/baremetal/prerequisites/","title":"Prerequisites","text":"<p>When installing simplyblock control planes and storage planes, a number of prerequisites are important to understand.</p> <p>Simplyblock uses Docker Swarm for the control plane cluster. In case of a bare metal installation, it will also use Docker Swarm for the storage plane. Hence, Docker has to be installed.</p> <p>Furthermore, simplyblock requires the installation of the <code>sbcli</code> command line tool. This tool is written in Python. Therefore, Python (3.5 or later) has to be installed. Likewise, pip, the Python package manager, has to be installed with version 20 or later.</p> <p>To install <code>sbcli</code> run:</p> <pre><code>sudo pip install sbcli --upgrade\n</code></pre>"},{"location":"deployments/baremetal/prerequisites/#node-sizing","title":"Node Sizing","text":"<p>Simplyblock has certain requirements in terms of CPU, RAM, and storage. See the specific Node Sizing documentation to learn more.</p>"},{"location":"deployments/baremetal/prerequisites/#network-configuration","title":"Network Configuration","text":"<p>Simplyblock requires a number of network ports to be available from different networks. The configuration of the required network ports is provided in the installation documentation.</p> <p>Additionally, IPv6 must be disabled on all nodes running simplyblock.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"},{"location":"deployments/baremetal/prerequisites/#network-ports-for-control-plane","title":"Network Ports for Control Plane","text":"Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP"},{"location":"deployments/baremetal/prerequisites/#network-ports-for-storage-plane","title":"Network Ports for Storage Plane","text":"Service Direction Source / Target Network Port(s) Protocol(s) bdts ingress storage 4420 TCP Cluster Control ingress control 5000 TCP spdk-http-proxy ingress storage, control 8080-8890 TCP lvol-proxy ingress storage, control 9090-9900 TCP SSH ingress storage, control, admin 22 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB egress storage 4500 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP Graylog egress control 12202 TCP"},{"location":"deployments/baremetal/prerequisites/#storage-configuration","title":"Storage Configuration","text":"<p>Simplyblock has certain requirements in terms of storage. While the most important facts are provided in the installation section, here are things to consider.</p>"},{"location":"deployments/baremetal/prerequisites/#root-volume","title":"Root Volume","text":"<p>The volume mounted as the root directory has to provide at least 35GiB of free capacity. More free space is recommended, especially for control plane nodes which collect logs and the cluster state.</p>"},{"location":"deployments/baremetal/prerequisites/#nvme-devices","title":"NVMe Devices","text":"<p>NVMe devices used for simplyblock, should ALWAYS be formatted using the <code>nvme</code> command line tool before adding them to a simplyblock storage node. Failing to do so can negatively impact storage performance and lead to data corruption or even data loss in case of a sudden power outage.</p> <p>The <code>lsblk</code> is the best way to find all NVMe devices attached to a system.</p> Example output of lsblk<pre><code>[demo@demo ~]# sudo lsblk\nNAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nsda           8:0    0   30G  0 disk\n\u251c\u2500sda1        8:1    0    1G  0 part /boot\n\u2514\u2500sda2        8:2    0   29G  0 part\n  \u251c\u2500rl-root 253:0    0   26G  0 lvm  /\n  \u2514\u2500rl-swap 253:1    0    3G  0 lvm  [SWAP]\nnvme3n1     259:0    0  6.5G  0 disk\nnvme2n1     259:1    0   70G  0 disk\nnvme1n1     259:2    0   70G  0 disk\nnvme0n1     259:3    0   70G  0 disk\n</code></pre> <p>In the example, we see four NVMe devices. Three devices of 70GiB and one device with 6.5GiB storage capacity.</p> <p>To find the correct LBA format (lbaf) for each of the devices, the <code>nvme</code> cli can be used.</p> Show NVMe namespace information<pre><code>sudo nvme id-ns /dev/nvmeXnY\n</code></pre> <p>The output depends on the NVMe device itself, but looks something like this:</p> Example output of NVMe namespace information<pre><code>[demo@demo ~]# sudo nvme id-ns /dev/nvme0n1\nNVME Identify Namespace 1:\n...\nlbaf  0 : ms:0   lbads:9  rp:0\nlbaf  1 : ms:8   lbads:9  rp:0\nlbaf  2 : ms:16  lbads:9  rp:0\nlbaf  3 : ms:64  lbads:9  rp:0\nlbaf  4 : ms:0   lbads:12 rp:0 (in use)\nlbaf  5 : ms:8   lbads:12 rp:0\nlbaf  6 : ms:16  lbads:12 rp:0\nlbaf  7 : ms:64  lbads:12 rp:0\n</code></pre> <p>From this output, the required lbaf configuration can be found. The necessary configuration has to have the following values:</p> Property Value ms 0 lbads 12 rp 0 <p>In the example, the required LBA format is 4. If a NVMe device doesn't have that combination, any other lbads=12 combination will work. However, simplyblock recommends to ask for the best available combination.</p> <p>In our example, the device is already formatted with the correct lbaf (see the \"in use\"). It is, however, recommended to always format the device before use.</p> <p>To format the drive, the <code>nvme</code> cli is used again.</p> Formatting the NVMe device<pre><code>sudo nvme format --lbaf=&lt;lbaf&gt; --ses=0 /dev/nvmeXnY\n</code></pre> <p>The output of the command should give a successful response when executing similar to the below example.</p> Example output of NVMe device formatting<pre><code>[demo@demo ~]# sudo nvme format --lbaf=4 --ses=0 /dev/nvme0n1\nYou are about to format nvme0n1, namespace 0x1.\nWARNING: Format may irrevocably delete this device's data.\nYou have 10 seconds to press Ctrl-C to cancel this operation.\n\nUse the force [--force] option to suppress this warning.\nSending format operation ...\nSuccess formatting namespace:1\n</code></pre>"},{"location":"deployments/deployment-planning/","title":"Deployment Planning","text":"<p>Proper deployment planning is essential for ensuring the performance, scalability, and resilience of a simplyblock storage cluster.</p> <p>Before installation, key factors such as node sizing, storage capacity, and fault tolerance mechanisms should be carefully evaluated to match workload requirements. This section provides guidance on sizing management nodes and storage nodes, helping administrators allocate adequate CPU, memory, and disk resources for optimal cluster performance.</p> <p>Additionally, it explores selectable erasure coding schemes, detailing how different configurations impact storage efficiency, redundancy, and recovery performance. Other critical considerations, such as network infrastructure, high availability strategies, and workload-specific optimizations, are also covered to assist in designing a simplyblock deployment that meets both operational and business needs.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/","title":"Erasure Coding Scheme","text":"<p>Choosing the appropriate erasure coding scheme is crucial when deploying a simplyblock storage cluster, as it directly impacts data redundancy, storage efficiency, and overall system performance. Simplyblock currently supports two erasure coding schemes: 1+1 and 2+1. Understanding the trade-offs between redundancy and storage utilization will help determine the best option for your workload.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#erasure-coding-schemes","title":"Erasure Coding Schemes","text":"<p>Erasure coding (EC) is a data protection mechanism that distributes data and parity across multiple storage nodes, allowing data recovery in case of hardware failures. The notation k+m represents:</p> <ul> <li>k: The number of data fragments.</li> <li>m: The number of parity fragments.</li> </ul> <p>If you need more information on erasure coding, see the dedicated concept page for erasure coding.</p>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-11","title":"Scheme: 1+1","text":"<ul> <li>Description: In the 1+1 scheme, data is mirrored, effectively creating an exact copy of every data block.</li> <li>Redundancy Level: Can tolerate the failure of one storage node.</li> <li>Storage Efficiency: 50% (since each piece of data is fully duplicated, requiring double the storage capacity).</li> <li>Performance Considerations: Offers fast recovery and high read performance due to data mirroring.</li> <li>Best Use Cases:<ul> <li>Workloads requiring high availability and minimal recovery time.</li> <li>Applications where performance is prioritized over storage efficiency.</li> <li>Small to mid-sized deployments with limited storage nodes.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#scheme-21","title":"Scheme: 2+1","text":"<ul> <li>Description: In the 2+1 scheme, data is divided into two fragments with one parity fragment, offering a   balance between redundancy and storage efficiency.</li> <li>Redundancy Level: Can tolerate the failure of one storage node.</li> <li>Storage Efficiency: 66% (since one-third of the total capacity is used for parity).</li> <li>Performance Considerations: Lower write amplification compared to 1+1, as data is distributed across   multiple nodes.</li> <li>Best Use Cases:<ul> <li>Deployments where storage efficiency is a priority without significantly compromising redundancy.</li> <li>Applications that can tolerate slightly higher recovery times compared to 1+1.</li> <li>Larger storage clusters where balanced resource utilization is necessary.</li> </ul> </li> </ul>"},{"location":"deployments/deployment-planning/erasure-coding-scheme/#choosing-the-scheme","title":"Choosing the Scheme","text":"<p>When selecting an erasure coding scheme for simplyblock, consider the following:</p> <ol> <li>Redundancy Requirements: If the priority is maximum data protection and quick recovery, 1+1 is ideal. For a    balance between protection and efficiency, 2+1 is preferred.</li> <li>Storage Capacity: 1+1 requires double the storage space, whereas 2+1 provides better storage efficiency.</li> <li>Performance Needs: 1+1 offers faster reads and writes due to mirroring, while 2+1 reduces write    amplification and optimizes storage usage.</li> <li>Cluster Size: Smaller clusters benefit from 1+1 due to its simplicity and faster rebuild times, whereas    2+1 is more effective in larger clusters.</li> <li>Recovery Time Objectives (RTOs): If minimizing downtime is critical, 1+1 offers near-instant recovery    compared to 2+1.</li> </ol>"},{"location":"deployments/deployment-planning/further-considerations/","title":"Further Considerations","text":""},{"location":"deployments/deployment-planning/further-considerations/#system-compatibility","title":"System Compatibility","text":"<p>Simplyblock contains two major components, the control plane and the storage plane.</p> <ul> <li>For the control plane, simplyblock requires x86-64 (AMD64 / Intel 64) compatible CPUs.</li> <li>For the storage plane, simplyblock supports x86-64 (AMD64 / Intel 64) or ARM64 (AArch64) compatible CPUs.</li> </ul> <p>Info</p> <p>A single storage plane cluster can be set up from both x86-64 and ARM64 CPUs. However, simplyblock recommends to build a storage plane from a single CPU architecture. When operating a hybrid storage cluster with Kubernetes a mixed CPU architecture should be limited to one CPU architecture for the disaggregated portion of the storage cluster and one architecture for the Kubernetes worker nodes.</p> <p>In terms of operating system, simplyblock supports Red Hat-based Linux distribution and recommends a Linux kernel 5.9 or later.</p>"},{"location":"deployments/deployment-planning/further-considerations/#storage-considerations","title":"Storage Considerations","text":"<p>Simplyblock is an NVMe-first storage architecture and requires NVMe device with exclusive access. Simplyblock does not support individual partitions, but requires full and exclusive access to the physical or virtual NVMe device.</p>"},{"location":"deployments/deployment-planning/network-considerations/","title":"Network Considerations","text":"<p>Simplyblock is a distributed storage platform. Hence, it highly relies on a strong network infrastructure for the performance and reliability of its virtual block storage devices (logical volume). </p>"},{"location":"deployments/deployment-planning/network-considerations/#network-type","title":"Network Type","text":"<p>Protocol-wise, simplyblock implements NVMe over Fabrics (NVMe-oF), meaning that simplyblock does not require any specific network infrastructure such as Fibre Channel or Infiniband, but works over commodity Ethernet interconnects.</p> <p>For data transmission, simplyblock provides NVMe over TCP (NVMe/TCP) and NVMe over RDMA over Converged Ethernet (NVMe/RoCE).</p>"},{"location":"deployments/deployment-planning/network-considerations/#network-infrastructure","title":"Network Infrastructure","text":"<p>In terms of bandwidth, simplyblock recommends at least 40GBit/s per second interconnects, but higher is better. Especially with a high number of cluster nodes and logical volumes, simplyblock can easily saturate 200 GBit/s and more interconnects.</p> <p>Recommendation</p> <p>Simplyblock recommends NVIDIA Mellanox network adapters. However, every network adapter, including virtual ones will work. If using virtual machines, the physical network adapter should be made available to the VM using PCI-e passthrough (IOMMU).</p> <p>Additionally, simplyblock recommends a physically separated storage network or using a VLAN to create a virtually separated network. This can improve performance and minimize network contention.</p> <p>Recommendation</p> <p>If VLANs are used, prefer hardware-based VLANs configured in switches over a software-based VLAN with Linux bridges.</p>"},{"location":"deployments/deployment-planning/network-considerations/#network-configuration","title":"Network Configuration","text":"<p>Lastly, simplyblock requires a set of TCP/IP ports to be opened towards specific subnets. The installation prerequisites for the deployment model of your choice list the required ports. Simplyblock also provides a shell script to pre-test the most important requirements to ensure a smooth installation.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two separate NICs, one for the control plane traffic and one for the storage plane. These can be implemented via VLAN. However, we recommend to port-based VLANs configured in the switch over virtual VLAN interfaces in Linux.</p> <p>Additionally, simplyblock strongly recommends to design any network interconnect as a fully redundant connection. All commonly found solutions to achieve that are supported, including but not limited to LACP and Static LAG configurations, stacked switches, bonded NICs.</p> <p>Danger</p> <p>Simplyblock, internally, always assumes the interconnect to be reliable, failing to provide such an interconnect may lead to data loss in failure situations.</p>"},{"location":"deployments/deployment-planning/node-sizing/","title":"Node Sizing","text":"<p>When planning the deployment of a simplyblock cluster, it is essential to plan the sizing of the nodes. The sizing requirements are elaborated below, whether deployed on a private or public cloud or inside or outside of Kubernetes.</p>"},{"location":"deployments/deployment-planning/node-sizing/#sizing-assumptions","title":"Sizing Assumptions","text":"<p>The following sizing information is meant for production environments.</p> <p>Warning</p> <p>Simplyblock always recommends using physical cores over virtual and hyper-threading cores. If the sizing document discusses virtual CPUs (vCPU), it means 0.5 physical CPUs. This corresponds to a typical hyper-threaded CPU core x86-64. This also relates to how AWS EC2 cores are measured.</p>"},{"location":"deployments/deployment-planning/node-sizing/#management-nodes","title":"Management Nodes","text":"<p>An appropriately sized management node cluster is required to ensure optimal performance and scalability. The management plane oversees critical functions such as cluster topology management, health monitoring, statistics collection, and automated maintenance tasks.</p> <p>The following hardware sizing specifications are recommended:</p> Hardware CPU Minimum 2 physical cores, plus<ul><li>1 vCPU per 5 storage nodes</li><li>1 vCPU 500 logical volumes</li></ul> RAM Minimum 8 GiB, plus:<ul><li>1 GiB RAM per 5 storage nodes</li><li>1 GiB per 500 logical volumes</li></ul> Disk Minimum 35 GiB, plus:<ul><li>500 MiB per 100 cluster objects (storage nodes, devices, logical volumes, snapshots)</li></ul> Node type Bare metal or virtual machine with a supported Linux distribution Number of nodes For a production environment, a minimum of 3 management nodes is required."},{"location":"deployments/deployment-planning/node-sizing/#storage-nodes","title":"Storage Nodes","text":"<p>A suitably sized storage node cluster is required to ensure optimal performance and scalability. Storage nodes are responsible for handling all I/O operations and data services for logical volumes and snapshots.</p> <p>The following hardware sizing specifications are recommended:</p> Hardware CPU Minimum 4 physical cores.3 cores are dedicated to service threads.Additionally, available cores are allocated to worker threads. Each additional core contributes about 200.000 IOPS to the node's performance profile (disregarding other limiting factors such as network bandwidth). RAM Minimum 4 GiB (for operating system) Disk Minimum 5 GiB boot volume"},{"location":"deployments/deployment-planning/node-sizing/#memory-requirements","title":"Memory Requirements","text":"<p>In addition to the above RAM requirements, the storage node requires additional memory based on the managed storage capacity.</p> <p>Simplyblock works with two types of memory: huge pages memory, which has to be pre-allocated prior to starting the storage node services and is then exclusively assigned to simplyblock, as well as system memory, which is required on demand.</p>"},{"location":"deployments/deployment-planning/node-sizing/#huge-pages","title":"Huge Pages","text":"<p>The exact amount of huge page memory is calculated when adding or restarting a node based on two parameters: the maximum amount of storage available in the cluster and the maximum amount of logical volumes which can be created on the node:</p> Unit Memory Requirement Per logical volume 6 MiB Per TB of max. cluster storage 256 MiB <p>Recommendation</p> <p>For bare metal or disaggregated deployments, simplyblock recommends allocating around 75% of the available memory as huge pages, minimizing memory overhead. For hyper-converged deployments, please use the huge pages calculator. </p> <p>If not enough huge pages memory is available, the node will refuse to start. In this case, you may check <code>/proc/meminfo</code> for total, reserved, and available huge page memory on a corresponding node.</p> <p>Execute the following command to allocate temporary huge pages while the system is already running. It will allocate 8 GiB in huge pages. Please adjust the number of huge pages depending on your requirements.</p> Allocate temporary huge pages<pre><code>sudo sysctl vm.nr_hugepages=4096\n</code></pre> <p>Since the allocation is temporary, it will disappear after a system reboot.</p> <p>Recommendation</p> <p>Simplyblock recommends to pre-allocate huge pages via the bootloader commandline. This prevents fragmentation of the huge pages memory and ensures a continuous memory area to be allocated. </p>GRUB configuration change<pre><code>GRUB_CMDLINE_LINUX=\"${GRUB_CMDLINE_LINUX} default_hugepagesz=2MB hugepagesz=2MB hugepages=4096\"\n</code></pre> Afterward, you need to persist the change to take effect. Persist GRUB configuration<pre><code>sudo grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg\n</code></pre>"},{"location":"deployments/deployment-planning/node-sizing/#conventional-memory","title":"Conventional Memory","text":"<p>Additionally to huge pages, simplyblock requires dynamically allocatable conventional system memory. The required amount depends on the utilized storage.</p> Unit Memory Requirement Per TiB of used local SSD storage 256 MiB Per TiB of used logical volume storage 256 MiB <p>Info</p> <p>Used local SSD storage is the physically utilized capacity of the local NVMe devices on the storage node at a point in time. Used logical volume storage is the physically utilized capacity of all logical volumes on a specific storage node at a point in time.</p>"},{"location":"deployments/deployment-planning/node-sizing/#storage-planning","title":"Storage Planning","text":"<p>Simplyblock storage nodes require one or more NVMe devices to provide storage capacity to the distributed storage pool of a storage cluster.</p> <p>Recommendation</p> <p>Simplyblock requires at least three similar sized NVMe devices per storage node.</p> <p>Furthermore, simplyblock storage nodes require one additional NVMe device with less capacity as a journaling device. The journaling device becomes part of the distributed record journal, keeping track of all changes before being persisted into their final position. This helps with write performance and transactional behavior by using a write-ahead log structure and replaying the journal in case of a issue.</p> <p>Info</p> <p>Secondary nodes don't need NVMe storage disks.</p>"},{"location":"deployments/deployment-planning/node-sizing/#caching-nodes-k8s-only","title":"Caching Nodes (K8s only)","text":"<p>In Kubernetes, simplyblock can be configured to deploy caching nodes. These nodes provide a ultra-low latency write-through cache to a disaggregated cluster, improving access latency substantially.</p> Hardware CPU Minimum 2 vCPU, better 2 physical cores. RAM Minimum 2 GiB, plus 25% of the configured huge pages <p>In addition to the base conventional memory configuration, a caching node requires huge pages memory. Calculating the required huge pages memory can be achieved using the following formula:</p> Huge page calculation (caching node)<pre><code>huge_pages_size=2GiB + 0.0025 * nvme_size_gib\n</code></pre> <p>With the above formula, a locally-attached NVMe device of 1.9TiB would require 6.75GiB huge pages memory (<code>2 + 0.0025 * 1900</code>).</p>"},{"location":"deployments/kubernetes/","title":"Kubernetes","text":"<p>Installing simplyblock into and using it with Kubernetes requires two or more components to be installed. The number of components depends on your deployment strategy and requirements.</p> <p>For Kubernetes-related installations, simplyblock provides three deployment models: hyper-converged (also known as co-located), disaggregated, and a hybrid model which combines the best of the former two.</p>"},{"location":"deployments/kubernetes/#prerequisites","title":"Prerequisites","text":"<p>Before starting with the installation of simplyblock, make yourself familiar with the requirements and prerequisites of simplyblock. You can find all necessary information under the Prerequisites section specific to Kubernetes deployments.</p>"},{"location":"deployments/kubernetes/#installation","title":"Installation","text":"<p>After making sure that all requirements are fulfilled, you can start with the installation. Follow the necessary section depending on your chosen deployment model:</p> <ul> <li>Hyper-Converged Setup</li> <li>Disaggregated Setup</li> <li>Hybrid Setup</li> </ul> <p>In either case, you start with installing the control plane, before going over to the actual storage cluster and the Kubernetes CSI driver.</p> <p>As a last step, you may want to install caching nodes on your Kubernetes workers to improve access latency. See the installation steps in the Install Caching Nodes section.</p>"},{"location":"deployments/kubernetes/cluster-topology-awareness/","title":"Cluster Topology Awareness","text":""},{"location":"deployments/kubernetes/data-migration/","title":"Data Migration","text":""},{"location":"deployments/kubernetes/prerequisites/","title":"Prerequisites","text":"<p>Before deploying simplyblock on a Kubernetes cluster, it is essential to ensure that the environment meets all necessary infrastructure, software, and configuration requirements. Proper planning and preparation will help guarantee a smooth installation, optimal performance, and long-term stability of the simplyblock storage system.</p> <p>This section outlines the key hardware and software prerequisites, including supported Kubernetes versions, required resources for management and storage nodes, necessary permissions, network configurations, and storage prerequisites.</p> <p>Verifying these requirements before installation will help avoid compatibility issues and ensure that Simplyblock integrates seamlessly with your Kubernetes deployment.</p>"},{"location":"deployments/kubernetes/prerequisites/#control-plane","title":"Control Plane","text":"<p>Minimum 2 physical cores Minimum 8GiB 35 GiB unused disk space Supported Linux distribution 3 management nodes</p> Create Management Cluster<pre><code>sbcli cluster create\nsbcli cluster list\n</code></pre> Sample output control plane creation<pre><code>[demo@demo ~]# sudo sbcli cluster create --ifname=ens18 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.151\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655 # (1)\n</code></pre> <ol> <li> This is the cluster id: 7bef076c-82b7-46a5-9f30-8c938b30e655</li> </ol> <p>Returns cluster-id</p> Example output for listing available clusters<pre><code>[demo@demo ~]# sudo sbcli cluster list\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n| UUID                                 | NQN                                                             | ha_type | tls   | mgmt nodes | storage nodes | Mod | Status  |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n| 7bef076c-82b7-46a5-9f30-8c938b30e655 | nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655 | ha      | False | 3          | 10             | 1x1 | unready |\n+--------------------------------------+-----------------------------------------------------------------+---------+-------+------------+---------------+-----+---------+\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/#get-cluster-secret","title":"Get Cluster Secret","text":"<p>sbcli-hmdi cluster get-secret </p> Example output get cluster secret<pre><code>[demo@demo ~]# sudo sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0 # (1)\n</code></pre> <ol> <li> This is the cluster secret: e8SQ1ElMm8Y9XIwyn8O0</li> </ol>"},{"location":"deployments/kubernetes/prerequisites/#control-plane-secondaries","title":"Control Plane Secondaries","text":"Adding a management node to the control plane<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\nsudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\nsbcli mgmt add 192.168.10.151 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 ens18\nsbcli mgmt add &lt;CTR_PLANE_PRI_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> Example output joining a control plane cluster<pre><code>[root@vm12 ~]# sbcli-dev mgmt add 192.168.10.151 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 ens18\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.152\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre>"},{"location":"deployments/kubernetes/prerequisites/#network-configuration","title":"Network Configuration","text":""},{"location":"deployments/kubernetes/prerequisites/#disable-ipv6","title":"Disable IPv6","text":"Permanently disable IPv6<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n</code></pre> <p>Defined networks:</p> <ul> <li>internal: The subnet for communication with the control plane</li> <li>storage: The subnet for communication with and between the storage plane</li> <li>loadbalancer: The subnet between the load balancer and control plane</li> <li>management: Valid IPs or IP ranges for direct management access</li> </ul> Direction Source or target nw ports protocol ingress management 80 tcp ingress mgmt 3000 tcp ingress mgmt 9000 tcp egress all all all Service Direction Source Network Port Protocol(s) API (HTTPS) ingress loadbalancer 80 TCP SSH ingress management 22 TCP Grafana ingress loadbalancer 3000 TCP Graylog ingress loadbalancer 9000 TCP Docker Swarm ingress storage, internal 2375 TCP Docker Swarm ingress storage, internal 2377 TCP Docker Swarm ingress storage, internal 4789 UDP Docker Swarm ingress storage, internal 7946 TCP / UDP Graylog ingress storage, internal 12201 TCP / UDP FoundationDB ingress storage, internal 4800 TCP FoundationDB ingress storage 4500 TCP Cluster Control ingress storage 4420 TCP Cluster Control ingress storage 9090-9099 TCP Test Service Direction Target Network Port Protocol(s) All traffic egress [0.0.0.0/0] ALL ALL"},{"location":"deployments/kubernetes/prerequisites/#storage-plane","title":"Storage Plane","text":"<p>```bash title=\" lsblk nvme id-ns /dev/nvmeXnY nvme format --lbaf= --ses=0 /dev/nvmeXnY <pre><code>### Deploy a Storge Node\nsbcli -d sn deploy --ifname ens18\n\n```plain title=\"Example output deploying storage node\"\n[demo@demo-sn-1 ~]# sbcli-hmdi -d sn deploy --ifname ens18\n2025-02-26 13:35:06,991: INFO: NVMe SSD devices found on node:\n2025-02-26 13:35:07,038: INFO: Installing dependencies...\n2025-02-26 13:35:13,508: INFO: Node IP: 192.168.10.153\n2025-02-26 13:35:13,623: INFO: Pulling image public.ecr.aws/simply-block/simplyblock:hmdi\n2025-02-26 13:35:15,219: INFO: Recreating SNodeAPI container\n2025-02-26 13:35:15,543: INFO: Pulling image public.ecr.aws/simply-block/ultra:main-latest\n192.168.10.153:5000\n</code></pre></p>"},{"location":"deployments/kubernetes/prerequisites/#add-storage-node-from-management-node","title":"Add Storage Node (from Management Node)","text":"Example output for adding a storage node<pre><code>[root@vm11 ~]# sbcli-hmdi -d sn add-node 3196b77c-e6ee-46c3-8291-736debfe2472 192.168.10.152:5000 ens18 --max-lvol 100 --max-prov 5000 --number-of-devices 3 --partitions 0\n2025-02-26 14:55:17,236: INFO: Adding Storage node: 192.168.10.152:5000\n2025-02-26 14:55:17,340: INFO: Instance id: 0b0c825e-3d16-4d91-a237-51e55c6ffefe\n2025-02-26 14:55:17,341: INFO: Instance cloud: None\n2025-02-26 14:55:17,341: INFO: Instance type: None\n2025-02-26 14:55:17,342: INFO: Instance privateIp: 192.168.10.152\n2025-02-26 14:55:17,342: INFO: Instance public_ip: 192.168.10.152\n2025-02-26 14:55:17,347: INFO: Node Memory info\n2025-02-26 14:55:17,347: INFO: Total: 24.3 GB\n2025-02-26 14:55:17,348: INFO: Free: 23.2 GB\n2025-02-26 14:55:17,348: INFO: Minimum required huge pages memory is : 14.8 GB\n2025-02-26 14:55:17,349: INFO: Joining docker swarm...\n2025-02-26 14:55:21,060: INFO: Deploying SPDK\n2025-02-26 14:55:31,969: INFO: adding alceml_2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,010: INFO: creating subsystem nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4\n2025-02-26 14:55:32,022: INFO: adding listener for nqn.2023-02.io.simplyblock:vm12:dev:2d1c235a-1f4d-44c7-9ac1-1db40e23a2c4 on IP 192.168.10.152\n2025-02-26 14:55:32,303: INFO: Connecting to remote devices\n2025-02-26 14:55:32,321: INFO: Connecting to remote JMs\n2025-02-26 14:55:32,342: INFO: Make other nodes connect to the new devices\n2025-02-26 14:55:32,346: INFO: Setting node status to Active\n2025-02-26 14:55:32,357: INFO: {\"cluster_id\": \"3196b77c-e6ee-46c3-8291-736debfe2472\", \"event\": \"STATUS_CHANGE\", \"object_name\": \"StorageNode\", \"message\": \"Storage node status changed from: in_creation to: online\", \"caused_by\": \"monitor\"}\n2025-02-26 14:55:32,361: INFO: Sending event updates, node: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5, status: online\n2025-02-26 14:55:32,368: INFO: Sending to: 37b404b9-36aa-40b3-8b74-7f3af86bd5a5\n2025-02-26 14:55:32,389: INFO: Connecting to remote devices\n2025-02-26 14:55:32,442: WARNING: The cluster status is not active (unready), adding the node without distribs and lvstore\n2025-02-26 14:55:32,443: INFO: Done\n</code></pre> <ul> <li>Kubernetes v1.25 or higher</li> <li>Privileged container</li> </ul> Example output of a storage node listing<pre><code>[root@vm11 ~]# sbcli-hmdi sn list\n+--------------------------------------+----------+----------------+---------+-------+--------+--------+-----------+--------------------------------------+------------+----------------+\n| UUID                                 | Hostname | Management IP  | Devices | LVols | Status | Health | Up time   | Cloud ID                             | Cloud Type | Ext IP         |\n+--------------------------------------+----------+----------------+---------+-------+--------+--------+-----------+--------------------------------------+------------+----------------+\n| 37b404b9-36aa-40b3-8b74-7f3af86bd5a5 | vm12     | 192.168.10.152 | 3/3     | 0     | online | True   | 1h 8m 11s | 0b0c825e-3d16-4d91-a237-51e55c6ffefe | None       | 192.168.10.152 |\n| 1a5d4106-1bc8-4d68-91e5-ac6e93fb0549 | vm13     | 192.168.10.153 | 3/3     | 0     | online | True   | 45m 27s   | de45504a-d36b-42e6-996d-133ec79f4d47 | None       | 192.168.10.153 |\n| 01b1caa0-b94a-4863-a735-832250faee61 | vm14     | 192.168.10.154 | 3/3     | 0     | online | True   | 43m 24s   | 1d20291f-3ca9-4aac-81e9-7d3e3e33e553 | None       | 192.168.10.154 |\n+--------------------------------------+----------+----------------+---------+-------+--------+--------+-----------+--------------------------------------+------------+----------------+\n</code></pre>"},{"location":"deployments/kubernetes/install-caching-nodes/","title":"Install Caching Nodes","text":""},{"location":"deployments/kubernetes/install-simplyblock/","title":"Install Simplyblock","text":""},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/","title":"Disaggregated Setup","text":""},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbcli cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbcli cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> <pre><code>sbcli cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/disaggregated/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster, requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbcli</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbcli mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbcli mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/","title":"Hybrid Setup","text":""},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbcli cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbcli cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> <pre><code>sbcli cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hybrid/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster, requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbcli</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbcli mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbcli mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/","title":"Hyper-Converged Setup","text":""},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#control-plane-installation","title":"Control Plane Installation","text":"<p>The first step when installing simplyblock, is to install the control plane. The control plane manages one or more storage clusters. If an existing control plane is available and the new cluster should be added to it, this section can be skipped. Jump right to the Storage Plane Installation.</p>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#firewall-configuration-cp","title":"Firewall Configuration (CP)","text":"<p>Simplyblock requires a number of TCP and UDP ports to be opened from certain networks. Additionally, it requires IPv6 to be disabled on management nodes.</p> <p>Following is a list of all ports (TCP and UDP) required for operation as a management node. Attention is required, as this list is for management nodes only. Storage nodes have a different port configuration. See the Firewall Configuration section for the storage plane.</p> Service Direction Source / Target Network Port Protocol(s) Cluster API ingress storage, control, admin 80 TCP SSH ingress storage, control, admin 22 TCP Graylog ingress storage, control 12201 TCP / UDP Graylog ingress storage, control 12202 TCP Graylog ingress storage, control 13201 TCP Graylog ingress storage, control 13202 TCP Docker Daemon Remote Access ingress storage, control 2375 TCP Docker Swarm Remote Access ingress storage, control 2377 TCP Docker Overlay Network ingress storage, control 4789 UDP Docker Network Discovery ingress storage, control 7946 TCP / UDP FoundationDB ingress storage, control 4500 TCP Prometheus ingress storage, control 9100 TCP Cluster Control egress storage, control 8080 TCP spdk-http-proxy egress storage, control 5000 TCP Docker Daemon Remote Access egress storage, control 2375 TCP Docker Swarm Remote Access egress storage, control 2377 TCP Docker Overlay Network egress storage, control 4789 UDP Docker Network Discovery egress storage, control 7946 TCP / UDP <p>With the previously defined subnets, the following snippet disables IPv6 and configures the iptables automatically.</p> <p>Danger</p> <p>The example assumes that you have an external firewall between the admin network and the public internet! If this is not the case, ensure the correct source access for ports 22 and 80.</p> Network Configuration<pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\n\n# Clean up\nsudo iptables -F SIMPLYBLOCK\nsudo iptables -D DOCKER-FORWARD -j SIMPLYBLOCK\nsudo iptables -X SIMPLYBLOCK\n# Setup\nsudo iptables -N SIMPLYBLOCK\nsudo iptables -I DOCKER-FORWARD 1 -j SIMPLYBLOCK\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A SIMPLYBLOCK -m state --state ESTABLISHED,RELATED -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 80 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2375 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 2377 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 4500 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 4789 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 7946 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 9100 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p udp --dport 12201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 12202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13201 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -p tcp --dport 13202 -s 192.168.10.0/24,10.10.10.0/24 -j RETURN\nsudo iptables -A SIMPLYBLOCK -s 0.0.0.0/0 -j DROP\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#management-node-installation","title":"Management Node Installation","text":"<p>Now that the network is configured, the management node software can be installed.</p> <p>Simplyblock provides a command line interface called <code>sbcli</code>. It's built in Python and required Python 3 and Pip (the Python package manager) installed on the machine. This can be achieved with <code>yum</code>.</p> Install Python and Pip<pre><code>sudo yum -y install python3-pip\n</code></pre> <p>Afterward, the <code>sbcli</code> command line interface can be installed. Upgrading the CLI later on, uses the same command.</p> Install Simplyblock CLI<pre><code>sudo pip install sbcli --upgrade\n</code></pre> <p>Recommendation</p> <p>Simplyblock recommends to only upgrade <code>sbcli</code> if a system upgrade is executed to prevent potential incompatibilities between the running simplyblock cluster and the version of <code>sbcli</code>.</p> <p>At this point, a quick check with the simplyblock provided system check can reveal potential issues quickly.</p> Automatically check your configuration<pre><code>curl -L https://sblk.xyz/prerequisites | bash\n</code></pre> <p>If the check succeeds, it's time to set up the primary management node:</p> Deploy the primary management node<pre><code>sbcli cluster create --ifname=&lt;IF_NAME&gt; --ha-type=ha\n</code></pre> <p>The output should look something like this:</p> Example output of control plane deployment<pre><code>[root@vm11 ~]# sbcli cluster create --ifname=eth0 --ha-type=ha\n2025-02-26 12:37:06,097: INFO: Installing dependencies...\n2025-02-26 12:37:13,338: INFO: Installing dependencies &gt; Done\n2025-02-26 12:37:13,358: INFO: Node IP: 192.168.10.1\n2025-02-26 12:37:13,510: INFO: Configuring docker swarm...\n2025-02-26 12:37:14,199: INFO: Configuring docker swarm &gt; Done\n2025-02-26 12:37:14,200: INFO: Adding new cluster object\nFile moved to /usr/local/lib/python3.9/site-packages/simplyblock_core/scripts/alerting/alert_resources.yaml successfully.\n2025-02-26 12:37:14,269: INFO: Deploying swarm stack ...\n2025-02-26 12:38:52,601: INFO: Deploying swarm stack &gt; Done\n2025-02-26 12:38:52,604: INFO: deploying swarm stack succeeded\n2025-02-26 12:38:52,605: INFO: Configuring DB...\n2025-02-26 12:39:06,003: INFO: Configuring DB &gt; Done\n2025-02-26 12:39:06,106: INFO: Settings updated for existing indices.\n2025-02-26 12:39:06,147: INFO: Template created for future indices.\n2025-02-26 12:39:06,505: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"Cluster\", \"message\": \"Cluster created 7bef076c-82b7-46a5-9f30-8c938b30e655\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,529: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm11\", \"caused_by\": \"cli\"}\n2025-02-26 12:39:06,533: INFO: Done\n2025-02-26 12:39:06,535: INFO: New Cluster has been created\n2025-02-26 12:39:06,535: INFO: 7bef076c-82b7-46a5-9f30-8c938b30e655\n7bef076c-82b7-46a5-9f30-8c938b30e655\n</code></pre> <p>If the deployment was successful, the last line returns the cluster id. This should be noted down. It's required in further steps of the installation.</p> <p>Additionally to the cluster id, the cluster secret is required in many further steps. The following command can be used to retrieve it.</p> <pre><code>sbcli cluster get-secret &lt;CLUSTER_ID&gt;\n</code></pre> Example output get cluster secret<pre><code>[root@vm11 ~]# sbcli cluster get-secret 7bef076c-82b7-46a5-9f30-8c938b30e655\ne8SQ1ElMm8Y9XIwyn8O0\n</code></pre>"},{"location":"deployments/kubernetes/install-simplyblock/hyper-converged/#secondary-management-nodes","title":"Secondary Management Nodes","text":"<p>A production cluster, requires at least three management nodes in the control plane. Hence, additional management nodes need to be added.</p> <p>On the secondary nodes, the network requires the same configuration as on the primary. Executing the commands under Firewall Configuration (CP) will get the node prepared.</p> <p>Afterward, Python, Pip, and <code>sbcli</code> need to be installed.</p> Deployment preparation<pre><code>sudo yum -y install python3-pip\npip install sbcli --upgrade\n</code></pre> <p>Finally, we deploy the management node software and join the control plane cluster.</p> Secondary management node deployment<pre><code>sbcli mgmt add &lt;CP_PRIMARY_IP&gt; &lt;CLUSTER_ID&gt; &lt;CLUSTER_SECRET&gt; &lt;IF_NAME&gt;\n</code></pre> <p>Running against the primary management node in the control plane should create an output similar to the following example:</p> Example output joining a control plane cluster<pre><code>[demo@demo ~]# sbcli mgmt add 192.168.10.1 7bef076c-82b7-46a5-9f30-8c938b30e655 e8SQ1ElMm8Y9XIwyn8O0 eth0\n2025-02-26 12:40:17,815: INFO: Cluster found, NQN:nqn.2023-02.io.simplyblock:7bef076c-82b7-46a5-9f30-8c938b30e655\n2025-02-26 12:40:17,816: INFO: Installing dependencies...\n2025-02-26 12:40:25,606: INFO: Installing dependencies &gt; Done\n2025-02-26 12:40:25,626: INFO: Node IP: 192.168.10.2\n2025-02-26 12:40:26,802: INFO: Joining docker swarm...\n2025-02-26 12:40:27,719: INFO: Joining docker swarm &gt; Done\n2025-02-26 12:40:32,726: INFO: Adding management node object\n2025-02-26 12:40:32,745: INFO: {\"cluster_id\": \"7bef076c-82b7-46a5-9f30-8c938b30e655\", \"event\": \"OBJ_CREATED\", \"object_name\": \"MgmtNode\", \"message\": \"Management node added vm12\", \"caused_by\": \"cli\"}\n2025-02-26 12:40:32,752: INFO: Done\n2025-02-26 12:40:32,755: INFO: Node joined the cluster\ncdde125a-0bf3-4841-a6ef-a0b2f41b8245\n</code></pre> <p>From here, additional management nodes can be added to the control plane cluster. If the control plane cluster is ready, the storage plane can be installed.</p>"},{"location":"important-notes/","title":"Important Notes","text":"<p>Simplyblock is a reliable but powerful block storage solution, optimized for Kubernetes, but compatible with bare metal and virtualized Linux environments.</p> <p>To enable the successful operation of your new simplyblock cluster, this section defines some initial conventions and terminology when working with this documentation.</p>"},{"location":"important-notes/contributing/","title":"Contributing","text":""},{"location":"important-notes/contributing/#contributing-to-simplyblock-documentation","title":"Contributing to Simplyblock Documentation","text":""},{"location":"important-notes/contributing/#overview","title":"Overview","text":"<p>Simplyblock's documentation is publicly available, and we welcome contributions from the community to improve clarity, fix errors, and enhance the overall quality of our documentation. While simplyblock itself is not open source, our documentation is publicly hosted  GitHub. We encourage users to provide feedback, report typos, suggest improvements, and submit fixes for documentation inconsistencies.</p>"},{"location":"important-notes/contributing/#how-to-contribute","title":"How to Contribute","text":"<p>The simplyblock documentation is built using mkdocs, specifically using the mkdocs-material variant.</p> <p>Changes to the documentation can be made by changing or adding the necessary Markdown files.</p>"},{"location":"important-notes/contributing/#1-provide-feedback-or-report-issues","title":"1. Provide Feedback or Report Issues","text":"<p>If you notice any inaccuracies, typos, missing information, or outdated content, you can submit an issue on our GitHub repository:</p> <ol> <li>Navigate to the Simplyblock Documentation GitHub Repository.</li> <li>Click on the Issues tab.</li> <li>Click New Issue and provide a clear description of the problem or suggestion.</li> <li>Submit the issue, and our team will review it.</li> </ol>"},{"location":"important-notes/contributing/#2-make-edits-and-submit-a-pull-request-pr","title":"2. Make Edits and Submit a Pull Request (PR)","text":"<p>If you'd like to make direct changes to the documentation, follow these steps:</p> <ol> <li> <p>Fork the Repository</p> </li> <li> <p>Visit Simplyblock Documentation GitHub and click Fork to create   your own copy of the repository.</p> </li> <li> <p>Clone the Repository</p> </li> <li> <p>Clone your fork to your local machine:   </p><pre><code>git clone https://github.com/YOUR_USERNAME/documentation.git\ncd documentation\n</code></pre> </li> <li> <p>Create a New Branch</p> </li> <li> <p>Always create a new branch for your changes:   </p><pre><code>git checkout -b update-docs\n</code></pre> </li> <li> <p>Make Changes</p> </li> <li> <p>Edit the relevant Markdown (<code>.md</code>) files using a text editor or IDE. The documentation files can be found in the   <code>/docs</code> directory.</p> </li> <li> <p>Ensure that formatting follows existing conventions.</p> </li> <li> <p>Commit and Push Your Changes</p> </li> <li> <p>Commit your changes with a clear message:   </p><pre><code>git commit -m \"Fix typo in installation guide\"\n</code></pre> </li> <li> <p>Push the changes to your fork:   </p><pre><code>git push origin update-docs\n</code></pre> </li> <li> <p>Create a Pull Request (PR)</p> </li> <li> <p>Navigate to the original simplyblock documentation repository.</p> </li> <li>Click New Pull Request and select your branch.</li> <li>Provide a concise description of the changes and submit the PR.</li> <li>Our team will review and merge accepted contributions.</li> </ol>"},{"location":"important-notes/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<ul> <li>Ensure all content remains clear, concise, and professional.</li> <li>Follow Markdown syntax conventions used throughout the documentation.</li> <li>Keep changes focused on documentation improvements (not product functionality).</li> <li>Be respectful and constructive in all discussions and contributions.</li> </ul>"},{"location":"important-notes/contributing/#getting-in-touch","title":"Getting in Touch","text":"<p>If you have questions about contributing, feel free to open an issue or contact us via the simplyblock support channels.</p>"},{"location":"important-notes/documentation-conventions/","title":"Documentation Conventions","text":""},{"location":"important-notes/documentation-conventions/#feature-stages","title":"Feature Stages","text":"<p>Features in simplyblock are released when reaching general availability. However, sometimes, features are made available earlier to receive feedback from testers. Those features must be explicitly enabled and are marked in the documentation accordingly. Features without a specific label are considered ready for production.</p> <p>The documentation uses the following feature stage labels:</p> <ul> <li>General Availability: This is the default stage, if nothing else is defined on the feature. In this stage the   feature is considered ready for production.</li> <li>Technical Preview: The feature is provided for testing and feedback acquisition. It is not considered stable   or complete. Breaking changes may occur, which could break backward compatibility. Features   in this stage are not considered ready for production. Features in this stage need to   be specifically enabled before use.</li> </ul>"},{"location":"important-notes/documentation-conventions/#admonitions-call-outs","title":"Admonitions (Call-Outs)","text":""},{"location":"important-notes/documentation-conventions/#notes","title":"Notes","text":"<p>Notes include additional information which may be interesting but not crucial.</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#recommendations","title":"Recommendations","text":"<p>Recommendations include best practices and recommendations.</p> <p>Recommendation</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#infos","title":"Infos","text":"<p>Information boxes include background and links to additional information.</p> <p>Info</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#warnings","title":"Warnings","text":"<p>Warnings contain crucial information that contain crucial information be considered before proceeding.</p> <p>Warning</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/documentation-conventions/#dangers","title":"Dangers","text":"<p>Dangers contain crucial information that can lead to harmful consequences, such as data loss and irreversible damage.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"important-notes/known-issues/","title":"Known Issues","text":""},{"location":"important-notes/known-issues/#kubernetes","title":"Kubernetes","text":"<ul> <li>Currently, it is not possible to resize a logical volume clone. The resize command does not fail and the new size    is shown by <code>lsblk</code>. But when remounting the filesystem with the option to resize, it fails.</li> <li>High-availability is not working for Graylog. In case of a management node failure, the Graylog monitoring service   will fail over, but recent system logs will be empty. However, historic logs remain on S3. This issue is to be   resolved with the next patch.</li> </ul>"},{"location":"important-notes/known-issues/#aws","title":"AWS","text":"<ul> <li>During a VPC peering connection, all possible CIDRs from the request's VPC should be added to the route table.   Be aware, that there might be more than one CIDR to be added.</li> </ul>"},{"location":"important-notes/known-issues/#bare-metal","title":"Bare Metal","text":"<ul> <li>At the moment, manually scanning for new devices isn't possible (when adding to the nodes). As a workaround, a   storage node restart should be performed. This will automatically detect all newly added devices.</li> </ul>"},{"location":"important-notes/known-issues/#simplyblock","title":"Simplyblock","text":"<ul> <li>Write IO errors after setting new cluster map (2+2, 2-3 nodes, 4-6 devices)</li> <li>2+1: I/O interruption with error when removing a node that was previously added</li> <li>2+1: Expansion migration completes with errors when there is a removed node in the cluster map</li> <li>I/O hangs and I/O errors on low memory (1+1, EBS). As a work-around assign a minimum of 6G of huge page memory to a   storage node</li> </ul>"},{"location":"important-notes/terminology/","title":"Terminology","text":""},{"location":"important-notes/terminology/#storage-related-terms","title":"Storage Related Terms","text":""},{"location":"important-notes/terminology/#storage-pool","title":"Storage Pool","text":"<p>A storage pool is a logical aggregation of multiple physical storage devices that provides a flexible and scalable foundation for managing storage resources. By pooling storage from different drives or nodes, a storage pool enables efficient capacity management, redundancy, and performance optimization through techniques such as thin provisioning, replication, and erasure coding. Storage pools are commonly used in software-defined storage (SDS), hyper-converged infrastructure (HCI), and enterprise storage systems to simplify storage provisioning and dynamically allocate space based on workload demands. This abstraction layer improves fault tolerance, scalability, and resource utilization in modern storage architectures.</p>"},{"location":"important-notes/terminology/#storage-device","title":"Storage Device","text":"<p>A storage device is a hardware component or system that stores and retrieves digital data in computing environments. Storage devices can be classified into different types based on technology and access speed, including hard disk drives (HDDs), solid-state drives (SSDs), NVMe drives, and optical or tape storage. They can be locally attached to a single machine or shared across multiple systems in networked storage architectures such as Storage Area Networks (SANs) and Network-Attached Storage (NAS). Modern distributed and cloud environments leverage software-defined storage (SDS) to manage multiple storage devices efficiently, ensuring scalability, redundancy, and optimized data access for various applications.</p>"},{"location":"important-notes/terminology/#nvme-non-volatile-memory-express","title":"NVMe (Non-Volatile Memory Express)","text":"<p>NVMe (Non-Volatile Memory Express) is a high-performance storage protocol explicitly designed for flash-based storage devices like SSDs, leveraging the PCIe (Peripheral Component Interconnect Express) interface for ultra-low latency and high throughput. Unlike traditional protocols such as SATA or SAS, NVMe takes advantage of parallelism and multiple queues, significantly improving data transfer speeds and reducing CPU overhead. It is widely used in enterprise storage, cloud computing, and high-performance computing (HPC) environments, where speed and efficiency are critical. NVMe is also the foundation for NVMe-over-Fabrics (NVMe-oF), which extends its benefits across networked storage systems, enhancing scalability and flexibility in distributed environments.</p>"},{"location":"important-notes/terminology/#nvme-of-nvme-over-fabrics","title":"NVMe-oF (NVMe over Fabrics)","text":"<p>NVMe-oF (NVMe over Fabrics) is an extension of the NVMe (Non-Volatile Memory Express) protocol that enables high-performance, low-latency access to remote NVMe storage devices over network fabrics such as TCP, RDMA (RoCE, iWARP), and Fibre Channel (FC). Unlike traditional networked storage protocols, NVMe-oF maintains the efficiency and parallelism of direct-attached NVMe storage while allowing disaggregation of compute and storage resources. This architecture improves scalability, resource utilization, and flexibility in cloud, enterprise, and high-performance computing (HPC) environments. NVMe-oF is a key technology in modern software-defined and disaggregated storage infrastructures, providing fast and efficient remote storage access.</p>"},{"location":"important-notes/terminology/#nvmetcp-nvme-over-tcp","title":"NVMe/TCP (NVMe over TCP)","text":"<p>NVMe/TCP (NVMe over TCP) is a transport protocol that extends NVMe-over-Fabrics (NVMe-oF) using standard TCP/IP networks to enable high-performance, low-latency access to remote NVMe storage. By leveraging existing Ethernet infrastructure, NVMe/TCP eliminates the need for specialized networking hardware such as RDMA (RoCE or iWARP) or Fibre Channel (FC), making it a cost-effective and easily deployable solution for cloud, enterprise, and data center storage environments. It maintains the efficiency of NVMe, providing scalable, high-throughput, and low-latency remote storage access while ensuring broad compatibility with modern network architectures.</p>"},{"location":"important-notes/terminology/#nvmeroce-nvme-over-rdma-over-converged-ethernet","title":"NVMe/RoCE (NVMe over RDMA over Converged Ethernet)","text":"<p>NVMe/RoCE (NVMe over RoCE) is a high-performance storage transport protocol that extends NVMe-over-Fabrics (NVMe-oF) using RDMA over Converged Ethernet (RoCE) to enable ultra-low-latency and high-throughput access to remote NVMe storage devices. By leveraging Remote Direct Memory Access (RDMA), NVMe/RoCE bypasses the CPU for data transfers, reducing latency and improving efficiency compared to traditional TCP-based storage protocols. This makes it ideal for high-performance computing (HPC), enterprise storage, and latency-sensitive applications such as financial trading and AI workloads. NVMe/RoCE requires lossless Ethernet networking and specialized NICs to fully utilize its performance advantages.</p>"},{"location":"important-notes/terminology/#multipathing","title":"Multipathing","text":"<p>Multipathing is a storage networking technique that enables multiple physical paths between a compute system and a storage device to improve redundancy, load balancing, and fault tolerance. Multipathing enhances performance and reliability by using multiple connections, ensuring continuous access to storage even if one path fails. It is commonly implemented in Fibre Channel (FC), iSCSI, and NVMe-oF (including NVMe/TCP and NVMe/RoCE) environments, where high availability and optimized data transfer are critical.</p>"},{"location":"important-notes/terminology/#storage-node","title":"Storage Node","text":"<p>A storage node in a distributed storage cluster is a physical or virtual machine that contributes storage resources to the cluster, providing a portion of the overall storage capacity and participating in data distribution, redundancy, and retrieval processes. Each storage node typically runs specialized storage software to manage data placement, replication, and access, ensuring high availability and fault tolerance. In modern distributed storage architectures, storage nodes communicate with one another to maintain data consistency, balance workloads, and optimize performance, often using techniques such as erasure coding or replication to safeguard against node failures.</p>"},{"location":"important-notes/terminology/#management-node","title":"Management Node","text":"<p>A management node is a containerized component that orchestrates, monitors, and controls the distributed storage cluster. It forms part of the control plane, managing cluster-wide configurations, provisioning logical volumes, handling metadata operations, and ensuring overall system health. Management nodes facilitate communication between storage nodes and client applications, enforcing policies such as access control, data placement, and fault tolerance. They also provide an interface for administrators to interact with the storage system via the Simplyblock CLI or API, enabling seamless deployment, scaling, and maintenance of the storage infrastructure.</p>"},{"location":"important-notes/terminology/#storage-cluster","title":"Storage Cluster","text":"<p>A storage cluster is a group of interconnected storage nodes that work together to provide a scalable, fault-tolerant, and high-performance storage system. Unlike traditional single-node storage solutions, storage clusters distribute data across multiple nodes, ensuring redundancy, load balancing, and resilience against hardware failures. To optimize data availability and efficiency, these clusters can be configured using different architectures, such as replication, erasure coding, or software-defined storage (SDS). Storage clusters are commonly used in cloud storage, high-performance computing (HPC), and enterprise data centers, enabling seamless scalability and improved data accessibility across distributed environments.</p>"},{"location":"important-notes/terminology/#erasure-coding","title":"Erasure Coding","text":"<p>Erasure coding is a data protection technique used in distributed storage systems to provide fault tolerance and redundancy while minimizing storage overhead. It works by breaking data into k data fragments and generating m parity fragments using mathematical algorithms. These k + m fragments are then distributed across multiple storage nodes, allowing the system to reconstruct lost or corrupted data from any k available fragments. Compared to traditional replication, erasure coding offers greater storage efficiency while maintaining high availability, making it ideal for cloud storage, object storage, and high-performance computing (HPC) environments where durability and cost-effectiveness are critical.</p>"},{"location":"important-notes/terminology/#replication","title":"Replication","text":"<p>Replication in storage is the process of creating and maintaining identical copies of data across multiple storage devices or nodes to ensure fault tolerance, high availability, and disaster recovery. Replication can occur synchronously, where data is copied in real-time to ensure consistency, or asynchronously, where updates are delayed to optimize performance. It is commonly used in distributed storage systems, cloud storage, and database management to protect against hardware failures and data loss. By maintaining redundant copies, replication enhances data resilience, load balancing, and accessibility, making it a fundamental technique for enterprise and cloud-scale storage solutions.</p>"},{"location":"important-notes/terminology/#raid-redundant-array-of-independent-disks","title":"RAID (Redundant Array of Independent Disks)","text":"<p>RAID (Redundant Array of Independent Disks) is a data storage technology that combines multiple physical drives into a single logical unit to improve performance, fault tolerance, or both. RAID configurations vary based on their purpose: RAID 0 (striping) enhances speed but offers no redundancy, RAID 1 (mirroring) duplicates data for high availability and RAID 5, 6, and 10 use combinations of striping and parity to balance performance and fault tolerance. RAID is widely used in enterprise storage, servers, and high-performance computing to protect against drive failures and optimize data access. It can be implemented in hardware controllers or software-defined storage solutions, depending on system requirements.</p>"},{"location":"important-notes/terminology/#quality-of-service","title":"Quality of Service","text":"<p>Quality of Service (QoS) refers to the ability to define and enforce performance guarantees for storage workloads by controlling key metrics such as IOPS (Input/Output Operations Per Second), throughput, and latency. QoS ensures that different applications receive appropriate levels of performance, preventing resource contention in multi-tenant environments. By setting limits and priorities for Logical Volumes (LVs), Simplyblock allows administrators to allocate storage resources efficiently, ensuring critical workloads maintain consistent performance even under high demand. This capability is essential for optimizing storage operations, improving reliability, and meeting service-level agreements (SLAs) in distributed cloud-native environments.</p>"},{"location":"important-notes/terminology/#spdk-storage-performance-development-kit","title":"SPDK (Storage Performance Development Kit)","text":"<p>Storage Performance Development Kit (SPDK) is an open-source set of libraries and tools designed to optimize high-performance, low-latency storage applications by bypassing traditional kernel-based I/O processing. SPDK leverages user-space and polled-mode drivers to eliminate context switching and interrupts, significantly reducing CPU overhead and improving throughput. It is particularly suited for NVMe storage, NVMe-over-Fabrics (NVMe-oF), and iSCSI target acceleration, making it a key technology in software-defined storage solutions. By providing a highly efficient framework for storage processing, SPDK enables modern storage architectures to achieve high IOPS, reduced latency, and better resource utilization in cloud and enterprise environments.</p>"},{"location":"important-notes/terminology/#volume-snapshot","title":"Volume Snapshot","text":"<p>A volume snapshot is a point-in-time copy of a storage volume, file system, or virtual machine that captures its state without duplicating the entire data set. Snapshots enable rapid data recovery, backup, and versioning by preserving only the changes made since the last snapshot, often using copy-on-write (COW) or redirect-on-write (ROW) techniques to minimize storage overhead. They are commonly used in enterprise storage, cloud environments, and virtualized systems to ensure data consistency, quick rollback capabilities, and protection against accidental deletions or system failures. Unlike full backups, snapshots are lightweight and allow near-instantaneous recovery of data.</p>"},{"location":"important-notes/terminology/#volume-clone","title":"Volume Clone","text":"<p>A volume clone is an exact, fully independent copy of a storage volume, virtual machine, or dataset that can be used for testing, development, backup, or deployment purposes. Unlike snapshots, which capture a point-in-time state and depend on the original data, a clone is a complete duplication that can operate separately without relying on the source. Cloning is commonly used in enterprise storage, cloud environments, and containerized applications to create quick, reproducible environments for workloads without affecting the original data. Storage systems often use thin cloning to optimize space by sharing unchanged data blocks between the original and the clone, reducing storage overhead. COW is widely implemented in storage virtualization and containerized environments, enabling fast, space-efficient backups, cloning, and data protection while maintaining high system performance.</p>"},{"location":"important-notes/terminology/#cow-copy-on-write","title":"CoW (Copy-on-Write)","text":"<p>Copy-on-Write (COW) is an efficient data management technique used in snapshots, cloning, and memory management to optimize storage usage and performance. Instead of immediately duplicating data, COW defers copying until a modification is made, ensuring that only changed data blocks are written to a new location. This approach minimizes storage overhead, speeds up snapshot creation, and reduces unnecessary data duplication.</p> <p></p>"},{"location":"important-notes/terminology/#kubernetes-related-terms","title":"Kubernetes Related Terms","text":""},{"location":"important-notes/terminology/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes (K8s) is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications across clusters of machines. Initially developed by Google and now maintained by the Cloud Native Computing Foundation (CNCF){: target=\"_blank\"}, Kubernetes provides a robust framework for load balancing, self-healing, storage orchestration, and automated rollouts and rollbacks. It manages application workloads using Pods, Deployments, Services, and Persistent Volumes (PVs), ensuring scalability and resilience. By abstracting underlying infrastructure, Kubernetes enables organizations to efficiently run containerized applications across on-premises, cloud, and hybrid environments, making it a cornerstone of modern cloud-native computing.</p>"},{"location":"important-notes/terminology/#kubernetes-csi-container-storage-interface","title":"Kubernetes CSI (Container Storage Interface)","text":"<p>The Kubernetes Container Storage Interface (CSI) is a standardized API enabling external storage providers to integrate their storage solutions with Kubernetes. CSI allows Kubernetes to dynamically provision, attach, mount, and manage Persistent Volumes (PVs) across different storage backends without requiring changes to the Kubernetes core. Using a CSI driver, storage vendors can offer block and file storage to Kubernetes workloads, supporting advanced features like snapshotting, cloning, and volume expansion. CSI enhances Kubernetes\u2019 flexibility by enabling seamless integration with cloud, on-premises, and software-defined storage solutions, making it the de facto method for managing storage in containerized environments.</p>"},{"location":"important-notes/terminology/#pod","title":"Pod","text":"<p>A Pod in Kubernetes is the smallest and most basic deployable unit, representing a single instance of a running process in a cluster. A Pod can contain one or multiple containerized applications that share networking, storage, and runtime configurations, enabling efficient communication and resource sharing. Kubernetes schedules and manages Pods, ensuring they are deployed on suitable worker nodes based on resource availability and constraints. Since Pods are ephemeral, they are often managed by higher-level controllers like Deployments, StatefulSets, or DaemonSets to maintain availability and scalability. Pods facilitate scalable, resilient, and cloud-native application deployments across diverse infrastructure environments.</p>"},{"location":"important-notes/terminology/#persistent-volume","title":"Persistent Volume","text":"<p>A Persistent Volume (PV) is a cluster-wide Kubernetes storage resource that provides durable and independent storage for Pods, allowing data to persist beyond the lifecycle of individual containers. Unlike ephemeral storage, which is tied to a Pod\u2019s runtime, a PV is provisioned either statically by an administrator or dynamically using StorageClasses. Applications request storage by creating Persistent Volume Claims (PVCs), which Kubernetes binds to an available PV based on capacity and access requirements. Persistent Volumes support different access modes, such as ReadWriteOnce ( RWO), ReadOnlyMany (ROX), and ReadWriteMany (RWX), and are backed by various storage solutions, including local disks, network-attached storage (NAS), and cloud-based storage services.</p>"},{"location":"important-notes/terminology/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>A Persistent Volume Claim (PVC) is a request for Kubernetes storage made by a Pod, allowing it to dynamically or statically access a Persistent Volume (PV). PVCs specify storage requirements such as size, access mode (ReadWriteOnce, ReadOnlyMany, or ReadWriteMany), and storage class. Kubernetes automatically binds a PVC to a suitable PV based on these criteria, abstracting the underlying storage details from applications. This separation enables dynamic storage provisioning, ensuring that Pods can seamlessly consume persistent storage resources without needing direct knowledge of the storage infrastructure. When a PVC is deleted, its associated PV handling depends on its reclaim policy (Retain, Recycle, or Delete), determining whether the storage is preserved, cleared, or removed.</p>"},{"location":"important-notes/terminology/#storage-class","title":"Storage Class","text":"<p>A StorageClass is a Kubernetes abstraction that defines different types of storage available within a cluster, enabling dynamic provisioning of Persistent Volumes (PVs). It allows administrators to specify storage requirements such as performance characteristics, replication policies, and backend storage providers (e.g., cloud block storage, network file systems, or distributed storage systems). Each StorageClass includes a provisioner, which determines how volumes are created and parameters that define specific configurations for the underlying storage system. By referencing a StorageClass in a Persistent Volume Claim (PVC), users can automatically provision storage that meets their application's needs without manually pre-allocating PVs, streamlining storage management in cloud-native environments.</p>"},{"location":"important-notes/terminology/#network-related-terms","title":"Network Related Terms","text":""},{"location":"important-notes/terminology/#tcp-transmission-control-protocol","title":"TCP (Transmission Control Protocol)","text":"<p>Transmission Control Protocol (TCP) is a core communication protocol in the Internet Protocol (IP) suite that ensures reliable, ordered, and error-checked data delivery between devices over a network. TCP operates at the transport layer and establishes a connection-oriented communication channel using a three-way handshake process to synchronize data exchange. It segments large data streams into smaller packets, ensures their correct sequencing, and retransmits lost packets to maintain data integrity. TCP is widely used in applications requiring stable and accurate data transmission, such as web browsing, email, and file transfers, making it a fundamental protocol for modern networked systems.</p>"},{"location":"important-notes/terminology/#udp-user-datagram-protocol","title":"UDP (User Datagram Protocol)","text":"<p>User Datagram Protocol (UDP) is a lightweight, connectionless communication protocol in the Internet Protocol (IP) suite that enables fast, low-latency data transmission without guaranteeing delivery, order, or error correction. Unlike Transmission Control Protocol (TCP), UDP does not establish a connection before sending data, making it more efficient for applications prioritizing speed over reliability. It is commonly used in real-time communications, streaming services, online gaming, and DNS lookups, where occasional data loss is acceptable in exchange for reduced latency and overhead.</p>"},{"location":"important-notes/terminology/#ip-internet-protocol-ipv4-ipv6","title":"IP (Internet Protocol), IPv4, IPv6","text":"<p>Internet Protocol (IP) is the fundamental networking protocol that enables devices to communicate over the Internet and private networks by assigning unique IP addresses to each device. Operating at the network layer of the Internet Protocol suite, IP is responsible for routing and delivering data packets from a source to a destination based on their addresses. It functions in a connectionless manner, meaning each packet is sent independently and may take different paths to reach its destination. IP exists in two primary versions: IPv4, which uses 32-bit addresses, and IPv6, which uses 128-bit addresses for expanded address space. IP works alongside transport layer protocols like TCP and UDP to ensure effective data transmission across networks.</p>"},{"location":"important-notes/terminology/#netmask","title":"Netmask","text":"<p>A netmask is a numerical value used in IP networking to define a subnet's range of IP addresses. It works by masking a portion of an IP address to distinguish the network part from the host part. A netmask consists of a series of binary ones (1s) followed by zeros (0s), where the ones represent the network portion and the zeros indicate the host portion. Common netmasks include 255.255.255.0 (/24) for standard subnets and 255.255.0.0 (/16) for larger networks. Netmasks are essential in subnetting, routing, and IP address allocation, ensuring efficient traffic management and communication within networks.</p>"},{"location":"important-notes/terminology/#cidr-classless-inter-domain-routing","title":"CIDR (Classless Inter-Domain Routing)","text":"<p>Classless Inter-Domain Routing (CIDR) is a method for allocating and managing IP addresses more efficiently than the traditional class-based system. CIDR uses variable-length subnet masking (VLSM) to define IP address ranges with flexible subnet sizes, reducing wasted addresses and improving routing efficiency. CIDR notation represents an IP address followed by a slash (/) and a number indicating the number of significant bits in the subnet mask (e.g., <code>192.168.1.0/24</code> means the first 24 bits define the network, leaving 8 bits for host addresses). Widely used in modern networking and the internet, CIDR helps optimize IP address distribution and enhances routing aggregation, reducing the size of global routing tables.</p>"},{"location":"important-notes/terminology/#hyper-converged","title":"Hyper-Converged","text":"<p>Hyper-converged refers to an IT infrastructure model that integrates compute, storage, and networking into a single, software-defined system. Unlike traditional architectures that rely on separate hardware components for each function, hyper-converged infrastructure (HCI) leverages virtualization and centralized management to streamline operations, improve scalability, and reduce complexity. This approach enhances performance, fault tolerance, and resource efficiency by distributing workloads across multiple nodes, allowing seamless scaling by simply adding more nodes. HCI is widely used in cloud environments, virtual desktop infrastructure (VDI), and enterprise data centers for its ease of deployment, automation capabilities, and cost-effectiveness.</p>"},{"location":"important-notes/terminology/#disaggregated","title":"Disaggregated","text":"<p>Disaggregated refers to an IT architecture approach where compute, storage, and networking resources are separated into independent components rather than being tightly integrated within the same physical system. In disaggregated storage, for example, storage resources are managed independently of compute nodes, allowing for flexible scaling, improved resource utilization, and reduced hardware dependencies. This contrasts with traditional or hyper-converged architectures, where these resources are combined. Disaggregated architectures are widely used in cloud computing, high-performance computing (HPC), and modern data centers to enhance scalability, cost-efficiency, and operational flexibility while optimizing performance for dynamic workloads.</p>"},{"location":"maintenance-operations/","title":"Operations","text":"<p>Ensuring data resilience and maintaining cluster health are critical aspects of managing a simplyblock storage deployment. This section covers best practices for backing up and restoring individual volumes or entire clusters, helping organizations safeguard their data against failures, corruption, or accidental deletions.</p> <p>Additionally, simplyblock provides comprehensive monitoring capabilities using built-in Prometheus and Grafana for real-time visualization of cluster health, I/O statistics, and performance metrics.</p> <p>For organizations leveraging third-party monitoring solutions, simplyblock supports integration with Datadog, AppDynamics, Dynatrace, and other observability platforms, enabling centralized performance tracking and alerting.</p> <p>This section details how to configure and use these monitoring tools, ensuring optimal performance, early issue detection, and proactive storage management in cloud-native and enterprise environments.</p>"},{"location":"maintenance-operations/cluster-replication/","title":"Cluster Replication","text":""},{"location":"maintenance-operations/disaster-recovery/","title":"Disaster Recovery","text":""},{"location":"maintenance-operations/upgrade/","title":"Upgrade","text":""},{"location":"maintenance-operations/backup-restore/","title":"Backup and Restore","text":""},{"location":"maintenance-operations/backup-restore/cluster/","title":"Cluster Backup and Restore","text":""},{"location":"maintenance-operations/backup-restore/volume/","title":"Volume Backup and Restore","text":""},{"location":"maintenance-operations/monitoring/","title":"Monitoring","text":""},{"location":"maintenance-operations/monitoring/accessing-grafana/","title":"Accessing Grafana","text":""},{"location":"maintenance-operations/monitoring/cluster-health/","title":"Cluster Health","text":""},{"location":"maintenance-operations/monitoring/io-stats/","title":"Accessing I/O Stats (sbcli)","text":""},{"location":"maintenance-operations/monitoring/lvol-conditions/","title":"Logical Volume Conditions","text":""},{"location":"maintenance-operations/monitoring/prometheus/","title":"Prometheus Metrics","text":""},{"location":"maintenance-operations/monitoring/third-party/","title":"Integration with Third-Party Tools","text":""},{"location":"maintenance-operations/monitoring/third-party/appdynamics/","title":"AppDynamics","text":""},{"location":"maintenance-operations/monitoring/third-party/datadog/","title":"DataDog","text":""},{"location":"maintenance-operations/monitoring/third-party/dynatrace/","title":"DynaTrace","text":""},{"location":"maintenance-operations/scaling/","title":"Scaling","text":""},{"location":"maintenance-operations/scaling/aws-ec2-autoscaling/","title":"AWS EC2 Autoscaling","text":""},{"location":"maintenance-operations/scaling/expanding-storage-pool/","title":"Expanding a Storage Pool","text":""},{"location":"maintenance-operations/scaling/kubernetes-autoscaling/","title":"Kubernetes Autoscaling","text":""},{"location":"maintenance-operations/security/","title":"Security","text":""},{"location":"maintenance-operations/security/encryption-aws/","title":"Encrypting with AWS","text":""},{"location":"maintenance-operations/security/encryption-kubernetes-secrets/","title":"Encrypting with Kubernetes Secrets","text":""},{"location":"maintenance-operations/security/multi-tenancy/","title":"Multi-Tenancy","text":""},{"location":"maintenance-operations/security/securing-simplyblock/","title":"Securing Simplyblock","text":""},{"location":"reference/","title":"Reference","text":"<p>Simplyblock provides multiple interfaces for managing and interacting with its distributed storage system, including the <code>sbcli</code> command-line interface (CLI) and Management API. The <code>sbcli</code> CLI offers a powerful, scriptable way to perform essential operations such as provisioning, expanding, snapshotting, and cloning logical volumes, making it ideal for administrators who prefer direct command-line access.</p> <p>The simplyblock Management API enables integration with external automation and orchestration tools, allowing seamless management of storage resources at scale. Additionally, this section includes a reference list of supported Linux kernels and distributions, ensuring compatibility across various environments.</p>"},{"location":"reference/api-sdk/","title":"API / Developer SDK","text":"<p>Simplyblock offers a comprehensive API to manage and automate cluster operations. This includes all cluster-wide operations, logical volume specific operations, health information, and </p> <ul> <li>Retrieve information about the cluster and its health status</li> <li>Automatically manage a logical volume lifecycle</li> <li>Integrate simplyblock into deployment processes and workflow automations</li> <li>Create custom alerts and warnings</li> </ul>"},{"location":"reference/api-sdk/#authentication","title":"Authentication","text":"<p>Any request to the simplyblock API requires authorization information to be provided. Unauthorized requests return an HTTP status 401 (Unauthorized).</p> <p>To provide authorization information, the simplyblock API uses the Authorization HTTP header with a combination of the cluster uuid and the cluster secret.</p> <p>HTTP Authorization header: </p><pre><code>Authorization: &lt;CLUSTER_UUID&gt; &lt;CLUSTER_SECRET&gt;\n</code></pre> <p>The cluster id is provided during the initial cluster installation. The cluster secret can be obtained using the simplyblock commandline interface tool <code>sbcli</code>.</p> <pre><code>sbcli cluster get-secret CLUSTER_UUID\n</code></pre>"},{"location":"reference/api-sdk/#api-documentation","title":"API Documentation","text":"<p>The full API documentation is hosted on Postman. You can find the full API collection on the Postman API project.</p>"},{"location":"reference/cli/","title":"CLI","text":"<p>Purpose: </p> <p>Format:</p> <p>Type:</p> <p>Remarks: Simplyblock provides a feature-rich CLI (command line interface) client to manage all aspects of the storage cluster.</p> <pre><code>usage: sbcli [-h] [-d]\n             {storage-node,sn,cluster,lvol,mgmt,pool,snapshot,caching-node,cn}\n             ...\n\npositional arguments:\n  {storage-node,sn,cluster,lvol,mgmt,pool,snapshot,caching-node,cn}\n    storage-node (sn)   Storage node commands\n    cluster             Cluster commands\n    lvol                LVol commands\n    mgmt                Management node commands\n    pool                Pool commands\n    snapshot            Snapshot commands\n    caching-node (cn)   Caching client node commands\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d, --debug           Print debug messages\n</code></pre>"},{"location":"reference/cli/#storage-node-commands","title":"Storage Node Commands","text":"<pre><code>usage: sbcli storage-node [-h]\n                          {deploy,deploy-cleaner,add-node,delete,remove,list,get,restart,shutdown,suspend,resume,get-io-stats,get-capacity,list-devices,device-testing-mode,get-device,reset-device,restart-device,add-device,remove-device,get-capacity-device,get-io-stats-device,port-list,port-io-stats,check,check-device,info,info-spdk}\n                          ...\n\npositional arguments:\n  {deploy,deploy-cleaner,add-node,delete,remove,list,get,restart,shutdown,suspend,resume,get-io-stats,get-capacity,list-devices,device-testing-mode,get-device,reset-device,restart-device,add-device,remove-device,get-capacity-device,get-io-stats-device,port-list,port-io-stats,check,check-device,info,info-spdk}\n    deploy              Deploy local services for remote ops (local run)\n    deploy-cleaner      clean local deploy (local run)\n    add-node            Add storage node by ip\n    delete              Delete storage node obj\n    remove              Remove storage node\n    list                List storage nodes\n    get                 Get storage node info\n    restart             Restart a storage node\n    shutdown            Shutdown a storage node\n    suspend             Suspend a storage node\n    resume              Resume a storage node\n    get-io-stats        Get node IO statistics\n    get-capacity        Get node capacity statistics\n    list-devices        List storage devices\n    device-testing-mode\n                        Set device testing mode\n    get-device          Get storage device by id\n    reset-device        Reset storage device\n    restart-device      Restart storage device\n    add-device          Add a new storage device\n    remove-device       Remove a storage device\n    get-capacity-device\n                        Get device capacity\n    get-io-stats-device\n                        Get device IO statistics\n    port-list           Get Data interfaces list for a node\n    port-io-stats       Get Data interfaces IO stats\n    check               Health check storage node\n    check-device        Health check device\n    info                Get node information\n    info-spdk           Get SPDK memory information\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#deploy-local-services-for-remote-ops-execute-locally","title":"Deploy Local Services for Remote Ops (execute locally)","text":"<pre><code>usage: sbcli storage-node deploy [-h] [--ifname IFNAME]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --ifname IFNAME  Management interface name, default: eth0\n</code></pre>"},{"location":"reference/cli/#clean-local-deploy-local-run","title":"Clean local deploy (local run)","text":"<pre><code>usage: sbcli storage-node deploy-cleaner [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#add-storage-node-by-ip","title":"Add Storage Node by IP","text":"<pre><code>usage: sbcli storage-node add-node [-h] [--partitions PARTITIONS]\n                                   [--jm-percent JM_PERCENT]\n                                   [--data-nics DATA_NICS [DATA_NICS ...]]\n                                   [--cpu-mask SPDK_CPU_MASK]\n                                   [--memory SPDK_MEM]\n                                   [--spdk-image SPDK_IMAGE] [--spdk-debug]\n                                   [--iobuf_small_pool_count SMALL_POOL_COUNT]\n                                   [--iobuf_large_pool_count LARGE_POOL_COUNT]\n                                   [--iobuf_small_bufsize SMALL_BUFSIZE]\n                                   [--iobuf_large_bufsize LARGE_BUFSIZE]\n                                   cluster_id node_ip ifname\n\npositional arguments:\n  cluster_id            UUID of the cluster to which the node will belong\n  node_ip               IP of storage node to add\n  ifname                Management interface name\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --partitions PARTITIONS\n                        Number of partitions to create per device\n  --jm-percent JM_PERCENT\n                        Number in percent to use for JM from each device\n  --data-nics DATA_NICS [DATA_NICS ...]\n                        Data interface names\n  --cpu-mask SPDK_CPU_MASK\n                        SPDK app CPU mask, default is all cores found\n  --memory SPDK_MEM     SPDK huge memory allocation, default is 4G\n  --spdk-image SPDK_IMAGE\n                        SPDK image uri\n  --spdk-debug          Enable spdk debug logs\n  --iobuf_small_pool_count SMALL_POOL_COUNT\n                        bdev_set_options param\n  --iobuf_large_pool_count LARGE_POOL_COUNT\n                        bdev_set_options param\n  --iobuf_small_bufsize SMALL_BUFSIZE\n                        bdev_set_options param\n  --iobuf_large_bufsize LARGE_BUFSIZE\n                        bdev_set_options param\n</code></pre>"},{"location":"reference/cli/#delete-storage-node-object","title":"Delete Storage Node Object","text":"<pre><code>usage: sbcli storage-node delete [-h] node_id\n\npositional arguments:\n  node_id     UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#remove-storage-node","title":"Remove Storage Node","text":"<pre><code>usage: sbcli storage-node remove [-h] [--force-remove] [--force-migrate]\n                                 node_id\n\npositional arguments:\n  node_id          UUID of storage node\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --force-remove   Force remove all LVols and snapshots\n  --force-migrate  Force migrate All LVols to other nodes\n</code></pre>"},{"location":"reference/cli/#list-storage-nodes","title":"List Storage Nodes","text":"<pre><code>usage: sbcli storage-node list [-h] [--cluster-id CLUSTER_ID] [--json]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cluster-id CLUSTER_ID\n                        id of the cluster for which nodes are listed\n  --json                Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#get-storage-node-info","title":"Get Storage Node Info","text":"<pre><code>usage: sbcli storage-node get [-h] id\n\npositional arguments:\n  id          UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#restart-storage-node","title":"Restart Storage Node","text":"<p>All functions and device drivers will be reset. During restart, the node does not accept IO. In a high-availability setup, this will not impact operations</p> <pre><code>usage: sbcli storage-node restart [-h] [--cpu-mask SPDK_CPU_MASK]\n                                  [--memory SPDK_MEM]\n                                  [--spdk-image SPDK_IMAGE] [--spdk-debug]\n                                  [--iobuf_small_pool_count SMALL_POOL_COUNT]\n                                  [--iobuf_large_pool_count LARGE_POOL_COUNT]\n                                  [--iobuf_small_bufsize SMALL_BUFSIZE]\n                                  [--iobuf_large_bufsize LARGE_BUFSIZE]\n                                  node_id\n\npositional arguments:\n  node_id               UUID of storage node\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cpu-mask SPDK_CPU_MASK\n                        SPDK app CPU mask, default is all cores found\n  --memory SPDK_MEM     SPDK huge memory allocation, default is 4G\n  --spdk-image SPDK_IMAGE\n                        SPDK image uri\n  --spdk-debug          Enable spdk debug logs\n  --iobuf_small_pool_count SMALL_POOL_COUNT\n                        bdev_set_options param\n  --iobuf_large_pool_count LARGE_POOL_COUNT\n                        bdev_set_options param\n  --iobuf_small_bufsize SMALL_BUFSIZE\n                        bdev_set_options param\n  --iobuf_large_bufsize LARGE_BUFSIZE\n                        bdev_set_options param\n</code></pre>"},{"location":"reference/cli/#shutdown-storage-node","title":"Shutdown Storage Node","text":"<p>Once the command is issued, the node will stop accepting IO,but IO, which was previously received, will still be processed. In a high-availability setup, this will not impact operations.</p> <pre><code>usage: sbcli storage-node shutdown [-h] [--force] node_id\n\npositional arguments:\n  node_id     UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force node shutdown\n</code></pre>"},{"location":"reference/cli/#suspend-storage-node","title":"Suspend Storage Node","text":"<p>The node will stop accepting new IO, but will finish processing any IO, which has been received already.</p> <pre><code>usage: sbcli storage-node suspend [-h] [--force] node_id\n\npositional arguments:\n  node_id     UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force node suspend\n</code></pre>"},{"location":"reference/cli/#resume-storage-node","title":"Resume Storage Node","text":"<pre><code>usage: sbcli storage-node resume [-h] node_id\n\npositional arguments:\n  node_id     UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-node-io-statistics","title":"Get Node IO Statistics","text":"<pre><code>usage: sbcli storage-node get-io-stats [-h] [--history HISTORY] node_id\n\npositional arguments:\n  node_id            Node ID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  list history records -one for every 15 minutes- for XX\n                     days and YY hours -up to 10 days in total-, format:\n                     XXdYYh\n</code></pre>"},{"location":"reference/cli/#get-node-capacity-statistics","title":"Get Node Capacity Statistics","text":"<pre><code>usage: sbcli storage-node get-capacity [-h] [--history HISTORY] node_id\n\npositional arguments:\n  node_id            Node ID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  list history records -one for every 15 minutes- for XX\n                     days and YY hours -up to 10 days in total-, format:\n                     XXdYYh\n</code></pre>"},{"location":"reference/cli/#list-storage-devices","title":"List Storage Devices","text":"<pre><code>usage: sbcli storage-node list-devices [-h] [-s {node-seq,dev-seq,serial}]\n                                       [--json]\n                                       node_id\n\npositional arguments:\n  node_id               the node's UUID\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s {node-seq,dev-seq,serial}, --sort {node-seq,dev-seq,serial}\n                        Sort the outputs\n  --json                Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#set-device-testing-mode","title":"Set Device Testing Mode","text":"<pre><code>usage: sbcli storage-node device-testing-mode [-h]\n                                              device_id\n                                              {full_pass_through,io_error_on_read,io_error_on_write,io_error_on_unmap,io_error_on_all,discard_io_all,hotplug_removal}\n\npositional arguments:\n  device_id             Device UUID\n  {full_pass_through,io_error_on_read,io_error_on_write,io_error_on_unmap,io_error_on_all,discard_io_all,hotplug_removal}\n                        Testing mode\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-storage-device-by-id","title":"Get Storage Device by Id","text":"<pre><code>usage: sbcli storage-node get-device [-h] device_id\n\npositional arguments:\n  device_id   the devices's UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#reset-storage-device","title":"Reset Storage Device","text":"<p>Hardware device reset. Resetting the device can return the device from an unavailable into online state, if successful</p> <pre><code>usage: sbcli storage-node reset-device [-h] device_id\n\npositional arguments:\n  device_id   the devices's UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#restart-storage-device","title":"Restart Storage Device","text":"<p>a previously removed or unavailable device may be returned into online state. If the device is not physically present, accessible or healthy, it will flip back into unavailable state again.</p> <pre><code>usage: sbcli storage-node restart-device [-h] id\n\npositional arguments:\n  id          the devices's UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#add-new-storage-device","title":"Add new Storage Device","text":"<p>Adding a device will include a previously detected device (currently in \"new\" state) into cluster and will launch and auto-rebalancing background process in which some cluster capacity is re-distributed to this newly added device.</p> <pre><code>usage: sbcli storage-node add-device [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#remove-storage-device","title":"Remove Storage Device","text":"<p>The device will become unavailable, independently if it was physically removed from the server. This function can be used if auto-detection of removal did not work or if the device must be maintained otherwise while remaining inserted into the server.</p> <pre><code>usage: sbcli storage-node remove-device [-h] [--force] device_id\n\npositional arguments:\n  device_id   Storage device ID\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force device remove\n</code></pre>"},{"location":"reference/cli/#get-device-capacity","title":"Get Device Capacity","text":"<pre><code>usage: sbcli storage-node get-capacity-device [-h] [--history HISTORY]\n                                              device_id\n\npositional arguments:\n  device_id          Storage device ID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  list history records -one for every 15 minutes- for XX\n                     days and YY hours -up to 10 days in total-, format:\n                     XXdYYh\n</code></pre>"},{"location":"reference/cli/#get-device-io-statistics","title":"Get Device IO Statistics","text":"<pre><code>usage: sbcli storage-node get-io-stats-device [-h] [--history HISTORY]\n                                              device_id\n\npositional arguments:\n  device_id          Storage device ID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  list history records -one for every 15 minutes- for XX\n                     days and YY hours -up to 10 days in total-, format:\n                     XXdYYh\n</code></pre>"},{"location":"reference/cli/#list-node-data-interfaces-list","title":"List Node Data Interfaces List","text":"<pre><code>usage: sbcli storage-node port-list [-h] node_id\n\npositional arguments:\n  node_id     Storage node ID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-sata-interfaces-io-stats","title":"Get Sata Interfaces IO Stats","text":"<pre><code>usage: sbcli storage-node port-io-stats [-h] [--history HISTORY] port_id\n\npositional arguments:\n  port_id            Data port ID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  list history records -one for every 15 minutes- for XX\n                     days and YY hours -up to 10 days in total, format: XXdYYh\n</code></pre>"},{"location":"reference/cli/#check-storage-node-health","title":"Check Storage Node Health","text":"<pre><code>usage: sbcli storage-node check [-h] id\n\npositional arguments:\n  id          UUID of storage node\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#check-device-health","title":"Check Device Health","text":"<pre><code>usage: sbcli storage-node check-device [-h] id\n\npositional arguments:\n  id          device UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-mode-information","title":"Get Mode Information","text":"<pre><code>usage: sbcli storage-node info [-h] id\n\npositional arguments:\n  id          Node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-spdk-memory-information","title":"Get SPDK Memory Information","text":"<pre><code>usage: sbcli storage-node info-spdk [-h] id\n\npositional arguments:\n  id          Node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#cluster-commands","title":"Cluster Commands","text":"<pre><code>usage: sbcli cluster [-h]\n                     {create,add,list,status,get,suspend,unsuspend,get-capacity,get-io-stats,get-logs,get-secret,upd-secret,check,update,graceful-shutdown,graceful-startup,list-tasks,delete}\n                     ...\n\npositional arguments:\n  {create,add,list,status,get,suspend,unsuspend,get-capacity,get-io-stats,get-logs,get-secret,upd-secret,check,update,graceful-shutdown,graceful-startup,list-tasks,delete}\n    create              Create an new cluster with this node as mgmt (local\n                        run)\n    add                 Add new cluster\n    list                Show clusters list\n    status              Show cluster status\n    get                 Show cluster info\n    suspend             Suspend cluster\n    unsuspend           Unsuspend cluster\n    get-capacity        Get cluster capacity\n    get-io-stats        Get cluster IO statistics\n    get-logs            Returns cluster status logs\n    get-secret          Get cluster secret\n    upd-secret          Updates the cluster secret\n    check               Health check cluster\n    update              Update cluster mgmt services\n    graceful-shutdown   Graceful shutdown of storage nodes\n    graceful-startup    Graceful startup of storage nodes\n    list-tasks          List tasks by cluster ID\n    delete              Delete Cluster\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#create-new-cluster-attached-to-this-management-node-execute-locally","title":"Create new Cluster attached to this Management Node (execute locally)","text":"<pre><code>usage: sbcli cluster create [-h] [--blk_size {512,4096}]\n                            [--page_size PAGE_SIZE] [--CLI_PASS CLI_PASS]\n                            [--cap-warn CAP_WARN] [--cap-crit CAP_CRIT]\n                            [--prov-cap-warn PROV_CAP_WARN]\n                            [--prov-cap-crit PROV_CAP_CRIT] [--ifname IFNAME]\n                            [--log-del-interval LOG_DEL_INTERVAL]\n                            [--metrics-retention-period METRICS_RETENTION_PERIOD]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --blk_size {512,4096}\n                        The block size in bytes\n  --page_size PAGE_SIZE\n                        The size of a data page in bytes\n  --CLI_PASS CLI_PASS   Password for CLI SSH connection\n  --cap-warn CAP_WARN   Capacity warning level in percent, default=80\n  --cap-crit CAP_CRIT   Capacity critical level in percent, default=90\n  --prov-cap-warn PROV_CAP_WARN\n                        Capacity warning level in percent, default=180\n  --prov-cap-crit PROV_CAP_CRIT\n                        Capacity critical level in percent, default=190\n  --ifname IFNAME       Management interface name, default: eth0\n  --log-del-interval LOG_DEL_INTERVAL\n                        graylog deletion interval, default: 7d\n  --metrics-retention-period METRICS_RETENTION_PERIOD\n                        retention period for prometheus metrics, default: 7d\n</code></pre>"},{"location":"reference/cli/#add-new-cluster","title":"Add new Cluster","text":"<pre><code>usage: sbcli cluster add [-h] [--blk_size {512,4096}] [--page_size PAGE_SIZE]\n                         [--cap-warn CAP_WARN] [--cap-crit CAP_CRIT]\n                         [--prov-cap-warn PROV_CAP_WARN]\n                         [--prov-cap-crit PROV_CAP_CRIT]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --blk_size {512,4096}\n                        The block size in bytes\n  --page_size PAGE_SIZE\n                        The size of a data page in bytes\n  --cap-warn CAP_WARN   Capacity warning level in percent, default=80\n  --cap-crit CAP_CRIT   Capacity critical level in percent, default=90\n  --prov-cap-warn PROV_CAP_WARN\n                        Capacity warning level in percent, default=180\n  --prov-cap-crit PROV_CAP_CRIT\n                        Capacity critical level in percent, default=190\n</code></pre>"},{"location":"reference/cli/#show-clusters-list","title":"Show Clusters List","text":"<pre><code>usage: sbcli cluster list [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#show-cluster-status","title":"Show Cluster Status","text":"<pre><code>usage: sbcli cluster status [-h] cluster_id\n\npositional arguments:\n  cluster_id  the cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#show-cluster-info","title":"Show Cluster Info","text":"<pre><code>usage: sbcli cluster get [-h] id\n\npositional arguments:\n  id          the cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#suspend-cluster","title":"Suspend Cluster","text":"<pre><code>usage: sbcli cluster suspend [-h] cluster_id\n\npositional arguments:\n  cluster_id  the cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#unsuspend-cluster","title":"Unsuspend Cluster","text":"<pre><code>usage: sbcli cluster unsuspend [-h] cluster_id\n\npositional arguments:\n  cluster_id  the cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-cluster-capacity","title":"Get Cluster Capacity","text":"<pre><code>usage: sbcli cluster get-capacity [-h] [--json] [--history HISTORY] cluster_id\n\npositional arguments:\n  cluster_id         the cluster UUID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --json             Print json output\n  --history HISTORY  (XXdYYh), list history records (one for every 15 minutes)\n                     for XX days and YY hours (up to 10 days in total).\n</code></pre>"},{"location":"reference/cli/#get-cluster-io-statistics","title":"Get Cluster IO Statistics","text":"<pre><code>usage: sbcli cluster get-io-stats [-h] [--records RECORDS] [--history HISTORY]\n                                  cluster_id\n\npositional arguments:\n  cluster_id         the cluster UUID\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --records RECORDS  Number of records, default: 20\n  --history HISTORY  (XXdYYh), list history records (one for every 15 minutes)\n                     for XX days and YY hours (up to 10 days in total).\n</code></pre>"},{"location":"reference/cli/#get-cluster-status-logs","title":"Get Cluster Status Logs","text":"<pre><code>usage: sbcli cluster get-logs [-h] cluster_id\n\npositional arguments:\n  cluster_id  cluster uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-cluster-secret","title":"Get Cluster Secret","text":"<pre><code>usage: sbcli cluster get-secret [-h] cluster_id\n\npositional arguments:\n  cluster_id  cluster uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#updates-cluster-secret","title":"Updates Cluster Secret","text":"<pre><code>usage: sbcli cluster upd-secret [-h] cluster_id secret\n\npositional arguments:\n  cluster_id  cluster uuid\n  secret      new 20 characters password\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#check-cluster-health","title":"Check Cluster Health","text":"<pre><code>usage: sbcli cluster check [-h] id\n\npositional arguments:\n  id          cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#update-cluster-management-services","title":"Update Cluster Management Services","text":"<pre><code>usage: sbcli cluster update [-h] id\n\npositional arguments:\n  id          cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#graceful-shutdown-of-storage-nodes","title":"Graceful Shutdown of Storage Nodes","text":"<pre><code>usage: sbcli cluster graceful-shutdown [-h] id\n\npositional arguments:\n  id          cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#graceful-startup-of-storage-nodes","title":"Graceful Startup of Storage Nodes","text":"<pre><code>usage: sbcli cluster graceful-startup [-h] id\n\npositional arguments:\n  id          cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#list-tasks-by-cluster-id","title":"List Tasks by Cluster Id","text":"<pre><code>usage: sbcli cluster list-tasks [-h] cluster_id\n\npositional arguments:\n  cluster_id  UUID of the cluster\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#delete-cluster","title":"Delete cluster","text":"<p>This is only possible, if no storage nodes and pools are attached to the cluster</p> <pre><code>usage: sbcli cluster delete [-h] id\n\npositional arguments:\n  id          cluster UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#logical-volume-commands","title":"Logical Volume Commands","text":"<pre><code>usage: sbcli lvol [-h]\n                  {add,qos-set,list,list-mem,get,delete,connect,resize,create-snapshot,clone,move,get-capacity,get-io-stats,send-cluster-map,get-cluster-map,check}\n                  ...\n\npositional arguments:\n  {add,qos-set,list,list-mem,get,delete,connect,resize,create-snapshot,clone,move,get-capacity,get-io-stats,send-cluster-map,get-cluster-map,check}\n    add                 Add a new logical volume\n    qos-set             Change qos settings for an active logical volume\n    list                List LVols\n    list-mem            Get the size and max_size of the lvol\n    get                 Get LVol details\n    delete              Delete LVol\n    connect             Get lvol connection strings\n    resize              Resize LVol\n    create-snapshot     Create snapshot from LVol\n    clone               create LVol based on a snapshot\n    move                Moves a full copy of the logical volume between nodes\n    get-capacity        Get LVol capacity\n    get-io-stats        Get LVol IO statistics\n    send-cluster-map    send cluster map\n    get-cluster-map     get cluster map\n    check               Health check LVol\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#add-new-logical-volume","title":"Add new Logical Volume","text":"<pre><code>usage: sbcli lvol add [-h] [--snapshot] [--max-size MAX_SIZE]\n                      [--host-id HOST_ID] [--ha-type {single,ha,default}]\n                      [--encrypt] [--crypto-key1 CRYPTO_KEY1]\n                      [--crypto-key2 CRYPTO_KEY2] [--max-rw-iops MAX_RW_IOPS]\n                      [--max-rw-mbytes MAX_RW_MBYTES]\n                      [--max-r-mbytes MAX_R_MBYTES]\n                      [--max-w-mbytes MAX_W_MBYTES] [--distr-vuid DISTR_VUID]\n                      [--distr-ndcs DISTR_NDCS] [--distr-npcs DISTR_NPCS]\n                      [--distr-bs DISTR_BS] [--distr-chunk-bs DISTR_CHUNK_BS]\n                      name size pool\n\npositional arguments:\n  name                  LVol name or id\n  size                  LVol size: 10M, 10G, 10(bytes)\n  pool                  Pool UUID or name\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --snapshot, -s        Make LVol with snapshot capability, default is False\n  --max-size MAX_SIZE   LVol max size\n  --host-id HOST_ID     Primary storage node UUID or Hostname\n  --ha-type {single,ha,default}\n                        LVol HA type (single, ha), default is cluster HA type\n  --encrypt             Use inline data encryption and de-cryption on the\n                        logical volume\n  --crypto-key1 CRYPTO_KEY1\n                        the hex value of key1 to be used for lvol encryption\n  --crypto-key2 CRYPTO_KEY2\n                        the hex value of key2 to be used for lvol encryption\n  --max-rw-iops MAX_RW_IOPS\n                        Maximum Read Write IO Per Second\n  --max-rw-mbytes MAX_RW_MBYTES\n                        Maximum Read Write Mega Bytes Per Second\n  --max-r-mbytes MAX_R_MBYTES\n                        Maximum Read Mega Bytes Per Second\n  --max-w-mbytes MAX_W_MBYTES\n                        Maximum Write Mega Bytes Per Second\n  --distr-vuid DISTR_VUID\n                        (Dev) set vuid manually, default: random (1-99999)\n  --distr-ndcs DISTR_NDCS\n                        (Dev) set ndcs manually, default: 4\n  --distr-npcs DISTR_NPCS\n                        (Dev) set npcs manually, default: 1\n  --distr-bs DISTR_BS   (Dev) distrb bdev block size, default: 4096\n  --distr-chunk-bs DISTR_CHUNK_BS\n                        (Dev) distrb bdev chunk block size, default: 4096\n</code></pre>"},{"location":"reference/cli/#change-qos-settings-for-an-active-logical-volume","title":"Change QOS Settings for an Active Logical Volume","text":"<pre><code>usage: sbcli lvol qos-set [-h] [--max-rw-iops MAX_RW_IOPS]\n                          [--max-rw-mbytes MAX_RW_MBYTES]\n                          [--max-r-mbytes MAX_R_MBYTES]\n                          [--max-w-mbytes MAX_W_MBYTES]\n                          id\n\npositional arguments:\n  id                    LVol id\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --max-rw-iops MAX_RW_IOPS\n                        Maximum Read Write IO Per Second\n  --max-rw-mbytes MAX_RW_MBYTES\n                        Maximum Read Write Mega Bytes Per Second\n  --max-r-mbytes MAX_R_MBYTES\n                        Maximum Read Mega Bytes Per Second\n  --max-w-mbytes MAX_W_MBYTES\n                        Maximum Write Mega Bytes Per Second\n</code></pre>"},{"location":"reference/cli/#list-logical-volumes","title":"List Logical Volumes","text":"<pre><code>usage: sbcli lvol list [-h] [--cluster-id CLUSTER_ID] [--pool POOL] [--json]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cluster-id CLUSTER_ID\n                        List LVols in particular cluster\n  --pool POOL           List LVols in particular Pool ID or name\n  --json                Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#get-the-size-and-maximum-size-of-the-logical-volume","title":"Get the Size and Maximum Size of the Logical Volume","text":"<pre><code>usage: sbcli lvol list-mem [-h] [--json] [--csv]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --json      Print outputs in json format\n  --csv       Print outputs in csv format\n</code></pre>"},{"location":"reference/cli/#get-logical-volume-details","title":"Get Logical Volume Details","text":"<pre><code>usage: sbcli lvol get [-h] [--json] id\n\npositional arguments:\n  id          LVol id or name\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --json      Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#delete-logical-volume","title":"Delete Logical Volume","text":"<p>This is only possible, if no more snapshots and non-inflated clones of the volume exist. The volume must be suspended before it can be deleted.</p> <pre><code>usage: sbcli lvol delete [-h] [--force] id [id ...]\n\npositional arguments:\n  id          LVol id or ids\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force delete LVol from the cluster\n</code></pre>"},{"location":"reference/cli/#get-logical-volume-connection-strings","title":"Get Logical Volume Connection Strings","text":"<p>Multiple connections to the cluster are always available for multipath and high-availability.</p> <pre><code>usage: sbcli lvol connect [-h] id\n\npositional arguments:\n  id          LVol id\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#resize-logical-volume","title":"Resize Logical Volume","text":"<p>The logical volume cannot exceed the maximum size for logical volumes or exceed total remaining provisioned space in pool. It cannot drop below the current utilization.</p> <pre><code>usage: sbcli lvol resize [-h] id size\n\npositional arguments:\n  id          LVol id\n  size        New LVol size size: 10M, 10G, 10(bytes)\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#create-snapshot-from-logical-volume","title":"Create Snapshot from Logical Volume","text":"<pre><code>usage: sbcli lvol create-snapshot [-h] id name\n\npositional arguments:\n  id          LVol id\n  name        snapshot name\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#create-logical-volume-from-snapshot","title":"Create Logical Volume from Snapshot","text":"<pre><code>usage: sbcli lvol clone [-h] [--resize RESIZE] snapshot_id clone_name\n\npositional arguments:\n  snapshot_id      snapshot UUID\n  clone_name       clone name\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --resize RESIZE  New LVol size: 10M, 10G, 10(bytes)\n</code></pre>"},{"location":"reference/cli/#moves-a-full-copy-of-the-logical-volume-between-nodes","title":"Moves a Full Copy of the Logical Volume between Nodes","text":"<pre><code>usage: sbcli lvol move [-h] [--force] id node_id\n\npositional arguments:\n  id          LVol UUID\n  node_id     Destination Node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force LVol delete from source node\n</code></pre>"},{"location":"reference/cli/#get-logical-volume-capacity","title":"Get Logical Volume Capacity","text":"<pre><code>usage: sbcli lvol get-capacity [-h] [--history HISTORY] id\n\npositional arguments:\n  id                 LVol id\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  (XXdYYh), list history records (one for every 15 minutes)\n                     for XX days and YY hours (up to 10 days in total).\n</code></pre>"},{"location":"reference/cli/#get-logical-volume-io-statistics","title":"Get Logical Volume IO Statistics","text":"<pre><code>usage: sbcli lvol get-io-stats [-h] [--history HISTORY] id\n\npositional arguments:\n  id                 LVol id\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  (XXdYYh), list history records (one for every 15 minutes)\n                     for XX days and YY hours (up to 10 days in total).\n</code></pre>"},{"location":"reference/cli/#send-cluster-map","title":"Send Cluster Map","text":"<pre><code>usage: sbcli lvol send-cluster-map [-h] id\n\npositional arguments:\n  id          LVol id\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-cluster-map","title":"Get Cluster Map","text":"<pre><code>usage: sbcli lvol get-cluster-map [-h] id\n\npositional arguments:\n  id          LVol id\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#check-logical-volume-health","title":"Check Logical Volume Health","text":"<pre><code>usage: sbcli lvol check [-h] id\n\npositional arguments:\n  id          UUID of LVol\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#management-node-commands","title":"Management Node Commands","text":"<pre><code>usage: sbcli mgmt [-h] {add,list,remove} ...\n\npositional arguments:\n  {add,list,remove}\n    add              Add Management node to the cluster (local run)\n    list             List Management nodes\n    remove           Remove Management node\n\noptional arguments:\n  -h, --help         show this help message and exit\n</code></pre>"},{"location":"reference/cli/#add-management-node-to-cluster-execute-locally","title":"Add Management Node to Cluster (execute locally)","text":"<pre><code>usage: sbcli mgmt add [-h] cluster_ip cluster_id ifname\n\npositional arguments:\n  cluster_ip  the cluster IP address\n  cluster_id  the cluster UUID\n  ifname      Management interface name\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#list-management-nodes","title":"List Management Nodes","text":"<pre><code>usage: sbcli mgmt list [-h] [--json]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --json      Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#remove-management-node","title":"Remove Management Node","text":"<pre><code>usage: sbcli mgmt remove [-h] id\n\npositional arguments:\n  id          Mgmt node uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#pool-commands","title":"Pool Commands","text":"<pre><code>usage: sbcli pool [-h]\n                  {add,set,list,get,delete,enable,disable,get-secret,upd-secret,get-capacity,get-io-stats}\n                  ...\n\npositional arguments:\n  {add,set,list,get,delete,enable,disable,get-secret,upd-secret,get-capacity,get-io-stats}\n    add                 Add a new Pool\n    set                 Set pool attributes\n    list                List pools\n    get                 get pool details\n    delete              Delete Pool\n    enable              Set pool status to Active\n    disable             Set pool status to Inactive.\n    get-secret          Get pool secret\n    upd-secret          Updates pool secret\n    get-capacity        Get pool capacity\n    get-io-stats        Get pool IO statistics\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#add-new-pool","title":"Add new Pool","text":"<pre><code>usage: sbcli pool add [-h] [--pool-max POOL_MAX] [--lvol-max LVOL_MAX]\n                      [--max-rw-iops MAX_RW_IOPS]\n                      [--max-rw-mbytes MAX_RW_MBYTES]\n                      [--max-r-mbytes MAX_R_MBYTES]\n                      [--max-w-mbytes MAX_W_MBYTES] [--has-secret]\n                      name cluster_id\n\npositional arguments:\n  name                  Pool name\n  cluster_id            Cluster UUID\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --pool-max POOL_MAX   Pool maximum size: 20M, 20G, 0(default)\n  --lvol-max LVOL_MAX   LVol maximum size: 20M, 20G, 0(default)\n  --max-rw-iops MAX_RW_IOPS\n                        Maximum Read Write IO Per Second\n  --max-rw-mbytes MAX_RW_MBYTES\n                        Maximum Read Write Mega Bytes Per Second\n  --max-r-mbytes MAX_R_MBYTES\n                        Maximum Read Mega Bytes Per Second\n  --max-w-mbytes MAX_W_MBYTES\n                        Maximum Write Mega Bytes Per Second\n  --has-secret          Pool is created with a secret (all further API\n                        interactions with the pool and logical volumes in the\n                        pool require this secret)\n</code></pre>"},{"location":"reference/cli/#set-pool-attributes","title":"Set Pool Attributes","text":"<pre><code>usage: sbcli pool set [-h] [--pool-max POOL_MAX] [--lvol-max LVOL_MAX]\n                      [--max-rw-iops MAX_RW_IOPS]\n                      [--max-rw-mbytes MAX_RW_MBYTES]\n                      [--max-r-mbytes MAX_R_MBYTES]\n                      [--max-w-mbytes MAX_W_MBYTES]\n                      id\n\npositional arguments:\n  id                    Pool UUID\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --pool-max POOL_MAX   Pool maximum size: 20M, 20G\n  --lvol-max LVOL_MAX   LVol maximum size: 20M, 20G\n  --max-rw-iops MAX_RW_IOPS\n                        Maximum Read Write IO Per Second\n  --max-rw-mbytes MAX_RW_MBYTES\n                        Maximum Read Write Mega Bytes Per Second\n  --max-r-mbytes MAX_R_MBYTES\n                        Maximum Read Mega Bytes Per Second\n  --max-w-mbytes MAX_W_MBYTES\n                        Maximum Write Mega Bytes Per Second\n</code></pre>"},{"location":"reference/cli/#list-pools","title":"List Pools","text":"<pre><code>usage: sbcli pool list [-h] [--json] [--cluster-id CLUSTER_ID]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --json                Print outputs in json format\n  --cluster-id CLUSTER_ID\n                        ID of the cluster\n</code></pre>"},{"location":"reference/cli/#get-pool-details","title":"Get Pool Details","text":"<pre><code>usage: sbcli pool get [-h] [--json] id\n\npositional arguments:\n  id          pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --json      Print outputs in json format\n</code></pre>"},{"location":"reference/cli/#delete-pool","title":"Delete Pool","text":"<p>It is only possible to delete a pool if it is empty (no provisioned logical volumes contained).</p> <pre><code>usage: sbcli pool delete [-h] id\n\npositional arguments:\n  id          pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#set-pool-status-to-active","title":"Set Pool Status to Active","text":"<pre><code>usage: sbcli pool enable [-h] pool_id\n\npositional arguments:\n  pool_id     pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#set-pool-status-to-inactive","title":"Set Pool Status to Inactive.","text":"<pre><code>usage: sbcli pool disable [-h] pool_id\n\npositional arguments:\n  pool_id     pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-pool-secret","title":"Get Pool Secret","text":"<pre><code>usage: sbcli pool get-secret [-h] pool_id\n\npositional arguments:\n  pool_id     pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#updates-pool-secret","title":"Updates Pool Secret","text":"<pre><code>usage: sbcli pool upd-secret [-h] pool_id secret\n\npositional arguments:\n  pool_id     pool uuid\n  secret      new 20 characters password\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-pool-capacity","title":"Get Pool Capacity","text":"<pre><code>usage: sbcli pool get-capacity [-h] pool_id\n\npositional arguments:\n  pool_id     pool uuid\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#get-pool-io-statistics","title":"Get Pool IO Statistics","text":"<pre><code>usage: sbcli pool get-io-stats [-h] [--history HISTORY] id\n\npositional arguments:\n  id                 Pool id\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --history HISTORY  (XXdYYh), list history records (one for every 15 minutes)\n                     for XX days and YY hours (up to 10 days in total).\n</code></pre>"},{"location":"reference/cli/#snapshot-commands","title":"Snapshot Commands","text":"<pre><code>usage: sbcli snapshot [-h] {add,list,delete,clone} ...\n\npositional arguments:\n  {add,list,delete,clone}\n    add                 Create new snapshot\n    list                List snapshots\n    delete              Delete a snapshot\n    clone               Create LVol from snapshot\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#create-new-snapshot","title":"Create new Snapshot","text":"<pre><code>usage: sbcli snapshot add [-h] id name\n\npositional arguments:\n  id          LVol UUID\n  name        snapshot name\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#list-snapshots","title":"List Snapshots","text":"<pre><code>usage: sbcli snapshot list [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#delete-a-snapshot","title":"Delete a Snapshot","text":"<pre><code>usage: sbcli snapshot delete [-h] id\n\npositional arguments:\n  id          snapshot UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#create-lvol-from-snapshot","title":"Create lvol from snapshot","text":"<pre><code>usage: sbcli snapshot clone [-h] [--resize RESIZE] id lvol_name\n\npositional arguments:\n  id               snapshot UUID\n  lvol_name        LVol name\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --resize RESIZE  New LVol size: 10M, 10G, 10(bytes)\n</code></pre>"},{"location":"reference/cli/#caching-client-node-commands","title":"Caching Client Node Commands","text":"<pre><code>usage: sbcli caching-node [-h]\n                          {deploy,add-node,list,list-lvols,remove,connect,disconnect,recreate}\n                          ...\n\npositional arguments:\n  {deploy,add-node,list,list-lvols,remove,connect,disconnect,recreate}\n    deploy              Deploy caching node on this machine (local exec)\n    add-node            Add new Caching node to the cluster\n    list                List Caching nodes\n    list-lvols          List connected lvols\n    remove              Remove Caching node from the cluster\n    connect             Connect to LVol\n    disconnect          Disconnect LVol from Caching node\n    recreate            recreate Caching node bdevs\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#deploy-caching-node-on-this-machine-execute-locally","title":"Deploy Caching Node on this Machine (execute locally)","text":"<pre><code>usage: sbcli caching-node deploy [-h] [--ifname IFNAME]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --ifname IFNAME  Management interface name, default: eth0\n</code></pre>"},{"location":"reference/cli/#add-new-caching-node-to-cluster","title":"Add new Caching Node to Cluster","text":"<pre><code>usage: sbcli caching-node add-node [-h] [--cpu-mask SPDK_CPU_MASK]\n                                   [--memory SPDK_MEM]\n                                   [--spdk-image SPDK_IMAGE]\n                                   [--namespace NAMESPACE]\n                                   cluster_id node_ip ifname\n\npositional arguments:\n  cluster_id            UUID of the cluster to which the node will belong\n  node_ip               IP of caching node to add\n  ifname                Management interface name\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cpu-mask SPDK_CPU_MASK\n                        SPDK app CPU mask, default is all cores found\n  --memory SPDK_MEM     SPDK huge memory allocation, default is Max hugepages\n                        available\n  --spdk-image SPDK_IMAGE\n                        SPDK image uri\n  --namespace NAMESPACE\n                        k8s namespace to deploy on\n</code></pre>"},{"location":"reference/cli/#list-caching-nodes","title":"List Caching Nodes","text":"<pre><code>usage: sbcli caching-node list [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#list-connected-logical-volumes","title":"List Connected Logical Volumes","text":"<pre><code>usage: sbcli caching-node list-lvols [-h] id\n\npositional arguments:\n  id          Caching Node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#remove-caching-node-from-cluster","title":"Remove Caching Node from Cluster","text":"<pre><code>usage: sbcli caching-node remove [-h] [--force] id\n\npositional arguments:\n  id          Caching Node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --force     Force remove\n</code></pre>"},{"location":"reference/cli/#connect-to-logical-volume","title":"Connect to Logical Volume","text":"<pre><code>usage: sbcli caching-node connect [-h] node_id lvol_id\n\npositional arguments:\n  node_id     Caching node UUID\n  lvol_id     LVol UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#disconnect-logical-volume-from-caching-node","title":"Disconnect Logical Volume From Caching Node","text":"<pre><code>usage: sbcli caching-node disconnect [-h] node_id lvol_id\n\npositional arguments:\n  node_id     Caching node UUID\n  lvol_id     LVol UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/cli/#recreate-a-caching-node-bdevs","title":"Recreate a Caching Node bdevs","text":"<pre><code>usage: sbcli caching-node recreate [-h] node_id\n\npositional arguments:\n  node_id     Caching node UUID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"reference/huge-pages-calculator/","title":"Huge Pages Calculator","text":"<p>Calculating the correct minimum size of huge pages reserved for simplyblock isn't an easy task. Therefore, simplyblock provides the following calculator.</p> <p>The calculator provides the number of 2 MiB sized huge pages.</p> Huge Pages Calculator Number of disks: Max provisioned storage (TB): Number of logical volumes: Number of CPUs: Number of distributed worker threads: Minimum required huge pages: 3436 <p>The resulting number can be used for temporary allocation of huge pages or to persist and pre-allocate them during system boot-up.</p> Temporary allocation<pre><code>sudo sysctl vm.nr_hugepages=4096\n</code></pre> Persisted allocation<pre><code>GRUB_CMDLINE_LINUX=\"${GRUB_CMDLINE_LINUX} default_hugepagesz=2MB hugepagesz=2MB hugepages=4096\"\n</code></pre> Persist the configuration change<pre><code>sudo grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg\n</code></pre>"},{"location":"reference/supported-linux-distributions/","title":"Supported Linux Distributions","text":"<p>Simplyblock requires a Linux Kernel 5.19 or later with NVMe over Fabrics and NVMe over TCP enabled. However, <code>sbcli</code>, the simplyblock commandline interface, requires some additional tools and expects certain conventions for configuration files and locations. Therefore, simplyblock officially only supports Red Hat-based Linux distribution as of now.</p> <p>While others may work, manual intervention may be required and simplyblock cannot support those as of now.</p> <p>The following Linux distributions are considered tested and supported:</p> Distribution Version Support Level Red Hat Enterprise Linux 9 Fully supported Rocky Linux 9 Fully supported AlmaLinux 9 Fully supported Amazon Linux 2 - Partially supported Amazon Linux 2023 - Fully supported"},{"location":"reference/supported-linux-kernels/","title":"Supported Linux Kernels","text":"<p>Simplyblock is built upon NVMe over Fabrics. Hence, it requires a Linux kernel with NVMe and NVMe-oF support.</p> <p>As a general rule, every Linux kernel 5.19 or later is expected to work, as long as the kernel modules for NVMe (nvme), NVMe over Fabrics (nvme-of), and NVMe over TCP (nvme-tcp) are available. In most cases, the latter two kernel modules need to be loaded manually or persisted. Please see the Bare Metal (Linux) installation section on how to do this.</p> <p>The following kernels are known to be compatible and tested. Additional kernel versions may work but are untested.</p> OS Linux Kernel Prerequisite Red Hat Enterprise Linux 4.18.0-xxx Kernel on x86_64 modprobe nvme-tcp Amazon Linux 2 Kernel 5.10 AMI 2.0.20230822.0 modprobe nvme-tcp Amazon Linux 2023 2023.1.20230825.0 x86_64 HVM kernel-6.1 modprobe nvme-tcp <p>Warning</p> <p>Amazon Linux 2 has a bug with NVMe multipathing. That means that NVMe over Fabrics on Amazon Linux operates in a degraded state. It is NOT recommended to use Amazon Linux 2 images to run simplyblock.</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"tutorials/","title":"Tutorials","text":"<p>The tutorial section provides a collection of tutorials, introduction videos, and step-by-step guides to help users get started with simplyblock.</p>"},{"location":"tutorials/aws-deployment/","title":"Deployment on AWS","text":""},{"location":"tutorials/kubernetes-deployment/","title":"Deployment on Kubernetes","text":""},{"location":"tutorials/simplyblock-intro/","title":"Introduction to Simplyblock","text":""},{"location":"tutorials/sla-intro/","title":"Introduction to SLA","text":""},{"location":"usage/","title":"Usage","text":"<p>Simplyblock provides powerful tools for managing Logical Volumes (LVs) through its command-line interface (sbcli) and Kubernetes CSI driver, enabling seamless storage operations in cloud-native and enterprise environments. Whether working in a standalone environment or integrating with Kubernetes, these tools allow users to provision, unprovision, expand, snapshot, and clone logical volumes efficiently.</p> <p>The <code>sbcli</code> utility offers direct control over storage management via the command line, making it ideal for administrators who need fine-grained operational control. Meanwhile, the Kubernetes CSI driver enables automated volume provisioning and lifecycle management within containerized workloads. This section provides detailed guidance on using both methods to perform essential storage operations, ensuring scalable and resilient data management within simplyblock\u2019s distributed storage architecture.</p>"},{"location":"usage/baremetal/","title":"Bare Metal (Linux)","text":""},{"location":"usage/baremetal/cloning/","title":"Cloning a Logical Volume","text":""},{"location":"usage/baremetal/encrypting/","title":"Encrypting a Logical Volume","text":""},{"location":"usage/baremetal/expanding/","title":"Expanding a Logical Volume","text":""},{"location":"usage/baremetal/provisioning/","title":"Provisioning a Logical Volume","text":""},{"location":"usage/baremetal/quality-of-service/","title":"Defining Quality of Service","text":""},{"location":"usage/baremetal/removing/","title":"Removing a Logical Volume","text":""},{"location":"usage/baremetal/snapshotting/","title":"Snapshotting a Logical Volume","text":""},{"location":"usage/cluster/","title":"Storage Cluster","text":""},{"location":"usage/cluster/cluster-management/","title":"General Cluster Management","text":""},{"location":"usage/cluster/replacing-storage-node/","title":"Replacing a Storage Node","text":""},{"location":"usage/cluster/storage-node-eviction/","title":"Storage Node Eviction (Kubernetes)","text":""},{"location":"usage/simplyblock-csi/","title":"Kubernetes CSI","text":""},{"location":"usage/simplyblock-csi/cloning/","title":"Cloning","text":"<p>Kubernetes PersistentVolumes, backed by simplyblock, can be instantly cloned. A clone refers back to the same data, as simplyblock is a full copy-on-write storage engine. That enables instant database forks which act independently after cloning.</p>"},{"location":"usage/simplyblock-csi/cloning/#create-a-volume-clone","title":"Create a Volume Clone","text":"<p>To clone an existing PersistentVolume, it has to be named as the clone basis (dataSource) in the PersistentVolumeClaim Kubernetes resource.</p> PersistentVolumeClaim to clone an existing volume<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-persistent-volume-clone\nspec:\n  storageClassName: simplyblock-storage-class\n  dataSource:\n    name: original-persistent-volume-name # &lt;- Name of the original volume\n    kind: PersistentVolumeClaim\n    apiGroup: \"\"\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the cloned PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: cloned-database\n  labels:\n    app: cloned-database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-cloned-volume\n  volumes:\n  - name: my-cloned-volume\n    persistentVolumeClaim:\n      claimName: my-persistent-volume-clone\n</code></pre>"},{"location":"usage/simplyblock-csi/data-locality/","title":"Defining Data Locality","text":""},{"location":"usage/simplyblock-csi/encrypting/","title":"Encrypting","text":"<p>Simplyblock supports the encryption of logical volumes. Internally, simplyblock utilizes the industry-proven crypto bdev provided by SPDK to implement its encryption functionality.</p> <p>The encryption uses a AES_XTS variable length block cipher. This cipher requires two keys of 16 to 32 bytes each. The keys need to have the same length, meaning, if one key is 32 bytes long, the other one has to be 32 bytes, too.</p> <p>Recommendation</p> <p>Simplyblock strongly recommends two keys of 32 bytes.</p>"},{"location":"usage/simplyblock-csi/encrypting/#generate-random-keys","title":"Generate Random Keys","text":"<p>Simplyblock does not provide an integrated way to generate encryption keys but recommends using the OpenSSL tool chain. For Kubernetes, the encryption key needs to be provided as base64, hence it's encoded right away.</p> <p>To generate the two keys, the following command is run twice. The result must be stored for later.</p> Create en Encryption Key<pre><code>openssl rand -hex 32 | base64 -w0\n</code></pre>"},{"location":"usage/simplyblock-csi/encrypting/#create-the-kubernetes-secret","title":"Create the Kubernetes Secret","text":"<p>Next up, a Kubernetes Secret is created, providing the just created two encryption keys.</p> Create a Kubernetes Secret Resource<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-encryption-keys\ndata:\n  crypto_key1: YzIzYzllY2I4MWJmYmY1ZDM5ZDA0NThjNWZlNzQwNjY2Y2RjZDViNWE4NTZkOTA5YmRmODFjM2UxM2FkZGU4Ngo=\n  crypto_key2: ZmFhMGFlMzZkNmIyODdhMjYxMzZhYWI3ZTcwZDEwZjBmYWJlMzYzMDRjNTBjYTY5Nzk2ZGRlZGJiMDMwMGJmNwo=\n</code></pre> <p>The Kubernetes Secret can be used for one or more logical volumes. Using different encryption keys, multiple tenants can be secured with an additional isolation layer against each other.</p>"},{"location":"usage/simplyblock-csi/encrypting/#storageclass-configuration","title":"StorageClass Configuration","text":"<p>A new Kubernetes StorageClass needs to be created, or an existing one needs to be configured. To use encryption on a persistent volume claim level, the storage class has to be set for encryption.</p> Example StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\" # This is important!\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"usage/simplyblock-csi/encrypting/#create-a-persistentvolumeclaim","title":"Create a PersistentVolumeClaim","text":"<p>When requesting a logical volume through a Kubernetes PersistentVolumeClaim, the storage class and the secret resources have to be connected to the PVC. When picked up, simplyblock will automatically collect the keys and creates the logical volumes as a fully encrypted logical volume.</p> Create an encrypting PersistentVolumeClaim<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  annotations:\n    simplybk/secret-name: my-encryption-keys # Encryption keys\n  name: my-encrypted-volume-claim\nspec:\n  storageClassName: my-encrypted-volumes # StorageClass\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 200Gi\n</code></pre>"},{"location":"usage/simplyblock-csi/expanding/","title":"Expanding","text":""},{"location":"usage/simplyblock-csi/provisioning/","title":"Provisioning","text":"<p>Provisioning a new PersistentVolume using simplyblock's Kubernetes CSI driver integration, requires at least one StorageClass to be set up.</p>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-new-volume","title":"Create a new Volume","text":"<p>To create a new persistent volume backed by simplyblock, requires a persistent volume claim with the correct storage class.</p> Create a new PersistentVolumeClaim<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-simplyblock-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n  storageClassName: simplyblock-storage-class     \n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: database\n  labels:\n    app: database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-volume\n  volumes:\n  - name: my-volume\n    persistentVolumeClaim:\n      claimName: my-simplyblock-volume\n</code></pre>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-volume-from-a-snapshot","title":"Create a Volume from a Snapshot","text":"<p>To create a new persistent volume claim from an existing snapshot, see the section about Restoring a Snapshot.</p>"},{"location":"usage/simplyblock-csi/provisioning/#create-a-cloned-volume","title":"Create a cloned Volume","text":"<p>To create a new persistent volume claim from an existing and live volume, see the section about Cloning.</p>"},{"location":"usage/simplyblock-csi/quality-of-service/","title":"Defining Quality of Service","text":""},{"location":"usage/simplyblock-csi/removing/","title":"Removing","text":"<p>A simplyblock-managed logical volume which is connected to a Kubernetes PersistentVolumeClaim is target to Kubernetes' automatic lifecycle management. Therefore, if the PVC is removed, the logical volume is removed as well.</p> <p>If the storage class is defined with a reclaim policy that keeps the volume around after it's claim has been deleted, it has to be removed specifically.</p>"},{"location":"usage/simplyblock-csi/removing/#removing-a-persistent-volume","title":"Removing a Persistent Volume","text":"<p>When a Persistent Volume (PV) in Kubernetes has its reclaim policy set to Retain, deleting the associated Persistent Volume Claim (PVC) does not automatically delete the PV or its underlying storage. Instead, the PV enters a Released state, signaling that the PVC has been deleted, but the storage remains intact and requires manual cleanup. This reclaim policy is commonly used when manual review of data or explicit deprovisioning is required.</p>"},{"location":"usage/simplyblock-csi/removing/#steps-to-remove-a-retained-persistent-volume","title":"Steps to remove a retained persistent volume","text":"<p>If the PersistentVolumeClaim still exists, it has to be deleted first:</p> Removing a PersistentVolumeClaim<pre><code>kubectl delete pvc &lt;pvc-name&gt;\n</code></pre> <p>When the PVC is deleted, the PersistentVolume state must be checked. It should be released:</p> Check PersistentVolume status<pre><code>kubectl get pv\n</code></pre> <p>Now the PV can be deleted:</p> Delete a PersistentVolume<pre><code>kubectl get pv &lt;pv-name&gt;\n</code></pre>"},{"location":"usage/simplyblock-csi/snapshotting/","title":"Snapshotting","text":"<p>Kubernetes PersistentVolumes backed by simplyblock can be instantly snapshotted. Snapshots are almost free due to simplyblock's copy-on-write nature.</p> <p>In simplyblock, a snapshot is comparable to the table of contents in a book, meaning, the snapshot refers to the same data as the original volume. If the volume diverges from the snapshot, the mutated data segment is duplicated, changed, and stored as a new data block. Now the volume refers to the new block, while the snapshot to the old one.</p> <p>A deeper explanation can be found here:</p> <p></p>"},{"location":"usage/simplyblock-csi/snapshotting/#snapshotting-a-persistentvolume","title":"Snapshotting a PersistentVolume","text":"<p>To snapshot a persistent volume, a new Kubernetes Snapshot resource is created. When applying the resource, the snapshot is taken immediately.</p> Creating a Snapshot resource<pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: my-volume-snapshot\nspec:\n  volumeSnapshotClassName: csi-spdk-snapclass\n  source:\n    persistentVolumeClaimName: my-persistent-volume-claim # &lt;- refers to the PVC to snapshot\n</code></pre>"},{"location":"usage/simplyblock-csi/snapshotting/#restore-a-volume-from-a-snapshot","title":"Restore a Volume from a Snapshot","text":"<p>After a snapshot was created, it can be used as a source (dataSource) of a new persistent volume. In this case, the new persistent volume claim refers to the snapshot which is automatically restored into the new persistent volume.</p> Restoring a snapshot<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-restored-snapshot-volume\nspec:\n  storageClassName: simplyblock-storage-class\n  dataSource:\n    name: my-volume-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\n</code></pre> <p>Afterward, the PVC can be used as a normal PVC and added to a pod.</p> Using the restored PersistentVolumeClaim<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: restored-database\n  labels:\n    app: restored-database\nspec:\n  containers:\n  - name: alpine\n    image: alpine:3\n    imagePullPolicy: \"IfNotPresent\"\n    command: [\"sleep\", \"365d\"]\n    volumeMounts:\n    - mountPath: \"/mounted\"\n      name: my-restored-volume\n  volumes:\n  - name: my-restored-volume\n    persistentVolumeClaim:\n      claimName: my-restored-snapshot-volume\n</code></pre>"},{"location":"usage/simplyblock-csi/storage-class/","title":"Storage Class","text":"<p>A Kubernetes StorageClass defines the way dynamic storage provisioning is handled within a cluster. StorageClasses allow administrators to specify different types of storage with varying performance characteristics, redundancy configurations, and provisioning parameters. When a PersistentVolumeClaim (PVC) references a StorageClass, Kubernetes automatically provisions a Persistent Volume (PV) according to the defined specifications.</p>"},{"location":"usage/simplyblock-csi/storage-class/#how-simplyblock-uses-storageclass","title":"How Simplyblock Uses StorageClass","text":"<p>Simplyblock integrates with Kubernetes through its CSI (Container Storage Interface) driver and leverages StorageClasses to manage the dynamic provisioning of Logical Volumes (LVs). The simplyblock StorageClass defines how LVs are created within the simplyblock cluster, specifying parameters such as:</p> <ul> <li>Provisioning size</li> <li>Quality of Service (QoS)</li> <li>Encryption</li> </ul> <p>When a user deploys a PVC referencing the simplyblock StorageClass, the CSI driver automatically communicates with the simplyblock control plane to provision a logical volume matching the requested specifications. This process abstracts the complexity of volume creation and ensures that workloads running in Kubernetes receive high-performance, resilient block storage directly backed by simplyblock.</p>"},{"location":"usage/simplyblock-csi/storage-class/#example-usage","title":"Example Usage","text":"<p>A typical simplyblock StorageClass contains the name of the storage class, a filesystem type to automatically format the logical volume (or provide a raw block device if missing), the reclaim policy.</p> Example StorageClass<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-volumes\nprovisioner: csi.simplyblock.io\nparameters:\n  encryption: \"True\"\n  csi.storage.k8s.io/fstype: ext4\n  ... other parameters\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n</code></pre>"},{"location":"usage/simplyblock-csi/storage-class/#available-parameters","title":"Available Parameters","text":"Parameter Name Value Type Description Optional Default csi.storage.k8s.io/fstype string Defines the filesystem to format the logical volume. If not specific, a raw block device is given to the container. true none pool_name string Defines the simplyblock storage pool name to use. false testing1 qos_rw_iops int Defines the minimum IOPS reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_rw_mbytes int Defines the minimum total throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_r_mbytes int Defines the minimum read throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 qos_w_mbytes int Defines the minimum write throughput in megabytes reserved for a logical volume of this storage class. A zero (0) means no minimum. true 0 compression bool Defines if the logical volume of this storage class will be stored compressed or not. true false encryption bool Defines if the logical volume of this storage class will be encrypted or not. true false distr_ndcs int ? true 1 distr_npcs int ? true 1 lvol_priority_class int Defines the priority class of a logical volume of this storage class. true 0"},{"location":"usage/simplyblock-csi/trimming/","title":"Trimming a Filesystem","text":""}]}